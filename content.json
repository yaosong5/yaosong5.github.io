{"meta":{"title":"é’¢é“é”…","subtitle":"åº”å°½ä¾¿é¡»å°½ï¼Œæ— å¤ç‹¬å¤šè™‘","description":"çºµæµªå¤§åŒ–ä¸­ï¼Œä¸å–œäº¦ä¸æ‚²","author":"GTG","url":"http://gangtieguo.cn","root":"/"},"pages":[{"title":"","date":"2019-06-02T15:55:26.158Z","updated":"2018-05-17T01:53:01.988Z","comments":true,"path":"baidu_verify_WHXmBFaAkY.html","permalink":"http://gangtieguo.cn/baidu_verify_WHXmBFaAkY.html","excerpt":"","text":"WHXmBFaAkY"},{"title":"","date":"2019-06-02T15:55:26.159Z","updated":"2018-05-17T02:15:40.515Z","comments":true,"path":"google00655d7c846aab3a.html","permalink":"http://gangtieguo.cn/google00655d7c846aab3a.html","excerpt":"","text":"google-site-verification: google00655d7c846aab3a.html"},{"title":"å…³äº","date":"2018-05-08T07:52:07.000Z","updated":"2018-08-07T07:27:50.398Z","comments":true,"path":"about/index.html","permalink":"http://gangtieguo.cn/about/index.html","excerpt":"","text":"Nothing"},{"title":"404 Not Foundï¼šè¯¥é¡µæ— æ³•æ˜¾ç¤º","date":"2018-05-18T06:50:42.172Z","updated":"2018-05-08T10:00:51.508Z","comments":false,"path":"/404.html","permalink":"http://gangtieguo.cn//404.html","excerpt":"","text":""},{"title":"åˆ†ç±»","date":"2018-08-13T06:25:13.838Z","updated":"2018-08-07T07:28:04.841Z","comments":true,"path":"categories/index.html","permalink":"http://gangtieguo.cn/categories/index.html","excerpt":"","text":""},{"title":"æ ‡ç­¾","date":"2018-08-07T07:27:29.140Z","updated":"2018-08-07T07:27:29.135Z","comments":true,"path":"tags/index.html","permalink":"http://gangtieguo.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Hiveè§£æä»»åŠ¡-å°†jsonçš„å¤šä¸ªå±æ€§æ‹†åˆ†æˆå¤šæ¡è®°å½•","slug":"Hiveè§£æä»»åŠ¡","date":"2019-03-12T08:07:30.918Z","updated":"2019-06-17T04:40:09.387Z","comments":true,"path":"2019/03/12/Hiveè§£æä»»åŠ¡/","link":"","permalink":"http://gangtieguo.cn/2019/03/12/Hiveè§£æä»»åŠ¡/","excerpt":"[TOC] éœ€æ±‚ç¯å¢ƒï¼š åœ¨hiveè¡¨dwb.dwb_r_thrid_dataä¸­ï¼Œdataå­—æ®µå­˜æ”¾æœ‰jsonå­—ç¬¦ä¸²","text":"[TOC] éœ€æ±‚ç¯å¢ƒï¼š åœ¨hiveè¡¨dwb.dwb_r_thrid_dataä¸­ï¼Œdataå­—æ®µå­˜æ”¾æœ‰jsonå­—ç¬¦ä¸² éœ€è¦ä»jsonå­—ç¬¦ä¸²ä¸­ï¼Œè§£æåˆ°éœ€è¦çš„å­—æ®µï¼šå°†ä¸€ä¸ªjsoné‡Œé¢çš„å±æ€§data.loanInfo.mobile.timeScopes.D360ã€data.loanInfo.mobile.timeScopes.D90æ‰€åŒ…å«çš„å­—æ®µåˆ†åˆ«è§£ææˆä¸€æ¡è®°å½•ï¼Œå¹¶ä¸”å°†D360ã€D90ä¹Ÿä½œä¸ºå­—æ®µtimeScopeçš„å€¼è§£æåˆ°è¯¥æ¡è®°å½•ä¸­ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122&#123; \"code\":\"0\", \"data\":&#123; \"loanInfo\":&#123; \"mobile\":&#123; \"timeScopes\":&#123; \"D360\":&#123; \"maxOverdueDays\":-1, \"loanTenantCount\":0, \"monthsFromFirstLoan\":-1, \"averageLoanGapDays\":-1, \"averageLoanAmount\":0, \"averageTenantGapDays\":-1, \"loanCount\":0, \"maxLoanAmount\":0, \"daysFromLastLoan\":-1, \"overdueTenantCount\":-1, \"queryCount\":0, \"monthsFromLastOverdue\":-1, \"maxLoanPeriodDays\":0, \"remainingAmount\":-1, \"monthsForNormalRepay\":-1, \"overdueLoanCount\":-1, \"overdueFor2TermTenantCount\":-1 &#125;, \"D90\":&#123; \"maxOverdueDays\":-1, \"loanTenantCount\":0, \"averageLoanGapDays\":-1, \"averageLoanAmount\":0, \"averageTenantGapDays\":-1, \"overdueLoanCount\":-1, \"overdueFor2TermTenantCount\":-1, \"loanCount\":0, \"maxLoanAmount\":0, \"overdueTenantCount\":-1, \"queryCount\":0, \"maxLoanPeriodDays\":0 &#125; &#125; &#125;, \"pid\":&#123; \"timeScopes\":&#123; \"D360\":&#123; \"maxOverdueDays\":-1, \"loanTenantCount\":0, \"monthsFromFirstLoan\":-1, \"averageLoanGapDays\":-1, \"averageLoanAmount\":0, \"averageTenantGapDays\":-1, \"loanCount\":0, \"maxLoanAmount\":0, \"daysFromLastLoan\":-1, \"overdueTenantCount\":-1, \"queryCount\":0, \"monthsFromLastOverdue\":-1, \"maxLoanPeriodDays\":0, \"remainingAmount\":-1, \"monthsForNormalRepay\":-1, \"overdueLoanCount\":-1, \"overdueFor2TermTenantCount\":-1 &#125;, \"D90\":&#123; \"maxOverdueDays\":-1, \"loanTenantCount\":0, \"averageLoanGapDays\":-1, \"averageLoanAmount\":0, \"averageTenantGapDays\":-1, \"overdueLoanCount\":-1, \"overdueFor2TermTenantCount\":-1, \"loanCount\":0, \"maxLoanAmount\":0, \"overdueTenantCount\":-1, \"queryCount\":0, \"maxLoanPeriodDays\":0 &#125; &#125; &#125;, \"deviceId\":&#123; \"timeScopes\":&#123; \"D360\":&#123; \"loanTenantCount\":0, \"loanCount\":0, \"queryCount\":0 &#125;, \"D90\":&#123; \"loanTenantCount\":0, \"loanCount\":0, \"queryCount\":0 &#125; &#125; &#125; &#125;, \"blacklist\":&#123; \"mobile\":&#123; \"lastConfirmAtDays\":-1, \"lastConfirmStatus\":\"\", \"blackLevel\":\"none\", \"last6MTenantCount\":0, \"last6MQueryCount\":0, \"last12MMaxConfirmStatus\":\"\" &#125;, \"pid\":&#123; \"lastConfirmAtDays\":-1, \"lastConfirmStatus\":\"\", \"blackLevel\":\"none\", \"last6MTenantCount\":0, \"last6MQueryCount\":0, \"last12MMaxConfirmStatus\":\"\" &#125;, \"deviceId\":&#123; \"lastConfirmAtDays\":-1, \"lastConfirmStatus\":\"\", \"blackLevel\":\"none\", \"last6MTenantCount\":0, \"last6MQueryCount\":0, \"last12MMaxConfirmStatus\":\"\" &#125; &#125; &#125;, \"message\":\"è¯·æ±‚æˆåŠŸ\"&#125; è¡¨ç»“æ„å½¢å¦‚ï¼š 12345678910111213141516171819202122232425262728create table if not exists dwb.dwb_r_morpho_loaninfo_mobile( apply_risk_id string comment \"é£æ§ID\", dp_data_id string comment \"dp_dataID\", maxOverdueDays string, loanTenantCount string, monthsFromFirstLoan string, averageLoanGapDays string, averageLoanAmount string, averageTenantGapDays string, loanCount string, maxLoanAmount string, daysFromLastLoan string, overdueTenantCount string, queryCount string, monthsFromLastOverdue string, maxLoanPeriodDays string, remainingAmount string, monthsForNormalRepay string, overdueLoanCount string, overdueFor2TermTenantCount string, timeScope string comment \"æ—¶é—´æœŸé™\", morpho_created_at string comment \"åˆ›å»ºæ—¶é—´\", etl_time string comment \"etlå¤„ç†æ—¶é—´\") comment 'moblie' PARTITIONED BY (dt string comment 'åˆ†åŒºæ—¥æœŸ') row format delimited fields terminated by '\\001' NULL DEFINED AS '' stored as orc; æ¥ä¸‹æ¥å°±å¼€å§‹è¡¨æ¼”å§ã€‚ å¦‚æœæ˜¯jsonæ•°ç»„,å¯ä»¥å¾ˆæ–¹ä¾¿æ‹†åˆ†æˆ‘ä»¬éƒ½çŸ¥é“å¯¹äºä¸€æ¡jsoné‡Œé¢å€¼ä¸ºjsonæ•°ç»„çš„å±æ€§ï¼Œhiveå¯ä»¥å°†å…¶è·å–åˆ°å¹¶ä¸”è¿›è¡Œæ‹†åˆ†æˆå¤šæ¡è®°å½•ï¼š å¦‚ä»¥ä¸‹infoquerybeanå±æ€§ï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#123; \"overduemoreamt\":\"0\", \"loancount\":\"0\", \"loanbal\":\"0\", \"outstandcount\":\"0\", \"queryatotalorg\":\"æœ€è¿‘***********\", \"loanamt\":\"0\", \"overdueamt\":\"0\", \"generationcount\":\"0\", \"msgContent\":\"æˆåŠŸ!\", \"generationamount\":\"0\", \"overduemorecount\":\"0\", \"totalorg\":\"*************\", \"infoquerybean\":[ &#123; \"s_value\":\"å®¡æ‰¹\", \"ddate\":\"2018-09-10\", \"ordernum\":\"1\" &#125;, &#123; \"s_value\":\"å®¡æ‰¹\", \"ddate\":\"2018-09-06\", \"ordernum\":\"2\" &#125;, &#123; \"s_value\":\"å®¡æ‰¹\", \"ddate\":\"2018-08-21\", \"ordernum\":\"3\" &#125;, &#123; \"s_value\":\"å®¡æ‰¹\", \"ddate\":\"2018-08-09\", \"ordernum\":\"4\" &#125;, &#123; \"s_value\":\"å®¡æ‰¹\", \"ddate\":\"2018-07-28\", \"ordernum\":\"5\" &#125;, &#123; \"s_value\":\"å®¡æ‰¹\", \"ddate\":\"2018-07-27\", \"ordernum\":\"6\" &#125; ], \"overduecount\":\"0\", \"msgCode\":\"200\"&#125; å¯ä»¥é€šè¿‡splitæ‹†åˆ†æˆç»“æœ,æ’å…¥åˆ°å½¢çŠ¶å¦‚ä¸‹çš„è¡¨ä¸­ï¼š 12345678910111213create table if not exists dwb.dwb_r_nifa_share_detail_n( apply_risk_id string comment \"é£æ§ID\", dp_data_id string comment \"dp_dataID\", nifa_share_detail_ordernum string comment 'åºå·', nifa_share_detail_ddate string comment 'æŸ¥è¯¢æ—¥æœŸ', nifa_share_detail_s_value string comment 'æŸ¥è¯¢åŸå› ', nifa_share_created_at string comment \"åˆ›å»ºæ—¶é—´\", etl_time string comment \"etlå¤„ç†æ—¶é—´\") comment 'table test' PARTITIONED BY (dt string comment 'åˆ†åŒºæ—¥æœŸ') row format delimited fields terminated by '\\001' NULL DEFINED AS '' stored as orc; é€šè¿‡ 1explode(split(default.get_json_path(a.data,'infoquerybean'),'@\\\\|@')) å°†å…¶æ‹†åˆ†æˆå¤šæ¡è®°å½•ï¼Œå®Œæ•´sqlè§ä¸‹ï¼š 123456789101112131415161718192021dt=$1hive&lt;&lt;!set mapreduce.job.queuename=root.dw;set hive.support.concurrency=false;insert overwrite table dwb.dwb_r_nifa_share_detail_n partition(dt='$dt')select a.apply_risk_id, a.dp_data_id, nifa_share_detail_ordernum, nifa_share_detail_ddate, nifa_share_detail_s_value, from_unixtime(cast(a.timestamp/1000 as bigint),'yyyy-MM-dd HH:mm:ss') as nifa_share_created_at, current_timestamp() as etl_timefrom (select dwb_r_thrid_data.apply_risk_id,dwb_r_thrid_data.dp_data_id,dwb_r_thrid_data.data,dwb_r_thrid_data.timestamp from dwb.dwb_r_thrid_data where channel_name = 'nifa_prod' and interface_name = 'share' and get_json_object(data,'$.msgCode') = '200' and dwb_r_thrid_data.dt='$dt' ) a lateral view explode(split(default.get_json_path(a.data,'infoquerybean'),'@\\\\\\\\|@')) b as infoquerybean lateral view default.json_tuple2(b.infoquerybean,'ordernum','ddate','ordernum') c as nifa_share_detail_ordernu,nifa_share_detail_ddate, nifa_share_detail_s_value;! å¾—åˆ°ç»“æœï¼š é¡ºç€jsonæ•°ç»„æ€è·¯ï¼Œæ”¹é€ jsonæ ·å¼é€šè¿‡get_json_object()æ–¹æ³•ï¼Œå¾—åˆ°ä¸¤ä¸ªjsonå±æ€§ï¼Œé€šè¿‡concatæ‹¼æ¥æˆjsonæ•°ç»„ï¼Œå°±å¯ä»¥åƒä¸Šé¢é‚£æ ·æ‹†åˆ†æˆå¤šæ¡è®°å½•ã€‚ï¼ˆæµ‹è¯•é˜¶æ®µçš„æ ·ä¾‹éƒ½ä½¿ç”¨äº†è®¾å®šåˆ†åŒºdt,é™åˆ¶æ¡æ•°ï¼Œå› ä¸ºè¿™æ ·æµ‹è¯•èµ·æ¥å¾ˆå¿«ï¼Œåªéœ€è¦ä¸‰ç§’!!!!ğŸ˜ğŸ˜ğŸ˜ğŸ˜ï¼‰ 1.é€šè¿‡get_json_objectæ–¹æ³•1select get_json_object(td.data,\"$.data.loanInfo.mobile.timeScopes.D360\") d3,get_json_object(td.data,\"$.data.loanInfo.mobile.timeScopes.D90\") d9 from dwb.dwb_r_thrid_data td where channel_name ='morpho' and interface_name ='query' and dt='20190218' limit 5 å¾—åˆ° 2.æ‹¼æ¥è·å–çš„D360å’ŒD90å­—æ®µ1select td.*,concat(regexp_replace(get_json_object(td.data,\"$.data.loanInfo.mobile.timeScopes.D360\"),'&#125;',',\"timeScope\":\"D360\"&#125;'),\"|\",regexp_replace(get_json_object(td.data,\"$.data.loanInfo.mobile.timeScopes.D90\"),'&#125;',',\"timeScope\":\"D90\"&#125;')) ts from dwb.dwb_r_thrid_data td where channel_name ='morpho' and interface_name ='query' and dt='20190218' limit 5 æ‹¼æ¥çš„å­—ç¬¦ä¸²æ ·å¼ï¼Œé€šè¿‡â€|â€åˆ†éš”ä¸¤ä¸ªå¯¹è±¡ 1&#123;\"maxOverdueDays\":-1,\"monthsFromFirstLoan\":-1,\"loanTenantCount\":0,\"averageLoanGapDays\":-1,\"averageTenantGapDays\":-1,\"averageLoanAmount\":0,\"loanCount\":0,\"maxLoanAmount\":0,\"overdueTenantCount\":-1,\"daysFromLastLoan\":-1,\"queryCount\":0,\"monthsFromLastOverdue\":-1,\"maxLoanPeriodDays\":0,\"remainingAmount\":-1,\"monthsForNormalRepay\":-1,\"overdueLoanCount\":-1,\"overdueFor2TermTenantCount\":-1,\"timeScope\":\"D360\"&#125;|&#123;\"maxOverdueDays\":-1,\"loanTenantCount\":0,\"averageLoanGapDays\":-1,\"averageTenantGapDays\":-1,\"averageLoanAmount\":0,\"overdueLoanCount\":-1,\"overdueFor2TermTenantCount\":-1,\"loanCount\":0,\"overdueTenantCount\":-1,\"maxLoanAmount\":0,\"queryCount\":0,\"maxLoanPeriodDays\":0,\"timeScope\":\"D90\"&#125; 3. æœ€ç»ˆé€šè¿‡åˆ†éš”ç¬¦è¿›è¡Œåˆ‡åˆ†å¯¹äºæ¶‰åŠåˆ°åˆ†éš”ç¬¦ï¼Œè½¬ä¹‰å­—ç¬¦çš„ä¸ªæ•°ï¼Œè¯·å‚è€ƒè¯¥æ–‡ç« æ•°ä»“-è§£å†³hiveå¤„ç†å¼‚å¸¸jsonå‘½ä»¤è¡Œè½¬ä¹‰å­—ç¬¦çš„é—®é¢˜ 1234567891011121314select a.apply_risk_id, a.dp_data_id, c.*, from_unixtime(cast(a.timestamp/1000 as bigint),'yyyy-MM-dd HH:mm:ss') as morpho_created_at, current_timestamp() as etl_timefrom (select td.*,concat(regexp_replace(get_json_object(td.data,\"$.data.loanInfo.mobile.timeScopes.D360\"),'&#125;',',\"timeScope\":\"D360\"&#125;'),\"|\",regexp_replace(get_json_object(td.data,\"$.data.loanInfo.mobile.timeScopes.D90\"),'&#125;',',\"timeScope\":\"D90\"&#125;')) ts from dwb.dwb_r_thrid_data td where channel_name ='morpho' and interface_name ='query' and td.dt='20190218' limit 5 ) alateral view explode(split(a.ts,'\\\\\\\\|')) b as listlateral view default.json_tuple2(b.list,'maxOverdueDays','loanTenantCount','monthsFromFirstLoan','averageLoanGapDays','averageLoanAmount','averageTenantGapDays','loanCount','maxLoanAmount','daysFromLastLoan','overdueTenantCount','queryCount','monthsFromLastOverdue','maxLoanPeriodDays','remainingAmount','monthsForNormalRepay','overdueLoanCount','overdueFor2TermTenantCount','timeScope') c as maxOverdueDays,loanTenantCount,monthsFromFirstLoan,averageLoanGapDays,averageLoanAmount,averageTenantGapDays,loanCount,maxLoanAmount,daysFromLastLoan,overdueTenantCount,queryCount,monthsFromLastOverdue,maxLoanPeriodDays,remainingAmount,monthsForNormalRepay,overdueLoanCount,overdueFor2TermTenantCount,timeScope æå®š å®Œæ•´æ ·ä¾‹123456789101112131415161718192021dt=$1hive&lt;&lt;!set mapreduce.job.queuename=root.dw;set hive.support.concurrency=false;insert overwrite table dwb.dwb_r_morpho_loaninfo_mobile partition(dt='$dt')select a.apply_risk_id, a.dp_data_id, c.*, from_unixtime(cast(a.timestamp/1000 as bigint),'yyyy-MM-dd HH:mm:ss') as morpho_created_at, current_timestamp() as etl_timefrom (select td.*,concat(regexp_replace(get_json_object(td.data,\"$.data.loanInfo.mobile.timeScopes.D360\"),'&#125;',',\"timeScope\":\"D360\"&#125;'),\"|\",regexp_replace(get_json_object(td.data,\"$.data.loanInfo.mobile.timeScopes.D90\"),'&#125;',',\"timeScope\":\"D90\"&#125;')) ts from dwb.dwb_r_thrid_data td where channel_name ='morpho' and interface_name ='query' ) alateral view explode(split(a.ts,'\\\\\\\\|')) b as listlateral view default.json_tuple2(b.list,'maxOverdueDays','loanTenantCount','monthsFromFirstLoan','averageLoanGapDays','averageLoanAmount','averageTenantGapDays','loanCount','maxLoanAmount','daysFromLastLoan','overdueTenantCount','queryCount','monthsFromLastOverdue','maxLoanPeriodDays','remainingAmount','monthsForNormalRepay','overdueLoanCount','overdueFor2TermTenantCount','timeScope') c as maxOverdueDays,loanTenantCount,monthsFromFirstLoan,averageLoanGapDays,averageLoanAmount,averageTenantGapDays,loanCount,maxLoanAmount,daysFromLastLoan,overdueTenantCount,queryCount,monthsFromLastOverdue,maxLoanPeriodDays,remainingAmount,monthsForNormalRepay,overdueLoanCount,overdueFor2TermTenantCount,timeScope;! å‚è€ƒï¼šHIVE: lateral view explode &amp; json_turpe å®ç° jsonæ•°ç»„è¡Œè½¬åˆ—&amp;å­—æ®µæ‹†åˆ†","categories":[{"name":"å®é™…é—®é¢˜","slug":"å®é™…é—®é¢˜","permalink":"http://gangtieguo.cn/categories/å®é™…é—®é¢˜/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://gangtieguo.cn/tags/Hive/"},{"name":"æ•°ä»“","slug":"æ•°ä»“","permalink":"http://gangtieguo.cn/tags/æ•°ä»“/"}]},{"title":"macä¸virtualboxå…±äº«æ–‡ä»¶å¤¹ï¼ŒæŒ‚è½½åˆ°dockerå®¹å™¨ä¸­æ–‡ä»¶ç›®å½•æƒé™è§£å†³","slug":"macä¸virtualboxå…±äº«æ–‡ä»¶å¤¹ï¼ŒæŒ‚è½½åˆ°dockerå®¹å™¨ä¸­æ–‡ä»¶ç›®å½•æƒé™è§£å†³","date":"2019-03-12T01:49:35.656Z","updated":"2019-06-17T04:40:09.392Z","comments":true,"path":"2019/03/12/macä¸virtualboxå…±äº«æ–‡ä»¶å¤¹ï¼ŒæŒ‚è½½åˆ°dockerå®¹å™¨ä¸­æ–‡ä»¶ç›®å½•æƒé™è§£å†³/","link":"","permalink":"http://gangtieguo.cn/2019/03/12/macä¸virtualboxå…±äº«æ–‡ä»¶å¤¹ï¼ŒæŒ‚è½½åˆ°dockerå®¹å™¨ä¸­æ–‡ä»¶ç›®å½•æƒé™è§£å†³/","excerpt":"[TOC] å…±äº«æ–‡ä»¶å¤¹çš„æƒé™é—®é¢˜å‚è€ƒï¼šVirtualboxè®¾ç½®å…±äº«æ–‡ä»¶å¤¹ ç”±äºå…±äº«æ–‡ä»¶å¤¹å¹¶ä¸æ˜¯è™šæ‹Ÿæœºçš„æœ¬åœ°ç›®å½•ï¼Œæˆ‘ä»¬åœ¨è™šæ‹Ÿæœºä¸­å¯ä»¥é…ç½®å…±äº«æ–‡ä»¶å¤¹çš„æƒé™æ˜¯æœ‰é™çš„ã€‚ æ‰‹åŠ¨æŒ‚è½½æˆ–è‡ªåŠ¨æŒ‚è½½çš„ç›®å½•ï¼Œæ‰€å±ç”¨æˆ·é»˜è®¤ä¸ºrootï¼Œç»„ä¸ºvboxsfï¼Œå¹¶ä¸”ä½¿ç”¨ chmod chown ç­‰å‘½ä»¤æ˜¯æ— æ³•æ”¹å˜çš„ã€‚","text":"[TOC] å…±äº«æ–‡ä»¶å¤¹çš„æƒé™é—®é¢˜å‚è€ƒï¼šVirtualboxè®¾ç½®å…±äº«æ–‡ä»¶å¤¹ ç”±äºå…±äº«æ–‡ä»¶å¤¹å¹¶ä¸æ˜¯è™šæ‹Ÿæœºçš„æœ¬åœ°ç›®å½•ï¼Œæˆ‘ä»¬åœ¨è™šæ‹Ÿæœºä¸­å¯ä»¥é…ç½®å…±äº«æ–‡ä»¶å¤¹çš„æƒé™æ˜¯æœ‰é™çš„ã€‚ æ‰‹åŠ¨æŒ‚è½½æˆ–è‡ªåŠ¨æŒ‚è½½çš„ç›®å½•ï¼Œæ‰€å±ç”¨æˆ·é»˜è®¤ä¸ºrootï¼Œç»„ä¸ºvboxsfï¼Œå¹¶ä¸”ä½¿ç”¨ chmod chown ç­‰å‘½ä»¤æ˜¯æ— æ³•æ”¹å˜çš„ã€‚ å¦‚æœæƒ³è¦é…ç½®æŒ‚è½½ç›®å½•çš„æƒé™ï¼Œéœ€è¦åœ¨æ‰‹åŠ¨æŒ‚è½½çš„æ—¶å€™æŒ‡å®šä¸€äº›é€‰é¡¹ï¼š 12345// uid gidæŒ‡å®šæŒ‚è½½ç›®å½•çš„æ‰€å±ç”¨æˆ·å’Œç»„sudo mount -t vboxsf -o uid=500,gid=500 &lt;folder name given in VirtualBox&gt;// fmodeæŒ‡å®šæ–‡ä»¶æƒé™ï¼ŒdmodeæŒ‡å®šç›®å½•æƒé™// æ³¨æ„ï¼Œè‹¥åŒæ—¶æŒ‡å®šæŒ‚è½½ç›®å½•çš„æ‰€å±ç”¨æˆ·å’Œç»„ï¼Œåˆ™fmodeå’Œdmodeé€‰é¡¹å¤±æ•ˆsudo mount -t vboxsf -o fmode=700,dmode=700 &lt;folder name given in VirtualBox&gt; æˆ‘ä½¿ç”¨çš„å‘½ä»¤ä¸ºï¼š 1sudo mount -t vboxsf -o uid=500,gid=500 Y /Users/yaosong/Yao æ­¤å¤„Yä¸ºæˆ‘è®¾ç½®çš„å…±äº«è·¯å¾„ è¸©è¿‡çš„å‘ï¼š ä¹‹å‰ç”±äºåœ¨æŒ‚è½½æ—¶ï¼Œä½¿ç”¨çš„ sudo mount -t vboxsf Yao /Users/yaosong/Yao å¹¶æœªè®¾ç½®æ‰€å±ç”¨æˆ·ç»„åŠæ‰€å±ç”¨æˆ·ï¼Œç„¶ååœ¨è™šæ‹Ÿæœºä¸­ä½¿ç”¨chownï¼Œæ— è®ºå¦‚ä½•éƒ½æœªæˆåŠŸï¼Œ çœ‹åˆ°ä¸Šä¸€ç¯‡åšæ–‡ï¼ŒèŒ…å¡é¡¿å¼€ ä½¿ç”¨sudo mount -t vboxsf -o uid=500,gid=500 Y /Users/yaosong/Yaoè¿‡å ç”±äºelkç”¨æˆ·å¯¹åº”ç”¨æˆ·idï¼Œç”¨æˆ·ç»„idä¸º500ï¼Œ å¯ä»¥å¾—åˆ°ä¸€ä¸‹æ‰€å±ç”¨æˆ·å’Œç”¨æˆ·ç»„ï¼š æ­¤å¤„è®¾ç½®ç”¨æˆ·idå’Œç»„idï¼Œä¸‹æ–‡ä¹Ÿè¦ç”¨åˆ° æŒ‚è½½åˆ°å®¹å™¨çš„æ–‡ä»¶ç›®å½•ä¸å®¿ä¸»æœºçš„è®¾ç½®å…³ç³» å‚è€ƒï¼šå…³äºDockerç›®å½•æŒ‚è½½çš„æ€»ç»“ï¼ŒDocker Volume - ç›®å½•æŒ‚è½½ä»¥åŠæ–‡ä»¶å…±äº« ä¹‹æ‰€ä»¥è®¾ç½®å…±äº«æ–‡ä»¶å¤¹æƒé™æ—¶è®¾ç½®ç”¨æˆ·idå’Œç»„idï¼Œè¿˜æœ‰ä¸€ä¸ªé‡è¦åŸå› ï¼š â€‹ åŸæ¥ï¼Œå®¿ä¸»ä¸å®¹å™¨çš„UIDæœ‰å…³ç³»ï¼ŒUIDï¼Œå³â€œç”¨æˆ·æ ‡è¯†å·â€ï¼Œæ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œç³»ç»Ÿå†…éƒ¨ç”¨å®ƒæ¥æ ‡è¯†ç”¨æˆ·ã€‚ä¸€èˆ¬æƒ…å†µä¸‹å®ƒä¸ç”¨æˆ·åæ˜¯ä¸€ä¸€å¯¹åº”çš„ã€‚å³ä¸ºï¼šåœ¨å®¿ä¸»ä¸­UIDä¸ºå¤šå°‘ï¼Œé‚£ä¹ˆåœ¨å®¹å™¨ä¸­çš„æ‰€å±ç”¨æˆ·æ‰€å±ç»„å°±ä¸ºå®¹å™¨ä¸­å’Œå®¿ä¸»UIDç›¸å¯¹åº”çš„ç”¨æˆ·ã€‚ å¯åŠ¨å®¹å™¨æ—¶æŒ‚è½½ç›®å½• 1docker run -itd --net=br --name elk1 --privileged=true -v /Users/yaosong/Yao/share/source/es1:/usr/es --hostname elk1 --ip=192.168.33.16 yaosong5/elk:1.0 &amp;&gt; /dev/null å¯çŸ¥å®¿ä¸»æœº/Users/yaosong/Yao/share/source/es1ç›®å½•æŒ‚è½½åˆ°å®¹å™¨/usr/esä¸­ â€‹ åœ¨å®¹å™¨å†…è®¾ç½®æƒé™ï¼š 1chown -R elk.elk $ES_HOME æ³¨æ„æ˜¯ç‚¹ä¸æ˜¯å†’å·ï¼ŒåŒºåˆ«æˆ‘è¿˜æ²¡ç ”ç©¶ æŸ¥çœ‹ç»“æœ å‚è€ƒæ–‡ç« Virtualboxè®¾ç½®å…±äº«æ–‡ä»¶å¤¹ å…³äºDockerç›®å½•æŒ‚è½½çš„æ€»ç»“ Docker Volume - ç›®å½•æŒ‚è½½ä»¥åŠæ–‡ä»¶å…±äº«","categories":[{"name":"é—®é¢˜è§£å†³","slug":"é—®é¢˜è§£å†³","permalink":"http://gangtieguo.cn/categories/é—®é¢˜è§£å†³/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"},{"name":"Mac","slug":"Mac","permalink":"http://gangtieguo.cn/tags/Mac/"}]},{"title":"Docker-machineçš„åˆ›å»ºï¼Œmacå®¿ä¸»æœºå’Œdockerå®¹å™¨ç½‘ç»œäº’é€šDockerå®¹å™¨ä¸å®¿ä¸»æœºåœ¨åŒä¸€ipæ®µä¸‹","slug":"Docker-machineçš„æ­å»º(ä¸å®¿ä¸»æœºåœ¨åŒä¸€ipæ®µä¸‹)","date":"2019-03-05T08:14:31.000Z","updated":"2019-06-17T04:40:09.360Z","comments":true,"path":"2019/03/05/Docker-machineçš„æ­å»º(ä¸å®¿ä¸»æœºåœ¨åŒä¸€ipæ®µä¸‹)/","link":"","permalink":"http://gangtieguo.cn/2019/03/05/Docker-machineçš„æ­å»º(ä¸å®¿ä¸»æœºåœ¨åŒä¸€ipæ®µä¸‹)/","excerpt":"æ­¤æ–‡çº¯å±å‘½ä»¤è®°å½•ï¼Œåç»­æ›´æ–°åŸç†è§£è¯´","text":"æ­¤æ–‡çº¯å±å‘½ä»¤è®°å½•ï¼Œåç»­æ›´æ–°åŸç†è§£è¯´ æ›´æ”¹virtual0çš„ip VBoxManage hostonlyif ipconfig vboxnet0 â€“ip 192.168.33.253 â€“netmask 255.255.255.0 ifconfig æŸ¥çœ‹åˆ›å»ºè™šæ‹Ÿæœºé…ç½®æ–‡ä»¶ Vagrantfile ä¹Ÿå¯ä»¥vagrant init ä¼šç”Ÿæˆä¸€ä¸ªç©ºç™½çš„Vagrantfile vi Vagrantfile1234567891011121314151617181920212223Vagrant.configure(2) do |config| config.vm.box = \"dolbager/centos-7-docker\" config.vm.hostname = \"default\" config.vm.network \"private_network\", ip: \"192.168.33.1\",netmask: \"255.255.255.0\" config.vm.provider \"virtualbox\" do |v| v.name = \"default\" v.memory = \"2048\" # Change the network adapter type and promiscuous mode v.customize ['modifyvm', :id, '--nictype1', 'Am79C973'] v.customize ['modifyvm', :id, '--nicpromisc1', 'allow-all'] v.customize ['modifyvm', :id, '--nictype2', 'Am79C973'] v.customize ['modifyvm', :id, '--nicpromisc2', 'allow-all'] end # Install bridge-utils config.vm.provision \"shell\", inline: &lt;&lt;-SHELL curl -o /etc/yum.repos.d/CentOS-Base.repohttp://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel.repohttp://mirrors.aliyun.com/repo/epel-7.repo yum clean all yum makecache yum update -y yum install bridge-utils net-tools -y SHELLend vagrant upvagrant ssh vagrant ssh-config 1scp ~/.vagrant.d/boxes/dolbager-VAGRANTSLASH-centos-7-docker/0.2/virtualbox/vagrant_private_key .vagrant/machines/default/virtualbox/private_key vagrant exit 1234567docker-machine create \\ --driver \"generic\" \\ --generic-ip-address 192.168.33.1 \\ --generic-ssh-user vagrant \\ --generic-ssh-key .vagrant/machines/default/virtualbox/private_key \\ --generic-ssh-port 22 \\ default åˆ›å»ºç½‘æ¡¥docker1 å’Œ docker network bré€šè¿‡vagrant ä»è™šæ‹Ÿæœºçš„ eth0 ç™»å½•åˆ°è™šæ‹Ÿæœº vagrant sship -4 addr åˆ›å»º docker network br 123456789sudo docker network create \\ --driver bridge \\ --subnet=192.168.33.0/24 \\ --gateway=192.168.33.1 \\ --opt \"com.docker.network.bridge.enable_icc\"=\"true\" \\ --opt \"com.docker.network.bridge.enable_ip_masquerade\"=\"true\" \\ --opt \"com.docker.network.bridge.name\"=\"docker1\" \\ --opt \"com.docker.network.driver.mtu\"=\"1500\" \\ br åˆ›å»ºç½‘æ¡¥é…ç½®æ–‡ä»¶docker1 vim /etc/sysconfig/network-scripts/ifcfg-docker1 123456789DEVICE=docker1TYPE=BridgeBOOTPROTO=staticONBOOT=yesSTP=onIPADDR=NETMASK=GATEWAY=DNS1= ä¿®æ”¹ç½‘å¡é…ç½® eth1 :sudo vi /etc/sysconfig/network-scripts/ifcfg-eth1 12345678DEVICE=eth1BOOTPROTO=staticHWADDR=ONBOOT=yesNETMASK=GATEWAY=BRIDGE=docker1TYPE=Ethernet ip -4 addr åˆå§‹åŒ–docker-machineå˜é‡ æç¤ºæŠ¥é”™Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? 123eval $(docker-machine env default) å¦‚æœè¦å–æ¶ˆå˜é‡ eval $(docker-machine env -u) å‚è€ƒï¼šdocker-install-mac-vm-centos","categories":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"},{"name":"Docker-machine","slug":"Docker-machine","permalink":"http://gangtieguo.cn/tags/Docker-machine/"},{"name":"å®‰è£…éƒ¨ç½²","slug":"å®‰è£…éƒ¨ç½²","permalink":"http://gangtieguo.cn/tags/å®‰è£…éƒ¨ç½²/"}]},{"title":"ELK-Logstash->kafka->esæµç¨‹å®ä¾‹","slug":"ELKæµç¨‹å°å®ä¾‹","date":"2018-09-08T14:30:24.772Z","updated":"2019-06-17T04:40:09.374Z","comments":true,"path":"2018/09/08/ELKæµç¨‹å°å®ä¾‹/","link":"","permalink":"http://gangtieguo.cn/2018/09/08/ELKæµç¨‹å°å®ä¾‹/","excerpt":"elkæµç¨‹å®ä¾‹ [TOC] å‰è¨€ï¼šä¸€èˆ¬æ—¥å¿—æ”¶é›†ï¼Œæˆ‘ä»¬é€šè¿‡logstashæ¥æ¥å…¥ï¼Œå†™å…¥åˆ°kafkaï¼Œå†é€šè¿‡logstashå°†kafkaçš„æ•°æ®å†™å…¥åˆ°es é‚£ä¹ˆå°±é€šè¿‡ä¸€ä¸ªèœçš„æŠ è„šçš„å®ä¾‹æ¥çœ‹çœ‹ LOGSTASH-&gt;KAFKA-&gt;ES1.å¯åŠ¨logstashå°†æ—¥å¿—å†™å…¥åˆ°kafkaâ‘ å‡†å¤‡å·¥ä½œåˆ›å»ºtopicå…ˆåˆ›å»ºå¯¹åº”topicï¼Œï¼ˆæŒ‰ç…§æœºå™¨é…ç½®ï¼Œé›†ç¾¤æƒ…å†µé…ç½®ï¼Œä¸ç„¶é»˜è®¤åˆ›å»ºçš„è¯ï¼Œä¸æ˜¯æœ€ä¼˜æ•ˆæœï¼‰","text":"elkæµç¨‹å®ä¾‹ [TOC] å‰è¨€ï¼šä¸€èˆ¬æ—¥å¿—æ”¶é›†ï¼Œæˆ‘ä»¬é€šè¿‡logstashæ¥æ¥å…¥ï¼Œå†™å…¥åˆ°kafkaï¼Œå†é€šè¿‡logstashå°†kafkaçš„æ•°æ®å†™å…¥åˆ°es é‚£ä¹ˆå°±é€šè¿‡ä¸€ä¸ªèœçš„æŠ è„šçš„å®ä¾‹æ¥çœ‹çœ‹ LOGSTASH-&gt;KAFKA-&gt;ES1.å¯åŠ¨logstashå°†æ—¥å¿—å†™å…¥åˆ°kafkaâ‘ å‡†å¤‡å·¥ä½œåˆ›å»ºtopicå…ˆåˆ›å»ºå¯¹åº”topicï¼Œï¼ˆæŒ‰ç…§æœºå™¨é…ç½®ï¼Œé›†ç¾¤æƒ…å†µé…ç½®ï¼Œä¸ç„¶é»˜è®¤åˆ›å»ºçš„è¯ï¼Œä¸æ˜¯æœ€ä¼˜æ•ˆæœï¼‰ 1$KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper zk1:2181,zk2:2181,zk3:2181 --replication-factor 2 --partitions 3 --topic gamekafka æŸ¥çœ‹topicæ˜¯å¦åˆ›å»ºæˆåŠŸ 1$KAFKA_HOME/bin/kafka-topics.sh --list --zookeeper zk1:2181,zk2:2181,zk3:2181 â‘¡ç¼–è¾‘å†™å…¥kafkaçš„logstashé…ç½®æ–‡ä»¶ vim $LOGSTASH_HOME/conf/logstash-game-kafka.conf 123456789101112131415161718192021222324input &#123; file &#123; codec=&gt;plain &#123; charset =&gt; &quot;UTF-8&quot; &#125; #è¿™æ˜¯æ—¥å¿—çš„è·¯å¾„ path =&gt; &quot;/BaseDir/2016-02-01/*.txt&quot; discover_interval =&gt; 5 start_position =&gt; &quot;beginning&quot; &#125;&#125;output &#123; kafka &#123; topic_id =&gt; &quot;gamekafka&quot; codec =&gt; plain &#123; format =&gt; &quot;%&#123;message&#125;&quot; charset =&gt; &quot;UTF-8&quot; &#125; #kafkaçš„åœ°å€ï¼Œç«¯å£ä¸º9092 bootstrap_servers =&gt; &quot;kafka1:9092,kafka2:9092,kafka3:9092&quot; &#125; stdout &#123;codec =&gt; rubydebug&#125;&#125; â‘¢å¯åŠ¨å°†æ–‡ä»¶å†™å…¥kafkaçš„logstash1234567891011$LOGSTASH_HOME/bin/logstash -f $LOGSTASH_HOME/conf/logstash-game-kafka.conf#å°è£…è¿‡ä¸€ä¸ªå°è„šæœ¬logstash-start.sh#!/bin/bashCONF_PATH=/usr/logstash/conf/$1echo $CONF_PATHnohup $LOGSTASH_HOME/bin/logstash -f $CONF_PATH &amp;# é‚£ä¹ˆæ‰§è¡Œå‘½ä»¤æ¢æˆsh logstash-start.sh logstash-game-kafka.conf è‹¥å¯åŠ¨æˆåŠŸ æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶æ˜¯å¦å†™å…¥äº†kafka1$KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server &quot;kafka1:9092,kafka2:9092,kafka3:9092&quot; --topic gamekafka --from-beginning --group testGroup è‹¥logstashæŠ¥é”™è‹¥æµ‹è¯•æŠ¥é”™å†…å­˜ä¸å¤Ÿï¼Œå‚è€ƒ elasticsearch6.2å’Œlogstashå¯åŠ¨å‡ºç°çš„é”™è¯¯ åœ¨eså’Œlogstashçš„é…ç½®ç›®å½•jvm.optionsä¸­è®¾ç½®æ›´å°å†…å­˜ 12-Xms400m -Xmx400m 2.å°†kafkaä¸­çš„æ•°æ®é€šè¿‡logstashå†™å…¥åˆ°eså½“ç„¶è¦å…ˆå¯åŠ¨eså•¦ï¼ˆæ­¤å¤„çœç•¥ï¼‰ 1$ES_HOME/bin/elasticsearch -d â‘ ç¼–è¾‘å†™å…¥åˆ°esçš„logstashé…ç½®æ–‡ä»¶é…ç½®æ–‡ä»¶ vim $LOGSTASH_HOME/conf/logstash-game-kafka-es.conf 1234567891011121314151617181920212223242526272829303132333435363738input &#123; kafka &#123; codec =&gt; &quot;plain&quot; group_id =&gt; &quot;es2&quot; bootstrap_servers =&gt; [&quot;kafka1:9092,kafka2:9092,kafka3:9092&quot;] # æ³¨æ„è¿™é‡Œé…ç½®çš„kafkaçš„brokeråœ°å€ä¸æ˜¯zkçš„åœ°å€ auto_offset_reset =&gt; &quot;earliest&quot; topics =&gt; [&quot;gamekafka&quot;] &#125;&#125;filter &#123; mutate &#123; split =&gt; &#123; &quot;message&quot; =&gt; &quot; &quot; &#125; add_field =&gt; &#123; &quot;event_type&quot; =&gt; &quot;%&#123;message[3]&#125;&quot; &quot;current_map&quot; =&gt; &quot;%&#123;message[4]&#125;&quot; &quot;current_X&quot; =&gt; &quot;%&#123;message[5]&#125;&quot; &quot;current_y&quot; =&gt; &quot;%&#123;message[6]&#125;&quot; &quot;user&quot; =&gt; &quot;%&#123;message[7]&#125;&quot; &quot;item&quot; =&gt; &quot;%&#123;message[8]&#125;&quot; &quot;item_id&quot; =&gt; &quot;%&#123;message[9]&#125;&quot; &quot;current_time&quot; =&gt; &quot;%&#123;message[12]&#125;&quot; &#125; remove_field =&gt; [ &quot;message&quot; ] &#125;&#125;output &#123; elasticsearch &#123; index =&gt; &quot;gamelogs&quot; codec =&gt; plain &#123; charset =&gt; &quot;UTF-8&quot; &#125; hosts =&gt; [&quot;elk1:9200&quot;, &quot;elk2:9200&quot;, &quot;elk3:9200&quot;] &#125; stdout &#123;codec =&gt; rubydebug&#125;&#125; â‘¡å¯åŠ¨å†™å…¥åˆ°esçš„logstash1$LOGSTASH_HOME/bin/logstash -f $LOGSTASH_HOME/conf/logstash-game-kafka-es.conf æŸ¥çœ‹esé‡Œé¢æ˜¯å¦æœ‰æ•°æ® å¦‚æœæ˜¯ä»‹ä¸ªæ ·å­ï¼Œé‚£ä¹ˆå¤§åŠŸå‘Šæˆ ä¹Ÿå¯ä»¥ç›´æ¥ä»logstash-&gt;esç”±äºlogstashçš„outputå½¢å¼å¤šæ ·ï¼Œä¹Ÿå¯ç›´æ¥é€šè¿‡logstashå°†æ—¥å¿—æ•°æ®å†™å…¥åˆ°eså½“ä¸­ â‘ é…ç½®æ–‡ä»¶vim logstash-game-file-es.conf 1234567891011121314151617181920212223242526272829303132333435363738input &#123; file &#123; codec=&gt;plain &#123; charset =&gt; &quot;UTF-8&quot; &#125; path =&gt; &quot;/BaseDir/2016-02-01/*.txt&quot; discover_interval =&gt; 5 start_position =&gt; &quot;beginning&quot; &#125;&#125;filter &#123; mutate &#123; split =&gt; &#123; &quot;message&quot; =&gt; &quot; &quot; &#125; add_field =&gt; &#123; &quot;event_type&quot; =&gt; &quot;%&#123;message[3]&#125;&quot; &quot;current_map&quot; =&gt; &quot;%&#123;message[4]&#125;&quot; &quot;current_X&quot; =&gt; &quot;%&#123;message[5]&#125;&quot; &quot;current_y&quot; =&gt; &quot;%&#123;message[6]&#125;&quot; &quot;user&quot; =&gt; &quot;%&#123;message[7]&#125;&quot; &quot;item&quot; =&gt; &quot;%&#123;message[8]&#125;&quot; &quot;item_id&quot; =&gt; &quot;%&#123;message[9]&#125;&quot; &quot;current_time&quot; =&gt; &quot;%&#123;message[12]&#125;&quot; &#125; remove_field =&gt; [ &quot;message&quot; ] &#125;&#125;output &#123; elasticsearch &#123; index =&gt; &quot;kafkagamelogs&quot; codec =&gt; plain &#123; charset =&gt; &quot;UTF-8&quot; &#125; hosts =&gt; [&quot;elk1:9200&quot;, &quot;elk2:9200&quot;, &quot;elk3:9200&quot;] &#125; stdout &#123;codec =&gt; rubydebug&#125;&#125; â‘¡å¯åŠ¨logstash1$LOGSTASH_HOME/bin/logstash -f $LOGSTASH_HOME/conf/logstash-game-file-es.conf","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://gangtieguo.cn/tags/ELK/"}]},{"title":"DataStream DataSetä»‹ç»","slug":"DataFrame DataSet","date":"2018-09-05T17:20:33.287Z","updated":"2018-12-25T03:51:31.046Z","comments":true,"path":"2018/09/06/DataFrame DataSet/","link":"","permalink":"http://gangtieguo.cn/2018/09/06/DataFrame DataSet/","excerpt":"[TOC] ä»€ä¹ˆæ˜¯DataStreamDiscretized Streamæ˜¯Spark Streamingçš„åŸºç¡€æŠ½è±¡ï¼Œä»£è¡¨æŒç»­æ€§çš„æ•°æ®æµå’Œç»è¿‡å„ç§SparkåŸè¯­æ“ä½œåçš„ç»“æœæ•°æ®æµã€‚åœ¨å†…éƒ¨å®ç°ä¸Šï¼ŒDStreamæ˜¯ä¸€ç³»åˆ—è¿ç»­çš„RDDæ¥è¡¨ç¤ºã€‚æ¯ä¸ªRDDå«æœ‰ä¸€æ®µæ—¶é—´é—´éš”å†…çš„æ•°æ®ï¼Œå¦‚ä¸‹å›¾ï¼š","text":"[TOC] ä»€ä¹ˆæ˜¯DataStreamDiscretized Streamæ˜¯Spark Streamingçš„åŸºç¡€æŠ½è±¡ï¼Œä»£è¡¨æŒç»­æ€§çš„æ•°æ®æµå’Œç»è¿‡å„ç§SparkåŸè¯­æ“ä½œåçš„ç»“æœæ•°æ®æµã€‚åœ¨å†…éƒ¨å®ç°ä¸Šï¼ŒDStreamæ˜¯ä¸€ç³»åˆ—è¿ç»­çš„RDDæ¥è¡¨ç¤ºã€‚æ¯ä¸ªRDDå«æœ‰ä¸€æ®µæ—¶é—´é—´éš”å†…çš„æ•°æ®ï¼Œå¦‚ä¸‹å›¾ï¼š ä¸RDDç±»ä¼¼ï¼ŒDataFrameä¹Ÿæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼æ•°æ®å®¹å™¨ã€‚ç„¶è€ŒDataFrameæ›´åƒä¼ ç»Ÿæ•°æ®åº“çš„äºŒç»´è¡¨æ ¼ï¼Œé™¤äº†æ•°æ®ä»¥å¤–ï¼Œè¿˜è®°å½•æ•°æ®çš„ç»“æ„ä¿¡æ¯ï¼Œå³schemaã€‚åŒæ—¶ï¼Œä¸Hiveç±»ä¼¼ï¼ŒDataFrameä¹Ÿæ”¯æŒåµŒå¥—æ•°æ®ç±»å‹ï¼ˆstructã€arrayå’Œmapï¼‰ã€‚ä»APIæ˜“ç”¨æ€§çš„è§’åº¦ä¸Š çœ‹ï¼ŒDataFrame APIæä¾›çš„æ˜¯ä¸€å¥—é«˜å±‚çš„å…³ç³»æ“ä½œï¼Œæ¯”å‡½æ•°å¼çš„RDD APIè¦æ›´åŠ å‹å¥½ï¼Œé—¨æ§›æ›´ä½ã€‚ç”±äºä¸Rå’ŒPandasçš„DataFrameç±»ä¼¼ï¼ŒSpark DataFrameå¾ˆå¥½åœ°ç»§æ‰¿äº†ä¼ ç»Ÿå•æœºæ•°æ®åˆ†æçš„å¼€å‘ä½“éªŒã€‚ å¯¹æ•°æ®çš„æ“ä½œä¹Ÿæ˜¯æŒ‰ç…§RDDä¸ºå•ä½æ¥è¿›è¡Œçš„ è®¡ç®—è¿‡ç¨‹ç”±Spark engineæ¥å®Œæˆ Datasets ä¸DataFrames ä¸RDDsçš„å…³ç³» Sparkå¼•å…¥DataFrameï¼Œå®ƒå¯ä»¥æä¾›high-level functionsè®©Sparkæ›´å¥½çš„å¤„ç†ç»“æ„æ•°æ®çš„è®¡ç®—ã€‚è¿™è®©Catalyst optimizer å’ŒTungstenï¼ˆé’¨ä¸ï¼‰ execution engineè‡ªåŠ¨åŠ é€Ÿå¤§æ•°æ®åˆ†æã€‚å‘å¸ƒDataFrameä¹‹åå¼€å‘è€…æ”¶åˆ°äº†å¾ˆå¤šåé¦ˆï¼Œå…¶ä¸­ä¸€ä¸ªä¸»è¦çš„æ˜¯å¤§å®¶åæ˜ ç¼ºä¹ç¼–è¯‘æ—¶ç±»å‹å®‰å…¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒSparké‡‡ç”¨æ–°çš„Dataset API (DataFrame APIçš„ç±»å‹æ‰©å±•)ã€‚Dataset APIæ‰©å±•DataFrame APIæ”¯æŒé™æ€ç±»å‹å’Œè¿è¡Œå·²ç»å­˜åœ¨çš„Scalaæˆ–Javaè¯­è¨€çš„ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ã€‚å¯¹æ¯”ä¼ ç»Ÿçš„RDD APIï¼ŒDataset APIæä¾›æ›´å¥½çš„å†…å­˜ç®¡ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿ä»»åŠ¡ä¸­æœ‰æ›´å¥½çš„æ€§èƒ½æå‡ DataStreamç›¸å…³æ“ä½œDStreamä¸Šçš„åŸè¯­ä¸RDDçš„ç±»ä¼¼ï¼Œåˆ†ä¸ºTransformationsï¼ˆè½¬æ¢ï¼‰å’ŒOutputOperationsï¼ˆè¾“å‡ºï¼‰ä¸¤ç§ï¼Œæ­¤å¤–è½¬æ¢æ“ä½œä¸­è¿˜æœ‰ä¸€äº›æ¯”è¾ƒç‰¹æ®Šçš„åŸè¯­ï¼Œå¦‚ï¼šupdateStateByKey()ã€transform()ä»¥åŠå„ç§Windowç›¸å…³çš„åŸè¯­ã€‚ Transformations on DStreams Transformation Meaning map(func) Return a new DStream by passing each element of the source DStream through a function func. flatMap(func) Similar to map, but each input item can be mapped to 0 or more output items. filter(func) Return a new DStream by selecting only the records of the source DStream on which func returns true. repartition(numPartitions) Changes the level of parallelism in this DStream by creating more or fewer partitions. union(otherStream) Return a new DStream that contains the union of the elements in the source DStream and otherDStream. count() Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream. reduce(func) Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func (which takes two arguments and returns one). The function should be associative so that it can be computed in parallel. countByValue() When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream. reduceByKey(func, [numTasks]) When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Sparkâ€™s default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks. join(otherStream, [numTasks]) When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key. cogroup(otherStream, [numTasks]) When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples. transform(func) Return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream. This can be used to do arbitrary RDD operations on the DStream. updateStateByKey(func) Return a new â€œstateâ€ DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key. ç‰¹æ®Šçš„Transformations UpdateStateByKeyOperation UpdateStateByKeyåŸè¯­ç”¨äºè®°å½•å†å²è®°å½•ï¼Œä¸Šæ–‡ä¸­Word Countç¤ºä¾‹ä¸­å°±ç”¨åˆ°äº†è¯¥ç‰¹æ€§ã€‚è‹¥ä¸ç”¨UpdateStateByKeyæ¥æ›´æ–°çŠ¶æ€ï¼Œé‚£ä¹ˆæ¯æ¬¡æ•°æ®è¿›æ¥ååˆ†æå®Œæˆåï¼Œç»“æœè¾“å‡ºåå°†ä¸åœ¨ä¿å­˜ TransformOperation TransformåŸè¯­å…è®¸DStreamä¸Šæ‰§è¡Œä»»æ„çš„RDD-to-RDDå‡½æ•°ã€‚é€šè¿‡è¯¥å‡½æ•°å¯ä»¥æ–¹ä¾¿çš„æ‰©å±•Spark APIã€‚æ­¤å¤–ï¼ŒMLlibï¼ˆæœºå™¨å­¦ä¹ ï¼‰ä»¥åŠGraphxä¹Ÿæ˜¯é€šè¿‡æœ¬å‡½æ•°æ¥è¿›è¡Œç»“åˆçš„ã€‚ WindowOperations Window Operationsæœ‰ç‚¹ç±»ä¼¼äºStormä¸­çš„Stateï¼Œå¯ä»¥è®¾ç½®çª—å£çš„å¤§å°å’Œæ»‘åŠ¨çª—å£çš„é—´éš”æ¥åŠ¨æ€çš„è·å–å½“å‰Steamingçš„å…è®¸çŠ¶æ€ Output Operations on DStreamsOutput Operationså¯ä»¥å°†DStreamçš„æ•°æ®è¾“å‡ºåˆ°å¤–éƒ¨çš„æ•°æ®åº“æˆ–æ–‡ä»¶ç³»ç»Ÿï¼Œå½“æŸä¸ªOutput OperationsåŸè¯­è¢«è°ƒç”¨æ—¶ï¼ˆä¸RDDçš„Actionç›¸åŒï¼‰ï¼Œstreamingç¨‹åºæ‰ä¼šå¼€å§‹çœŸæ­£çš„è®¡ç®—è¿‡ç¨‹ã€‚ Output Operation Meaning print() Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging. saveAsTextFiles(prefix, [suffix]) Save this DStreamâ€™s contents as text files. The file name at each batch interval is generated based on prefix and suffix: â€œprefix-TIME_IN_MS[.suffix]â€. saveAsObjectFiles(prefix, [suffix]) Save this DStreamâ€™s contents as SequenceFiles of serialized Java objects. The file name at each batch interval is generated based on prefix and suffix: â€œprefix-TIME_IN_MS[.suffix]â€. saveAsHadoopFiles(prefix, [suffix]) Save this DStreamâ€™s contents as Hadoop files. The file name at each batch interval is generated based on prefix and suffix: â€œprefix-TIME_IN_MS[.suffix]â€. foreachRDD(func) The most generic output operator that applies a function, func, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function func is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs. ç”¨Spark Streamingå®ç°å®æ—¶WordCountæ¶æ„å›¾ï¼š 1.å®‰è£…å¹¶å¯åŠ¨ç”Ÿæˆè€… é¦–å…ˆåœ¨ä¸€å°Linuxï¼ˆipï¼š192.168.10.101ï¼‰ä¸Šç”¨YUMå®‰è£…ncå·¥å…· yum install -y nc å¯åŠ¨ä¸€ä¸ªæœåŠ¡ç«¯å¹¶ç›‘å¬9999ç«¯å£ nc -lk 9999 2.ç¼–å†™Spark Streamingç¨‹åº 1234567891011121314151617package me.yao.spark.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object NetworkWordCount &#123; def main(args: Array[String]) &#123; //è®¾ç½®æ—¥å¿—çº§åˆ« LoggerLevel.setStreamingLogLevels() //åˆ›å»ºSparkConfå¹¶è®¾ç½®ä¸ºæœ¬åœ°æ¨¡å¼è¿è¡Œ //æ³¨æ„local[2]ä»£è¡¨å¼€ä¸¤ä¸ªçº¿ç¨‹ val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkWordCount\") //è®¾ç½®DStreamæ‰¹æ¬¡æ—¶é—´é—´éš”ä¸º2ç§’ val ssc = new StreamingContext(conf, Seconds(2)) //é€šè¿‡ç½‘ç»œè¯»å–æ•°æ® val lines = ssc.socketTextStream(\"192.168.10.101\", 9999) //å°†è¯»åˆ°çš„æ•°æ®ç”¨ç©ºæ ¼åˆ‡æˆå•è¯ val words = lines.flatMap(_.split(\" \")) //å°†å•è¯å’Œ1ç»„æˆä¸€ä¸ªpair val pairs = words.map(word =&gt; (word, 1)) //æŒ‰å•è¯è¿›è¡Œåˆ†ç»„æ±‚ç›¸åŒå•è¯å‡ºç°çš„æ¬¡æ•° val wordCounts = pairs.reduceByKey(_ + _) //æ‰“å°ç»“æœåˆ°æ§åˆ¶å° wordCounts.print() //å¼€å§‹è®¡ç®— ssc.start() //ç­‰å¾…åœæ­¢ ssc.awaitTermination() &#125; &#125; é—®é¢˜ï¼šç»“æœæ¯æ¬¡åœ¨Linuxç«¯è¾“å…¥çš„å•è¯æ¬¡æ•°éƒ½è¢«æ­£ç¡®çš„ç»Ÿè®¡å‡ºæ¥ï¼Œä½†æ˜¯ç»“æœä¸èƒ½ç´¯åŠ ï¼å¦‚æœéœ€è¦ç´¯åŠ éœ€è¦ä½¿ç”¨updateStateByKey(func)æ¥æ›´æ–°çŠ¶æ€ï¼Œä¸‹é¢ç»™å‡ºä¸€ä¸ªä¾‹å­ï¼š 12345678910111213141516171819202122232425262728 package me.yao.spark.streaming import org.apache.spark.&#123;HashPartitioner, SparkConf&#125; import org.apache.spark.streaming.&#123;StreamingContext, Seconds&#125; object NetworkUpdateStateWordCount &#123; val updateFunc = (iter: Iterator[(String, Seq[Int], Option[Int])]) =&gt; &#123; //iter.flatMap(it=&gt;Some(it._2.sum + it._3.getOrElse(0)).map(x=&gt;(it._1,x))) iter.flatMap&#123; case(x,y,z)=&gt;Some(y.sum + z.getOrElse(0)).map(m=&gt;(x, m))&#125; &#125; def main(args: Array[String]) &#123; LoggerLevel.setStreamingLogLevels*() val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkUpdateStateWordCount\") val ssc = new StreamingContext(conf, Seconds(5)) //åšcheckpoint å†™å…¥å…±äº«å­˜å‚¨ä¸­ ssc.checkpoint(\"c://aaa\") **val **lines = ssc.socketTextStream(\"192.168.10.100\", 9999) //reduceByKey **ç»“æœä¸ç´¯åŠ  //val result = lines.flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_+_) //updateStateByKeyç»“æœå¯ä»¥ç´¯åŠ ä½†æ˜¯éœ€è¦ä¼ å…¥ä¸€ä¸ªè‡ªå®šä¹‰çš„ç´¯åŠ å‡½æ•°ï¼šupdateFunc val results = lines.flatMap(_.split(\" \")).map((_,1)).updateStateByKey(updateFunc, new HashPartitioner(ssc.sparkContext.defaultParallelism), true) results.print() ssc.start() ssc.awaitTermination() &#125;&#125; Datasetæ¯”RDDæ‰§è¡Œé€Ÿåº¦å¿«å¾ˆå¤šå€ï¼Œå ç”¨çš„å†…å­˜æ›´å°ï¼Œæ˜¯ä»dataFrameå‘å±•è€Œæ¥ï¼ŒåŒ…å«dataFramedataFrameæ˜¯å¤„ç†ç»“æ„åŒ–æ•°æ®ï¼Œæœ‰è¡¨å¤´ï¼Œæœ‰ç±»å‹ï¼Œ dataSetä»1.6.0å¼€å§‹å‡ºç°ï¼Œ2.0åšäº†é‡å¤§æ”¹è¿›ï¼Œå¯¹dataFrameè¿›è¡Œäº†æ•´åˆdataFrameåœ¨1.4ç³»åˆ—å‡ºç°çš„ï¼Œç°åœ¨å¾ˆå¤šå…¬å¸éƒ½æ˜¯ç”¨çš„RDD åœ¨sparkçš„å‘½ä»¤è¡Œé‡Œé¢ï¼šå°†dataFrameè½¬æˆdataSetval ds = df.as[person]è°ƒç”¨dataSetçš„æ–¹æ³• 1234ds.map ds.show val ds = sqlContext.read.text(\"hdfs://bigdata1:9000/wc/).as[String] val res5 = ds.flatmap(.split(\" \")).map((,1)) flatmapå°†æ–‡æœ¬é‡Œé¢çš„æ¯ä¸€è¡Œè¿›è¡Œåˆ‡åˆ†ï¼Œrest.reduceByKey();ä¼šå‘ç°dataSeté‡Œé¢æ²¡æœ‰è¿™ä¸ªæ–¹æ³•ï¼Œåœ¨dataSeté‡Œé¢åº”è¯¥è°ƒç”¨æ›´é«˜çº§çš„åšæ³•ds.flatmap(_.split(â€œ â€œ)).groupBy($â€â€value).count.show æˆ–è€…collect åœ¨importé‡Œé¢æ‰“å¼€ideaæŸ¥çœ‹ç±»é‡Œé¢æœ‰å“ªäº›æ–¹æ³•ã€‚åœ¨spark1.6é‡Œé¢sqlContext.readâ€¦.è¯»å–çš„å°±æ˜¯dataFrameï¼Œå’ŒdataSetè¿˜æœªç»Ÿä¸€ï¼Œéœ€è¦å°†dataFrameç”¨asè½¬ä¸ºdataSet Sparkå¼•å…¥DataFrameï¼Œå®ƒå¯ä»¥æä¾›high-level functionsè®©Sparkæ›´å¥½çš„å¤„ç†ç»“æ„æ•°æ®çš„è®¡ç®—ã€‚è¿™è®©Catalyst optimizer å’ŒTungstenï¼ˆé’¨ä¸ï¼‰ execution engineè‡ªåŠ¨åŠ é€Ÿå¤§æ•°æ®åˆ†æã€‚å‘å¸ƒDataFrameä¹‹åå¼€å‘è€…æ”¶åˆ°äº†å¾ˆå¤šåé¦ˆï¼Œå…¶ä¸­ä¸€ä¸ªä¸»è¦çš„æ˜¯å¤§å®¶åæ˜ ç¼ºä¹ç¼–è¯‘æ—¶ç±»å‹å®‰å…¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒSparké‡‡ç”¨æ–°çš„Dataset API (DataFrame APIçš„ç±»å‹æ‰©å±•)ã€‚Dataset APIæ‰©å±•DataFrame APIæ”¯æŒé™æ€ç±»å‹å’Œè¿è¡Œå·²ç»å­˜åœ¨çš„Scalaæˆ–Javaè¯­è¨€çš„ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ã€‚å¯¹æ¯”ä¼ ç»Ÿçš„RDD APIï¼ŒDataset APIæä¾›æ›´å¥½çš„å†…å­˜ç®¡ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿ä»»åŠ¡ä¸­æœ‰æ›´å¥½çš„æ€§èƒ½æå‡ #åˆ›å»ºDataSetcase class Data(a: Int, b: String)val ds = Seq(Data(1, â€œoneâ€), Data(2, â€œtwoâ€)).toDS()ds.collect()ds.show() #åˆ›å»ºDataSet 1234case class Person(name: String, zip: Long)val df = sqlContext.read.json(sc.parallelize(\"\"\"&#123;\"zip\": 94709, \"name\": \"Michael\"&#125;\"\"\" :: Nil))df.as[Person].collect()df.as[Person].show() #DataSetçš„WordCount 123456import org.apache.spark.sql.functions._val ds = sqlContext.read.text(\"hdfs://node-1.itcast.cn:9000/wc\").as[String]val result = ds.flatMap(_.split(\" \")).filter(_ != \"\").toDF().groupBy($\"value\").agg(count(\"*\") as \"numOccurances\").orderBy($\"numOccurances\" desc)val wordCount = ds.flatMap(_.split(\" \")).filter(_ != \"\").groupBy(_.toLowerCase()).count() #åˆ›å»ºDataSet 1val lines = sqlContext.read.text(\"hdfs://node-1.itcast.cn:9000/wc\").as[String] #å¯¹DataSetè¿›è¡Œæ“ä½œ 1val words = lines.flatMap(_.split(\" \")).filter(_ != \"\") #æŸ¥çœ‹DataSetä¸­çš„å†…å®¹ 12words.collectwords.show #åˆ†ç»„æ±‚å’Œ 12345val counts = words.groupBy(_.toLowerCase).count()--------------------------------------------------------------------------------------------------------------&#123;\"name\": \"UC Berkeley\", \"yearFounded\": 1868, \"numStudents\": 37581&#125;&#123;\"name\": \"MIT\", \"yearFounded\": 1860, \"numStudents\": 11318&#125; #å‘hdfsä¸­ä¸Šä¼ æ•°æ® 1/usr/local/hadoop-2.6.4/bin/hdfs dfs -put schools.json / #å®šä¹‰case class 1case class University(name: String, numStudents: Long, yearFounded: Long) #åˆ›å»ºDataSet 1val schools = sqlContext.read.json(\"hdfs://node-1.itcast.cn:9000/schools.json\").as[University] #æ“ä½œDataSet 1schools.map(sc =&gt; s\"$&#123;sc.name&#125; is $&#123;2015 - sc.yearFounded&#125; years old\").show #JSON -&gt; DataFrame 12345val df = sqlContext.read.json(\"hdfs://node-1.itcast.cn:9000/person.json\")df.where($\"age\" &gt;= 20).showdf.where(col(\"age\") &gt;= 20).showdf.printSchema #DataFrame -&gt; Dataset 1234567891011121314151617181920212223242526272829303132333435363738case class Person(age: Long, name: String)val ds = df.as[Person]ds.filter(_.age &gt;= 20).show// Dataset -&gt; DataFrameval df2 = ds.toDFimport org.apache.spark.sql.types._df.where($\"age\" &gt; 0).groupBy((($\"age\" / 10) cast IntegerType) * 10 as \"decade\").agg(count(\"*\")).orderBy($\"decade\").show ds.filter(_.age &gt; 0).groupBy(p =&gt; (p.age / 10) * 10).agg(count(\"name\")).toDF().withColumnRenamed(\"value\", \"decade\").orderBy(\"decade\") .show val df = sqlContext.read.json(\"hdfs://node-1.itcast.cn:9000/student.json\")case class Student(name: String, age: Long, major: String)val studentDS = df.as[Student]studentDS.select($\"name\".as[String], $\"age\".as[Long]).filter(_._2 &gt; 19).collect()studentDS.groupBy(_.major).count().collect()import org.apache.spark.sql.functions._studentDS.groupBy(_.major).agg(avg($\"age\").as[Double]).collect()case class Major(shortName: String, fullName: String)val majors = Seq(Major(\"CS\", \"Computer Science\"), Major(\"Math\", \"Mathematics\")).toDS()val joined = studentDS.joinWith(majors, $\"major\" === $\"shortName\")joined.map(s =&gt; (s._1.name, s._2.fullName)).show()joined.explain()","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"}]},{"title":"SparkSQLä½¿ç”¨","slug":"SparkSqlçš„ä½¿ç”¨","date":"2018-09-05T17:16:31.239Z","updated":"2018-09-05T17:28:42.768Z","comments":true,"path":"2018/09/06/SparkSqlçš„ä½¿ç”¨/","link":"","permalink":"http://gangtieguo.cn/2018/09/06/SparkSqlçš„ä½¿ç”¨/","excerpt":"[TOC] //1.è¯»å–æ•°æ®ï¼Œå°†æ¯ä¸€è¡Œçš„æ•°æ®ä½¿ç”¨åˆ—åˆ†éš”ç¬¦åˆ†å‰² val lineRDD = sc.textFile(â€œhdfs://bigdata1:9000/person.txtâ€, 1).map(_.split(â€œ â€œ))","text":"[TOC] //1.è¯»å–æ•°æ®ï¼Œå°†æ¯ä¸€è¡Œçš„æ•°æ®ä½¿ç”¨åˆ—åˆ†éš”ç¬¦åˆ†å‰² val lineRDD = sc.textFile(â€œhdfs://bigdata1:9000/person.txtâ€, 1).map(_.split(â€œ â€œ)) //2.å®šä¹‰case classï¼ˆç›¸å½“äºè¡¨çš„schemaï¼‰ case class Person(id:Int, name:String, age:Int) //3.å¯¼å…¥éšå¼è½¬æ¢,åœ¨å½“å‰ç‰ˆæœ¬ä¸­å¯ä»¥ä¸ç”¨å¯¼å…¥ import sqlContext.implicits._ //4.å°†lineRDDè½¬æ¢æˆpersonRDD val personRDD = lineRDD.map(x =&gt; Person(x(0).toInt, x(1), x(2).toInt)) //5.å°†personRDDè½¬æ¢æˆDataFrame val personDF = personRDD.toDF 6.å¯¹personDFè¿›è¡Œå¤„ç† #(SQLé£æ ¼è¯­æ³•) personDF.registerTempTable(â€œt_personâ€) sqlContext.sql(â€œselect * from t_person order by age desc limit 2â€).show sqlContext.sql(â€œdesc t_personâ€).show val result = sqlContext.sql(â€œselect * from t_person order by age descâ€) 7.ä¿å­˜ç»“æœ result.save(â€œhdfs://bigdata1:9000/sql/res1â€) result.save(â€œhdfs://bigdata1:9000/sql/res2â€, â€œjsonâ€) #ä»¥JSONæ–‡ä»¶æ ¼å¼è¦†å†™HDFSä¸Šçš„JSONæ–‡ä»¶ import org.apache.spark.sql.SaveMode._ result.save(â€œhdfs://bigdata1:9000/sql/res2â€, â€œjsonâ€ , Overwrite) 8.é‡æ–°åŠ è½½ä»¥å‰çš„å¤„ç†ç»“æœï¼ˆå¯é€‰ï¼‰ sqlContext.load(â€œhdfs://bigdata1:9000/sql/res1â€) sqlContext.load(â€œhdfs://bigdata1:9000/sql/res2â€, â€œjsonâ€)","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"},{"name":"SparkSQL","slug":"SparkSQL","permalink":"http://gangtieguo.cn/tags/SparkSQL/"}]},{"title":"Spark-on-Yarnæºç è§£æ(å››)Sparkä¸šåŠ¡ä»£ç çš„æ‰§è¡ŒåŠå…¶ä»»åŠ¡åˆ†é…è°ƒåº¦stageåˆ’åˆ†","slug":"Spark-on-Yarnæºç è§£æâ‘£Sparkä¸šåŠ¡ä»£ç çš„æ‰§è¡ŒåŠå…¶ä»»åŠ¡åˆ†é…è°ƒåº¦stageåˆ’åˆ†","date":"2018-09-04T08:16:39.931Z","updated":"2019-03-11T07:45:16.244Z","comments":true,"path":"2018/09/04/Spark-on-Yarnæºç è§£æâ‘£Sparkä¸šåŠ¡ä»£ç çš„æ‰§è¡ŒåŠå…¶ä»»åŠ¡åˆ†é…è°ƒåº¦stageåˆ’åˆ†/","link":"","permalink":"http://gangtieguo.cn/2018/09/04/Spark-on-Yarnæºç è§£æâ‘£Sparkä¸šåŠ¡ä»£ç çš„æ‰§è¡ŒåŠå…¶ä»»åŠ¡åˆ†é…è°ƒåº¦stageåˆ’åˆ†/","excerpt":"spark-on-yarnç³»åˆ— Spark-on-Yarn æºç è§£æâ‘ Yarn ä»»åŠ¡è§£æSpark-on-Yarn æºç è§£æâ‘¡Spark-Submit è§£æSpark-on-Yarn æºç è§£æâ‘¢client åšçš„äº‹æƒ…Spark-on-Yarn æºç è§£æâ‘£Spark ä¸šåŠ¡ä»£ç çš„æ‰§è¡ŒåŠå…¶ä»»åŠ¡åˆ†é…è°ƒåº¦ stage åˆ’åˆ† çœ‹çœ‹è‡ªå®šä¹‰çš„ç±» 1234567891011121314151617181920object WordCount &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"yaoWordCount\").setMaster(\"local[2]\") val sc = new SparkContext(conf) var hadoopRDD: RDD[String] = sc.textFile(args(0)) var hdfsRDD: RDD[String] = hadoopRDD.flatMap(_.split(\"\")) //å•è¯å’Œå‡ºç°çš„æ¬¡æ•°ï¼Œæ„å»ºRDDå¹¶ä¸”è°ƒç”¨äº†ä»–çš„Transformation //è¿”å›çš„æ˜¯ä¸€ä¸ªhadoopRDD //transFormationéƒ½æ˜¯è¿”å›çš„RDD var wordAndCount: RDD[(String, Int)] = hdfsRDD.map((_, 1)) //åˆ›å»ºRDD è¿™é‡Œé¢æœ‰ä¸¤ä¸ªRDD,ä¸€ä¸ªæ˜¯hadoopRDDï¼Œç„¶åä¼šç”Ÿæˆä¸€ä¸ªparitionRDD //savaasTextfileè¿˜ä¼šäº§ç”Ÿä¸€ä¸ªRDD,å› ä¸ºä¼šè°ƒç”¨mapPartitons //è°ƒç”¨RDDçš„action å¼€å§‹çœŸæ­£æäº¤ä»»åŠ¡ var reducedRDD: RDD[(String, Int)] = wordAndCount.reduceByKey(_ + _) reducedRDD.saveAsTextFile(args(1)) //å…³é—­saprkContextèµ„æº sc.stop() &#125;&#125;","text":"spark-on-yarnç³»åˆ— Spark-on-Yarn æºç è§£æâ‘ Yarn ä»»åŠ¡è§£æSpark-on-Yarn æºç è§£æâ‘¡Spark-Submit è§£æSpark-on-Yarn æºç è§£æâ‘¢client åšçš„äº‹æƒ…Spark-on-Yarn æºç è§£æâ‘£Spark ä¸šåŠ¡ä»£ç çš„æ‰§è¡ŒåŠå…¶ä»»åŠ¡åˆ†é…è°ƒåº¦ stage åˆ’åˆ† çœ‹çœ‹è‡ªå®šä¹‰çš„ç±» 1234567891011121314151617181920object WordCount &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"yaoWordCount\").setMaster(\"local[2]\") val sc = new SparkContext(conf) var hadoopRDD: RDD[String] = sc.textFile(args(0)) var hdfsRDD: RDD[String] = hadoopRDD.flatMap(_.split(\"\")) //å•è¯å’Œå‡ºç°çš„æ¬¡æ•°ï¼Œæ„å»ºRDDå¹¶ä¸”è°ƒç”¨äº†ä»–çš„Transformation //è¿”å›çš„æ˜¯ä¸€ä¸ªhadoopRDD //transFormationéƒ½æ˜¯è¿”å›çš„RDD var wordAndCount: RDD[(String, Int)] = hdfsRDD.map((_, 1)) //åˆ›å»ºRDD è¿™é‡Œé¢æœ‰ä¸¤ä¸ªRDD,ä¸€ä¸ªæ˜¯hadoopRDDï¼Œç„¶åä¼šç”Ÿæˆä¸€ä¸ªparitionRDD //savaasTextfileè¿˜ä¼šäº§ç”Ÿä¸€ä¸ªRDD,å› ä¸ºä¼šè°ƒç”¨mapPartitons //è°ƒç”¨RDDçš„action å¼€å§‹çœŸæ­£æäº¤ä»»åŠ¡ var reducedRDD: RDD[(String, Int)] = wordAndCount.reduceByKey(_ + _) reducedRDD.saveAsTextFile(args(1)) //å…³é—­saprkContextèµ„æº sc.stop() &#125;&#125; sparkContextçš„åˆå§‹åŒ–å¯¹äºSparkç¨‹åºå…¥å£ä¸ºSparkContext,å½“æˆ‘ä»¬ä½¿ç”¨spark-submit/spark-shellç­‰å‘½ä»¤æ¥å¯åŠ¨ä¸€ä¸ªå®¢æˆ·ç«¯,å®¢æˆ·ç«¯ä¸é›†ç¾¤éœ€è¦å»ºç«‹é“¾æ¥ï¼Œå»ºç«‹çš„è¿™ä¸ªé“¾æ¥å¯¹è±¡å°±å«åšsparkContextï¼Œåªæœ‰è¿™ä¸ªå¯¹è±¡åˆ›å»ºæˆåŠŸæ‰æ ‡å¿—è¿™è¿™ä¸ªå®¢æˆ·ç«¯ä¸sparké›†ç¾¤é“¾æ¥æˆåŠŸã€‚ç°å°±å°†ä»SparkContextå±•å¼€æ¥æè¿°ä¸€ä¸‹Sparkçš„ä»»åŠ¡å¯åŠ¨å’Œæ‰§è¡Œæµç¨‹ã€‚SparkContext å®Œæˆäº†ä»¥ä¸‹å‡ ä¸ªä¸»è¦çš„åŠŸèƒ½ï¼šï¼ˆ1ï¼‰åˆ›å»º RDDï¼Œé€šè¿‡ç±»ä¼¼ textFile ç­‰çš„æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ä¸èµ„æºç®¡ç†å™¨äº¤äº’ï¼Œé€šè¿‡ runJob ç­‰æ–¹æ³•å¯åŠ¨åº”ç”¨ã€‚ï¼ˆ3ï¼‰åˆ›å»º DAGSchedulerã€TaskScheduler ç­‰ã€‚ åœ¨SparkContextç±»ä¸­ï¼ŒSparkContextä¸»æ„é€ å™¨ä¸»è¦åš æˆ‘ä»¬çœ‹ä¸€ä¸‹SparkContextçš„ä¸»æ„é€ å™¨ è°ƒç”¨CreateSparkEnvæ–¹æ³•åˆ›å»ºSparkEnv(å°†driverçš„ä¿¡æ¯ï¼Œurlï¼Œipç­‰éƒ½å°è£…)ï¼ŒSparkEnvä¸­æœ‰ä¸€ä¸ªå¯¹è±¡ActorSystem åˆ›å»ºTaskScheduler ï¼Œæ ¹æ®æäº¤ä»»åŠ¡çš„URLï¼ˆå¦‚ï¼šspark://(.*)â€ï¼Œlocal[1]ç­‰ï¼Œå»åˆ›å»ºTaskSchedulerImpl ï¼Œç„¶åå†åˆ›å»ºSparkDeploySchedulerBackend(å…ˆååˆ›å»ºdriverActorå’ŒclientActor) åˆ›å»ºDAGScheduler TaskSchedulerå¯åŠ¨ï¼ŒTaskScheduler.start() SparkEnvæœ€ç»ˆå°†driverçš„host,portç«¯å£ç­‰å„ç§ä¿¡æ¯éƒ½å°è£…åˆ°é‡Œé¢ 123456789101112131415161718new SparkEnv( executorId, actorSystem, serializer, closureSerializer, cacheManager, mapOutputTracker, shuffleManager, broadcastManager, blockTransferService, blockManager, securityManager, httpFileServer, sparkFilesDir, metricsSystem, shuffleMemoryManager, outputCommitCoordinator, conf) TaskScheduleråœ¨SparkContextç±»ä¸­å¯ä»¥çœ‹åˆ°ï¼ŒTaskScheduleræ ¹æ®urlç±»å‹åŒ¹é…åˆ›å»ºTaskSchedulerImpl 1234567891011121314151617181920212223 //TODO æ ¹æ®æäº¤ä»»åŠ¡æ—¶æŒ‡å®šçš„URLåˆ›å»ºç›¸åº”çš„TaskScheduler private def createTaskScheduler( sc: SparkContext, master: String): (SchedulerBackend, TaskScheduler) = &#123; ... case \"yarn-standalone\" | \"yarn-cluster\" =&gt;... val scheduler = try &#123; val clazz = Class.forName(\"org.apache.spark.scheduler.cluster.YarnClusterScheduler\") val cons = clazz.getConstructor(classOf[SparkContext]) cons.newInstance(sc).asInstanceOf[TaskSchedulerImpl] &#125; ... val backend = try &#123; val clazz = Class.forName(\"org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend\") val cons = clazz.getConstructor(classOf[TaskSchedulerImpl], classOf[SparkContext]) cons.newInstance(scheduler, sc).asInstanceOf[CoarseGrainedSchedulerBackend] &#125; scheduler.initialize(backend) (backend, scheduler) .... &#125; å¯çŸ¥TaskScheduler çš„å®ç°ç±»org.apache.spark.scheduler.cluster.YarnSchedulerTaskSchedulerBacked çš„å®ç°ç±»ä¸ºorg.apache.spark.scheduler.cluster.YarnClientSchedulerBackendä¸”TaskSchedulerå¯¹TaskSchedulerBackedä¿æŒäº†å¼•ç”¨scheduler.initialize(backend) å¯åŠ¨TaskScheduleråœ¨Sparkçš„æ„é€ å‡½æ•°ä¸­,ä¼šå¯åŠ¨TaskScheduler 1taskScheduler.start() å¯ä»¥çœ‹åˆ°ç»§æ‰¿å…³ç³» 12private[spark] class YarnClusterScheduler(sc: SparkContext) extends YarnScheduler(sc) private[spark] class YarnScheduler(sc: SparkContext) extends TaskSchedulerImpl(sc) å¯ä»¥è·Ÿè¸ªåˆ°ï¼Œstartæ–¹æ³•æœ€ç»ˆè°ƒç”¨çš„æ˜¯TaskSchedulerImplé‡Œé¢startæ–¹æ³•ï¼Œåœ¨startæ–¹æ³•é‡Œé¢ 12345 override def start() &#123; //TODO é¦–å…ˆè°ƒç”¨SparkDeploySchedulerBackendçš„startæ–¹æ³• backend.start() ......&#125; ,è¿™é‡Œçš„backendå°±æ˜¯YarnClusterSchedulerBackendï¼Œè€Œè¿™ä¸ªæœ€ç»ˆç»§æ‰¿çš„æ˜¯CoarseGrainedSchedulerBackendä¸­startæ–¹æ³• 12345override def start() &#123;... driverActor = actorSystem.actorOf( Props(new DriverActor(properties)), name = CoarseGrainedSchedulerBackend.ACTOR_NAME)&#125; è·å–åˆ°sparkçš„é…ç½®ä¿¡æ¯åï¼Œä¼šåˆ›å»ºdriverActor DAGScheduleråœ¨SparkContextçš„æ„é€ å‡½æ•°ä¸­ï¼Œä¼šåˆ›å»ºDAGScheduler 1dagScheduler= new DAGScheduler(this) åœ¨DAGScheduleræ„é€ å‡½æ•°ä¸­ 1def this(sc: SparkContext) = this(sc, sc.taskScheduler) å¯ä»¥çœ‹åˆ°DAGSchedulerå¯¹TaskSchedulerä¿æŒäº†å¼•ç”¨ 1234567891011class DAGScheduler( private[scheduler] val sc: SparkContext, private[scheduler] val taskScheduler: TaskScheduler, listenerBus: LiveListenerBus, mapOutputTracker: MapOutputTrackerMaster, blockManagerMaster: BlockManagerMaster, env: SparkEnv, clock: Clock = new SystemClock()) extends Logging &#123; ...... &#125; mapOutputTrackerï¼šæ˜¯è¿è¡Œåœ¨ Driver ç«¯ç®¡ç† shuffle çš„ä¸­é—´è¾“å‡ºä½ç½®ä¿¡æ¯çš„ã€‚ blockManagerMasterï¼šä¹Ÿæ˜¯è¿è¡Œåœ¨ Driver ç«¯çš„ï¼Œå®ƒæ˜¯ç®¡ç†æ•´ä¸ª Job çš„ Bolck ä¿¡æ¯ã€‚ RDDçš„æ„å»ºè¿‡ç¨‹å…¶ä¸­hadoopRDDï¼ŒhdfsRDDï¼ŒwordRDDï¼ŒreduceRDDæ˜¯ç»è¿‡ä¸€ç³»åˆ—transformationè£…æ¢rddï¼Œåªæœ‰ç­‰åˆ°actionæ—¶ï¼Œæ‰ä¼šè§¦å‘æ•°æ®çš„æµè½¬ è¯¥ä¾‹çš„actionä¸ºsaveAsTextFileè°ƒç”¨é“¾ä¸º 123456saveAsTextFile() saveAsHadoopFile() saveAsHadoopFileï¼ˆé‡è½½å‡½æ•°ï¼‰ saveAsHadoopDataset() runJob()ä¹‹é—´ä¼šè°ƒç”¨å‡ ä¸ªé‡è½½å‡½æ•° dagScheduler.runJob()æœ€ç»ˆè°ƒç”¨ ä½œä¸šæäº¤ä»»åŠ¡æµè½¬é¦–å…ˆæ³¨æ„åŒºåˆ† 2 ä¸ªæ¦‚è¿°ï¼šjob: æ¯ä¸ª action éƒ½æ˜¯æ‰§è¡Œ runJob æ–¹æ³•ï¼Œå¯ä»¥å°†ä¹‹è§†ä¸ºä¸€ä¸ª jobã€‚stageï¼šåœ¨è¿™ä¸ª job å†…éƒ¨ï¼Œä¼šæ ¹æ®å®½ä¾èµ–ï¼Œåˆ’åˆ†æˆå¤šä¸ª stageã€‚ åœ¨actionè§¦å‘åï¼Œæœ€æœ€ç»ˆè°ƒç”¨çš„æ˜¯DAGScheduler.runJob() 1dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) è€ŒrunJob() çš„æ ¸å¿ƒä»£ç ä¸ºï¼š 1val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties) å³è°ƒç”¨ submitJob æ–¹æ³•ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥çœ‹çœ‹ submitJob() 1234567891011121314151617 def submitJob[T, U]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =&gt; U, partitions: Seq[Int], callSite: CallSite, resultHandler: (Int, U) =&gt; Unit, properties: Properties): JobWaiter[U] = &#123;.... val jobId = nextJobId.getAndIncrement()..... val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler) eventProcessLoop.post(JobSubmitted( jobId, rdd, func2, partitions.toArray, callSite, waiter, SerializationUtils.clone(properties))) waiter &#125; submitJob() æ–¹æ³•ä¸»è¦å®Œæˆäº†ä»¥ä¸‹ 3 ä¸ªå·¥ä½œï¼š è·å–ä¸€ä¸ªæ–°çš„ jobId ç”Ÿæˆä¸€ä¸ª JobWaiterï¼Œå®ƒä¼šç›‘å¬ Job çš„æ‰§è¡ŒçŠ¶æ€ï¼Œè€Œ Job æ˜¯ç”±å¤šä¸ª Task ç»„æˆçš„ï¼Œå› æ­¤åªæœ‰å½“ Job çš„æ‰€æœ‰ Task å‡å·²å®Œæˆï¼ŒJob æ‰ä¼šæ ‡è®°æˆåŠŸ æœ€åè°ƒç”¨ eventProcessLoop.post() å°† Job æäº¤åˆ°ä¸€ä¸ªé˜Ÿåˆ—ä¸­ï¼Œç­‰å¾…å¤„ç†ã€‚è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„ç”Ÿäº§è€…æ¶ˆè´¹è€…æ¨¡å¼ã€‚è¿™äº›æ¶ˆæ¯éƒ½æ˜¯é€šè¿‡ handleJobSubmitted æ¥å¤„ç†ã€‚ ç®€å•çœ‹ä¸€ä¸‹ handleJobSubmitted æ˜¯å¦‚ä½•è¢«è°ƒç”¨çš„ã€‚é¦–å…ˆæ˜¯ DAGSchedulerEventProcessLoop#onReceive è°ƒç”¨ 1234567//TODO é€šè¿‡æ¨¡å¼åŒ¹é…åˆ¤æ–­äº‹ä»¶çš„ç±»å‹ æ¯”å¦‚ä»»åŠ¡æäº¤ï¼Œä½œä¸šå–æ¶ˆ ...override def onReceive(event: DAGSchedulerEvent): Unit = event match &#123; //TODO æäº¤è®¡ç®—ä»»åŠ¡ case JobSubmitted(jobId, rdd, func, partitions, allowLocal, callSite, listener, properties) =&gt; //todo è°ƒç”¨dagSchedulerçš„handlerJobSubmittedæ–¹æ³•å¤„ç† dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, allowLocal, callSite, listener, properties) ... ... DAGSchedulerEventProcessLoop æ˜¯ EventLoop çš„å­ç±»ï¼Œå®ƒé‡å†™äº† EventLoop çš„ onReceive æ–¹æ³•ã€‚ä»¥åå†åˆ†æè¿™ä¸ª EventLoopã€‚onReceive ä¼šè°ƒç”¨ handleJobSubmittedã€‚ stage çš„åˆ’åˆ†åˆšæ‰è¯´åˆ° handleJobSubmitted ä¼šä» eventProcessLoop ä¸­å–å‡º Job æ¥è¿›è¡Œå¤„ç†ï¼Œå¤„ç†çš„ç¬¬ä¸€æ­¥å°±æ˜¯å°† Job åˆ’åˆ†æˆä¸åŒçš„ stageã€‚handleJobSubmitted ä¸»è¦ 2 ä¸ªå·¥ä½œï¼Œä¸€æ˜¯è¿›è¡Œ stage çš„åˆ’åˆ†ï¼Œè¿™æ˜¯è¿™éƒ¨åˆ†è¦ä»‹ç»çš„å†…å®¹ï¼›äºŒæ˜¯åˆ›å»ºä¸€ä¸ª activeJobï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªä»»åŠ¡ï¼Œè¿™åœ¨ä¸‹ä¸€å°èŠ‚ä»‹ç»ã€‚ è¿˜æ˜¯å…ˆçœ‹çœ‹è°ƒç”¨é“¾ 12345handleJobSubmitted -&gt;newStage() -&gt;getParentStages()//æ­¤å¤„ä¼šéå†RDDæ‰€æœ‰ä¾èµ– -&gt;getShuffleMapStage()//å¦‚æœæ˜¯ShuffleDependencyï¼ˆå®½ä¾èµ–ï¼Œè·å–åˆ°ä¸€ä¸ªMapï¼‰ -&gt;newOrUsedStage()//è¿™å°±å¯ä»¥è§£é‡Šæˆ‘ä»¬å¸¸è¯´çš„é‡åˆ°å®½ä¾èµ–å°±ä¼šåˆ’åˆ†stageï¼Œå¹¶ä¸”è¿”å›stage æ‰€ä»¥æœ€ç»ˆè¿”å›çš„æ˜¯ä¸€ä¸ªæ‹¥æœ‰æ¬¾ä¾èµ–çš„ 1234567891011121314151617181920private[scheduler] def handleJobSubmitted(jobId: Int, finalRDD: RDD[_], func: (TaskContext, Iterator[_]) =&gt; _, partitions: Array[Int], callSite: CallSite, listener: JobListener, properties: Properties) &#123; ... //todo é‡è¦ï¼šè¯¥æ–¹æ³•ç”¨äºåˆ’åˆ†stageï¼Œä¸»è¦ä¾èµ–çš„æ˜¯finalStage finalStage = newStage(finalRDD, partitions.size, None, jobId, callSite) ..... //TODO é›†ç¾¤æ¨¡å¼ activeJobs += job ...... //todo æäº¤stage submitStage(finalStage) &#125; //TODO å¼€å§‹å‘é›†ç¾¤æäº¤è¿˜åœ¨ç­‰å¾…çš„stage submitWaitingStages()&#125; getParentStages()ã€‚å› ä¸ºæ˜¯ä»æœ€ç»ˆçš„ stage å¾€å›æ¨ç®—çš„ï¼Œè¿™éœ€è¦è®¡ç®—æœ€ç»ˆ stage æ‰€ä¾èµ–çš„å„ä¸ª stageã€‚ 123456789101112131415161718192021222324//TODO ç”¨äºè·å–çˆ¶stage private def getParentStages(rdd: RDD[_], jobId: Int): List[Stage] = &#123; val parents = new HashSet[Stage] val waitingForVisit = new Stack[RDD[_]] def visit(r: RDD[_]) &#123; if (!visited(r)) &#123; visited + r for (dep &lt;- r.dependencies) &#123; dep match &#123; case shufDep: ShuffleDependency[_, _, _] =&gt; //TODO æŠŠå®½ä¾èµ–ä¼ è¿›å»ï¼Œè·å¾—çˆ¶stage parents += getShuffleMapStage(shufDep, jobId) case _ =&gt; waitingForVisit.push(dep.rdd) &#125; &#125; &#125; &#125; waitingForVisit.push(rdd) while (!waitingForVisit.isEmpty) &#123; visit(waitingForVisit.pop()) &#125; parents.toList &#125; ä»»åŠ¡çš„ç”Ÿæˆå›åˆ° handleJobSubmitted ä¸­çš„ä»£ç ï¼š 1submitStage(finalStage) submitStage ä¼šæäº¤ finalStageï¼Œå¦‚æœè¿™ä¸ª stage çš„æŸäº› parentStage æœªæäº¤ï¼Œåˆ™é€’å½’è°ƒç”¨ submitStage()ï¼Œç›´è‡³æ‰€æœ‰çš„ stage å‡å·²è®¡ç®—å®Œæˆã€‚ submitStage() ä¼šè°ƒç”¨ submitMissingTasks(): submitMissingTasks(stage, jobId.get) è€Œ submitMissingTasks() ä¼šå®Œæˆ DAGScheduler æœ€åçš„å·¥ä½œï¼šå®ƒåˆ¤æ–­å‡ºå“ªäº› Partition éœ€è¦è®¡ç®—ï¼Œä¸ºæ¯ä¸ª Partition ç”Ÿæˆ Taskï¼Œç„¶åè¿™äº› Task å°±ä¼šå°é—­åˆ° TaskSet 12345678910111213141516171819202122232425//TODO DAGæäº¤stage æ ¹æ®æœ€åä¸€ä¸ªstage å¼€å§‹æ‰¾åˆ°ç¬¬ä¸€ä¸ªstageé€’å½’æäº¤stage /** Submits stage, but first recursively submits any missing parents. */ private def submitStage(stage: Stage) &#123; val jobId = activeJobForStage(stage) if (jobId.isDefined) &#123; if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123; //TODO è·å–ä»–çš„çˆ¶stage æ²¡æœ‰æäº¤çš„stage val missing = getMissingParentStages(stage).sortBy(_.id) //todo åˆ¤æ–­çˆ¶stageæ˜¯å¦ä¸ºç©ºï¼Œä¸ºç©ºå°±ä»¥ä¸ºç€ä»–æ˜¯ç¬¬ä¸€stage if (missing == Nil) &#123; //TODO å¼€å§‹æäº¤æœ€å‰é¢çš„stage, DAGæäº¤stageç»™TaskScheduler ä¼šå°†stageè½¬æ¢æˆtaskSet submitMissingTasks(stage, jobId.get) &#125; else &#123; //TODO æœ‰çˆ¶stage å°±é€’å½’æäº¤ for (parent &lt;- missing) &#123; submitStage(parent) &#125; waitingStages += stage &#125; &#125; &#125; else &#123; abortStage(stage, \"No active job for stage \" + stage.id) &#125; &#125; submitMissingTasksåœ¨æœ€åæäº¤ç»™ TaskScheduler è¿›è¡Œå¤„ç† 123456789101112131415161718192021222324252627282930313233343536 //TODO DAGæäº¤stageç»™TaskScheduler ä¼šå°†stageè½¬æ¢æˆtaskSet private def submitMissingTasks(stage: Stage, jobId: Int) &#123;...//TODO åˆ›å»ºå¤šå°‘ä¸ªTask val tasks: Seq[Task[_]] = if (stage.isShuffleMap) &#123; partitionsToCompute.map &#123; id =&gt; //TODO æ•°æ®å­˜å‚¨çš„æœ€ä½³ä½ç½® ç§»åŠ¨è®¡ç®—ï¼Œè€Œä¸æ˜¯ç§»åŠ¨æ•°æ® val locs = getPreferredLocs(stage.rdd, id) val part = stage.rdd.partitions(id) //TODO ä»ä¸Šæ¸¸æ‹‰å–æ•°æ® new ShuffleMapTask(stage.id, taskBinary, part, locs) &#125; &#125; else &#123; val job = stage.resultOfJob.get partitionsToCompute.map &#123; id =&gt; val p: Int = job.partitions(id) val part = stage.rdd.partitions(p) val locs = getPreferredLocs(stage.rdd, p) //TODO å°†æ•°æ®å†™å…¥æŸä¸ªä»‹è´¨é‡Œé¢ï¼Œnosql hdfs ç­‰ç­‰ new ResultTask(stage.id, taskBinary, part, locs, id) &#125; &#125;//TODO taskçš„æ•°é‡æœ€å¥½å’Œåˆ†åŒºæ•°ä¸€æ · å¦‚æœåˆ†åŒºæ•°å¤§äº0 //TODO taskçš„æ•°é‡æœ€å¥½å’Œåˆ†åŒºæ•°ä¸€æ · å¦‚æœåˆ†åŒºæ•°å¤§äº0 if (tasks.size &gt; 0) &#123; logInfo(\"Submitting \" + tasks.size + \" missing tasks from \" + stage + \" (\" + stage.rdd + \")\") stage.pendingTasks ++= tasks //TODO è°ƒç”¨taskSchedulerçš„submitTasksæäº¤taskSet ç°åœ¨å°†taskè½¬æ¢æˆä¸€ä¸ªarraytaskScheduler.submitTasks(new TaskSet( tasks.toArray, stage.id, stage.latestInfo.attemptId, stage.firstJobId, properties)) stage.latestInfo.submissionTime = Some(clock.getTimeMillis()) .....&#125; TaskScheduler &amp;&amp; TaskSchedulerBackendä¸Šæ–‡åˆ†æåˆ°åœ¨ DAGScheduler ä¸­æœ€ç»ˆä¼šæ‰§è¡Œ taskScheduler.submitTasks() æ–¹æ³•ï¼Œæˆ‘ä»¬å…ˆç®€å•çœ‹ä¸€ä¸‹ä»è¿™é‡Œå¼€å§‹å¾€ä¸‹çš„æ‰§è¡Œé€»è¾‘ï¼š 12345678â‘ taskScheduler.submitTasks() -&gt;â‘¡schedulableBuilder.addTaskSetManager() è°ƒåº¦æ¨¡å¼ï¼Œæ˜¯å…ˆæ¥å…ˆæœåŠ¡è¿˜æ˜¯å…¬å¹³è°ƒåº¦æ¨¡å¼ -&gt;â‘¢CoarseGrainedSchedulerBackend.reviveOffers() è¿™ä¸ªæ˜¯å‘driverActorå‘é€æ¶ˆæ¯driverActor ! ReviveOffers -&gt;â‘£CoarseGrainedSchedulerBackend.receiveWithLogging è¿™æ˜¯driverActoræ¥æ”¶æ¶ˆæ¯çš„éƒ¨åˆ† -&gt;â‘¤CoarseGrainedSchedulerBackend.makeOffers() //case ReviveOffers =&gt;makeOffers() è¿™ä¸ªæ¨¡å¼åŒ¹é…ä¼šè°ƒç”¨maksOffersæ–¹æ³• -&gt;â‘¥launchTasks()è°ƒç”¨launchTaskå‘Executoræäº¤task -&gt;â‘¦ executorData.executorActor ! LaunchTask(new SerializableBuffer(serializedTask))å‘executorå‘é€åºåˆ—åŒ–å¥½çš„taskï¼Œå‘é€ä¸€ä¸ªTask æ­¥éª¤ä¸€ã€äºŒä¸­ä¸»è¦å°†è¿™ç»„ä»»åŠ¡çš„ TaskSet åŠ å…¥åˆ°ä¸€ä¸ª TaskSetManager ä¸­ã€‚TaskSetManager ä¼šæ ¹æ®æ•°æ®å°±è¿‘åŸåˆ™ä¸º task åˆ†é…è®¡ç®—èµ„æºï¼Œç›‘æ§ task çš„æ‰§è¡ŒçŠ¶æ€ç­‰ï¼Œæ¯”å¦‚å¤±è´¥é‡è¯•ï¼Œæ¨æµ‹æ‰§è¡Œç­‰ã€‚æ­¥éª¤ä¸‰ã€å››é€»è¾‘è¾ƒä¸ºç®€å•ã€‚æ­¥éª¤äº”ä¸ºæ¯ä¸ª task å…·ä½“åˆ†é…èµ„æºï¼Œå®ƒçš„è¾“å…¥æ˜¯ä¸€ä¸ª Executor çš„åˆ—è¡¨ï¼Œè¾“å‡ºæ˜¯ TaskDescription çš„äºŒç»´æ•°ç»„ã€‚TaskDescription åŒ…å«äº† TaskID, Executor ID å’Œ task æ‰§è¡Œçš„ä¾èµ–ä¿¡æ¯ç­‰ã€‚æ­¥éª¤å…­ã€ä¸ƒå°±æ˜¯å°†ä»»åŠ¡çœŸæ­£çš„å‘é€åˆ° executor ä¸­æ‰§è¡Œäº†ï¼Œå¹¶ç­‰å¾… executor çš„çŠ¶æ€è¿”å›ã€‚ â€‹","categories":[{"name":"Spark-On-Yarn","slug":"Spark-On-Yarn","permalink":"http://gangtieguo.cn/categories/Spark-On-Yarn/"}],"tags":[{"name":"åŸç†","slug":"åŸç†","permalink":"http://gangtieguo.cn/tags/åŸç†/"},{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"}]},{"title":"Spark-on-Yarnæºç è§£æ(ä¸‰)clientåšçš„äº‹æƒ…","slug":"Spark-on-Yarnæºç è§£æâ‘¢clientåšçš„äº‹æƒ…","date":"2018-09-04T08:13:34.283Z","updated":"2019-03-11T07:44:55.663Z","comments":true,"path":"2018/09/04/Spark-on-Yarnæºç è§£æâ‘¢clientåšçš„äº‹æƒ…/","link":"","permalink":"http://gangtieguo.cn/2018/09/04/Spark-on-Yarnæºç è§£æâ‘¢clientåšçš„äº‹æƒ…/","excerpt":"[TOC] spark-on-yarnç³»åˆ— Spark-on-Yarn æºç è§£æâ‘ Yarn ä»»åŠ¡è§£æSpark-on-Yarn æºç è§£æâ‘¡Spark-Submit è§£æSpark-on-Yarn æºç è§£æâ‘¢client åšçš„äº‹æƒ…Spark-on-Yarn æºç è§£æâ‘£Spark ä¸šåŠ¡ä»£ç çš„æ‰§è¡ŒåŠå…¶ä»»åŠ¡åˆ†é…è°ƒåº¦ stage åˆ’åˆ† org.apache.spark.deploy.yarn.Client è¯ä¸å¤šè¯´ï¼Œå…ˆä¸Šæºç ï¼Œå½“ç„¶è¿˜æ˜¯ç®€æ´ç‰ˆæœ¬çš„ è¿™å„¿æˆ‘å…ˆä¸Šä¸€ä¸‹æœ€ç®€æ´çš„è°ƒç”¨é“¾ã€‚ Client.main() -&gt;new Client().run() -&gt;monitorApplication(submitApplication()) -&gt;submitApplication() -&gt;createContainerLaunchContext()ä¼šå°è£…ä¸€äº›å¯åŠ¨ä¿¡æ¯å¦‚æˆ‘ä»¬å¯åŠ¨çš„ç±» --class -&gt;userClass -&gt;amArgs -&gt;commands -&gt;printableCommands -&gt;amClass applicationMasterå¯åŠ¨çš„çœŸå®ç±» -&gt;createApplicationSubmissionContext() -&gt;Records.newRecord(classOf[Resource])å¯åŠ¨ -&gt;yarnClientImpl.submitApplication(appContext)","text":"[TOC] spark-on-yarnç³»åˆ— Spark-on-Yarn æºç è§£æâ‘ Yarn ä»»åŠ¡è§£æSpark-on-Yarn æºç è§£æâ‘¡Spark-Submit è§£æSpark-on-Yarn æºç è§£æâ‘¢client åšçš„äº‹æƒ…Spark-on-Yarn æºç è§£æâ‘£Spark ä¸šåŠ¡ä»£ç çš„æ‰§è¡ŒåŠå…¶ä»»åŠ¡åˆ†é…è°ƒåº¦ stage åˆ’åˆ† org.apache.spark.deploy.yarn.Client è¯ä¸å¤šè¯´ï¼Œå…ˆä¸Šæºç ï¼Œå½“ç„¶è¿˜æ˜¯ç®€æ´ç‰ˆæœ¬çš„ è¿™å„¿æˆ‘å…ˆä¸Šä¸€ä¸‹æœ€ç®€æ´çš„è°ƒç”¨é“¾ã€‚ Client.main() -&gt;new Client().run() -&gt;monitorApplication(submitApplication()) -&gt;submitApplication() -&gt;createContainerLaunchContext()ä¼šå°è£…ä¸€äº›å¯åŠ¨ä¿¡æ¯å¦‚æˆ‘ä»¬å¯åŠ¨çš„ç±» --class -&gt;userClass -&gt;amArgs -&gt;commands -&gt;printableCommands -&gt;amClass applicationMasterå¯åŠ¨çš„çœŸå®ç±» -&gt;createApplicationSubmissionContext() -&gt;Records.newRecord(classOf[Resource])å¯åŠ¨ -&gt;yarnClientImpl.submitApplication(appContext) æœ€ç»ˆæ˜¯è°ƒç”¨çš„clienté‡Œé¢mainæ–¹æ³•-&gt;run-&gt; monitorApplication(submitApplication()) object Client extends Logging { def main(argStrings: Array[String]) { ... ... val sparkConf = new SparkConf val args = new ClientArguments(argStrings, sparkConf) new Client(args, sparkConf).run() ... ... } } ... ... def run(): Unit = { val (yarnApplicationState, finalApplicationStatus) = monitorApplication(submitApplication()) } ... ... def submitApplication(): ApplicationId = { // TODO: åˆå§‹åŒ–å¹¶ä¸”å¯åŠ¨client yarnClient.init(yarnConf) yarnClient.start() // TODO: å‡†å¤‡æäº¤è¯·æ±‚åˆ°resouceManager val newApp = yarnClient.createApplication() val newAppResponse = newApp.getNewApplicationResponse() val appId = newAppResponse.getApplicationId() // TODO: æ£€æŸ¥é›†ç¾¤çš„å†…å­˜æ˜¯å¦æ»¡è¶³å½“å‰çš„ä»»åŠ¡è¦æ±‚ verifyClusterResources(newAppResponse) // TODO: è®¾ç½®é€‚å½“ä¸Šä¸‹æ–‡ç¯å¢ƒæ¥å¯åŠ¨applicationMaster val containerContext = createContainerLaunchContext(newAppResponse) val appContext = createApplicationSubmissionContext(newApp, containerContext) // TODO: æäº¤application yarnClient.submitApplication(appContext) appId } private def createContainerLaunchContext(newAppResponse: GetNewApplicationResponse) : ContainerLaunchContext = { ... ... val userClass = if (isClusterMode) { Seq(&quot;--class&quot;, YarnSparkHadoopUtil.escapeForShell(args.userClass)) } else { Nil } ... val amClass = if (isClusterMode) { Class.forName(&quot;org.apache.spark.deploy.yarn.ApplicationMaster&quot;).getName } else { Class.forName(&quot;org.apache.spark.deploy.yarn.ExecutorLauncher&quot;).getName } val amArgs = Seq(amClass) ++ userClass ++ userJar ++ primaryPyFile ++ pyFiles ++ userArgs ++ Seq( &quot;--executor-memory&quot;, args.executorMemory.toString + &quot;m&quot;, &quot;--executor-cores&quot;, args.executorCores.toString, &quot;--num-executors &quot;, args.numExecutors.toString) val commands = prefixEnv ++ Seq(YarnSparkHadoopUtil.expandEnvironment(Environment.JAVA_HOME) + &quot;/bin/java&quot;, &quot;-server&quot; ) ++ javaOpts ++ amArgs ++ ... ... val printableCommands = commands.map(s =&gt; if (s == null) &quot;null&quot; else s).toList amContainer.setCommands(printableCommands) } ... ... def createApplicationSubmissionContext( newApp: YarnClientApplication, containerContext: ContainerLaunchContext): ApplicationSubmissionContext = { val appContext = newApp.getApplicationSubmissionContext appContext.setApplicationName(args.appName) appContext.setQueue(args.amQueue) appContext.setAMContainerSpec(containerContext) appContext.setApplicationType(&quot;SPARK&quot;) sparkConf.getOption(&quot;spark.yarn.maxAppAttempts&quot;).map(_.toInt) match { case Some(v) =&gt; appContext.setMaxAppAttempts(v) case None =&gt; logDebug(&quot;spark.yarn.maxAppAttempts is not set. &quot; + &quot;Cluster&apos;s default value will be used.&quot;) } val capability = Records.newRecord(classOf[Resource]) capability.setMemory(args.amMemory + amMemoryOverhead) capability.setVirtualCores(args.amCores) appContext.setResource(capability) appContext } //yarnClient.submitApplication(appContext)æäº¤çš„çœŸå®å¤„ @Override public ApplicationId submitApplication(ApplicationSubmissionContext appContext) throws YarnException, IOException { ... //æ­¤å¤„é€šè¿‡yarnçš„åè®®å¯¹applicationMasterè¿›è¡Œæäº¤å’Œå¯åŠ¨ ï¼ˆæ­¤å¤„ä¸ºä¸ªäººç†è§£æœ‰ç–‘æƒ‘ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œè¿˜æœ›ç•™è¨€åˆ†äº«ï¼Œä¼šç«‹å³ä½œå‡ºæ›´æ­£ï¼‰ SubmitApplicationRequest request = Records.newRecord(SubmitApplicationRequest.class); request.setApplicationSubmissionContext(appContext); ... æ­¤å¤„clientçš„äº‹æƒ…éƒ½å·²ç»åšå®Œäº†ï¼Œè¯·æ‘„å½±å¸ˆå°†é•œå¤´åˆ‡æ¢åˆ°applicationMaster å°ç»†èŠ‚ç”¨æˆ·ä¸šåŠ¡ä»£ç ä¿¡æ¯çš„å°è£…åŠæµè½¬ æˆ‘ä»¬æäº¤çš„classçš„å°è£…æµç¨‹ â€‹ -&gt;sublimitçš„prepareSubmitEnvironmentä¸­å°è£…åˆ°childArgsä¸­--class -&gt;ä¼ å…¥åˆ°clientçš„æ„é€ å‡½æ•°é‡Œé¢ä½œä¸ºclientArgsï¼Œå°†å…¶å°è£…åˆ°userClasså±æ€§é‡Œé¢ åœ¨submitApplicationä¸­createContainerLaunchContextä¼šå°†å…¶é€šè¿‡é‡æ–°å°åˆ°userClass userClass-&gt;amArgs-&gt;commands-&gt;printableCommands -&gt;amContainer.setCommands(printableCommands) åœ¨æ­¤ï¼ŒcreateContainerLaunchContextæ–¹æ³•æ¥æ”¶åˆ°amContainerèµ‹åä¸ºcontainerContextä¼ é€’ç»™createApplicationSubmissionContext(..,containerContext) é‚£ä¹ˆåœ¨createApplicationSubmissionContextä¸­åˆæœ‰å“ªäº›æƒŠå¤©å˜åŒ–ï¼ˆå…¶å®å¹¶æ²¡æœ‰ï¼‰ appContext.setAMContainerSpec(containerContext) é‚£ä¹ˆappContextä½œä¸ºcreateApplicationSubmissionContextæ–¹æ³•è¿”å›å€¼ï¼Œç”±appContextæ¥æ”¶ï¼Œçœ‹ç  appContext = createApplicationSubmissionContext(newApp, containerContext) æœ€åï¼Œç”±yarnClientImplæäº¤ yarnClient.submitApplication(appContext) ç åˆæ¥äº†ï¼Œæœ€ç»ˆæ‰§è¡Œçš„æ˜¯ SubmitApplicationRequest request =Records.newRecord(SubmitApplicationRequest.class); å¯åŠ¨applicationMaster å¯¹äºclientçš„å°è£…ï¼Œå¯¹äºapplicationMasteréœ€è¦å¯åŠ¨çš„ä¿¡æ¯(å¦‚èµ„æºä¿¡æ¯)åŠç”¨æˆ·æäº¤çš„ä¸šåŠ¡ä»£ç ï¼ˆwordcountçš„ç±»ä¿¡æ¯ï¼‰ä¿¡æ¯éƒ½å·²ç»å°è£…åˆ°appContextï¼Œå¹¶ä¸”ä¼ é€’åˆ°applicationmasterï¼Œé‚£ä¹ˆæ¥çœ‹çœ‹applicationMasterçš„æ‰§è¡Œæµç¨‹ã€‚ ç¨‹åºè°ƒç”¨ç»“æ„ ApplicationMaster.main() -&gt;run() -&gt;runDriver() -&gt;run() -&gt;startUserApplication() //å¯åŠ¨userClass -&gt;userClassLoader.loadClass(args.userClass) .getMethod(&quot;main&quot;, classOf[Array[String]]) -&gt;mainMethod.invoke(null, mainArgs) runAMActor() registerAM() -&gt;yarnRmClient.register()-&gt;return new YarnAllocater(......) -&gt;yarnAllocator.allocateResources() -&gt;yarnAllocator.handleAllocatedContainers() //å¯åŠ¨executor -&gt;yarnAllocator.runAllocatedContainers(containersToUse) runAllocatedContainers(containersToUse)æ˜¯å»å¯åŠ¨ executorï¼Œæœ€ç»ˆçœŸæ­£æ‰§è¡Œå¯åŠ¨Containerçš„æ˜¯åœ¨ ExecutorRunnable.run()ä¸­ã€‚ åˆ›å»ºäº† NMClient å®¢æˆ·ç«¯è°ƒç”¨æä¾›çš„ API æœ€ç»ˆå®ç°åœ¨ NM ä¸Šå¯åŠ¨ Containerï¼Œå…·ä½“å¦‚ä½•å¯åŠ¨ Container å°†åœ¨åæ–‡ä¸­è¿›è¡Œä»‹ç»ã€‚ â€‹ launcherPoolçº¿ç¨‹æ± ä¼šå°†containerï¼Œdriverç­‰ç›¸å…³ä¿¡æ¯å°è£…æˆExecutorRunnableå¯¹è±¡ï¼Œé€šè¿‡ExecutorRunnableå¯åŠ¨æ–°çš„containerä»¥è¿è¡Œexecutorã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼ŒæŒ‡å®šå¯åŠ¨executorçš„ç±»æ˜¯ org.apache.spark.executor.CoarseGrainedExecutorBackendã€‚spark yarn cluster æ¨¡å¼ä¸‹ä»»åŠ¡æäº¤å’Œè®¡ç®—æµç¨‹åˆ†æ ç¨‹åºçš„ç»†èŠ‚ def main(args: Array[String]) = { SignalLogger.register(log) val amArgs = new ApplicationMasterArguments(args) SparkHadoopUtil.get.runAsSparkUser { () =&gt; master = new ApplicationMaster(amArgs, new YarnRMClient(amArgs)) System.exit(master.run()) } } ...... final def run(): Int = { .... if (isClusterMode) { runDriver(securityMgr) } else { runExecutorLauncher(securityMgr) } ... } private def runDriver(securityMgr: SecurityManager): Unit = { addAmIpFilter() // TODO: å¯åŠ¨æˆ‘ä»¬è‡ªå®šçš„ç±»ï¼Œä¹Ÿå°±æ˜¯å¯åŠ¨submité‡Œé¢çš„--classçš„ä¸œè¥¿ userClassThread = startUserApplication() val sc = waitForSparkContextInitialized() ... actorSystem = sc.env.actorSystem runAMActor( sc.getConf.get(&quot;spark.driver.host&quot;), sc.getConf.get(&quot;spark.driver.port&quot;), isClusterMode = true) registerAM(sc.ui.map(_.appUIAddress).getOrElse(&quot;&quot;), securityMgr) userClassThread.join() ... } åœ¨ApplicationMasterArgumentsè®¾ç½®äº†è¦å¯åŠ¨çš„ä¿¡æ¯ class ApplicationMasterArguments(val args: Array[String]) { var userJar: String = null var userClass: String = null var primaryPyFile: String = null var pyFiles: String = null var userArgs: Seq[String] = Seq[String]() var executorMemory = 1024 var executorCores = 1 var numExecutors = DEFAULT_NUMBER_EXECUTORS ...... } startUserApplication ä¸»è¦æ‰§è¡Œäº†è°ƒç”¨ç”¨æˆ·çš„ä»£ç ï¼Œä»¥åŠåˆ›å»ºäº†ä¸€ä¸ª spark driver çš„è¿›ç¨‹ã€‚ Start the user class, which contains the spark driver, in a separate Thread. private def startUserApplication(): Thread = { val classpath = Client.getUserClasspath(sparkConf) val urls = classpath.map { entry =&gt; new URL(&quot;file:&quot; + new File(entry.getPath()).getAbsolutePath()) } val userClassLoader = ... // TODO: userClasså°±æ˜¯submité‡Œé¢çš„--class æäº¤çš„ç±» val mainMethod = userClassLoader.loadClass(args.userClass) .getMethod(&quot;main&quot;, classOf[Array[String]]) userThread.setContextClassLoader(userClassLoader) userThread.setName(&quot;Driver&quot;) userThread.start() userThread } ä»userThread.setName(â€œDriverâ€)ä¹Ÿå¯ä»¥çœ‹å‡ºåˆ›å»ºçš„æ˜¯åä¸ºdriverçš„è¿›ç¨‹ registerAM å‘ resourceManager ä¸­æ­£å¼æ³¨å†Œ applicationMasterã€‚æ³¨å†ŒapplicationMaster ä»¥åï¼Œå¹¶ä¸”åˆ†é…èµ„æºï¼Œè¿™æ ·ï¼Œç”¨æˆ·ä»£ç å°±å¯ä»¥æ‰§è¡Œäº†ï¼Œä»»åŠ¡åˆ‡åˆ†ã€è°ƒåº¦ã€æ‰§è¡Œã€‚ ç„¶åï¼Œç”¨æˆ·ä»£ç ä¸­çš„ action ä¼šè°ƒç”¨ SparkContext çš„ runJobï¼ŒSparkContext ä¸­æœ‰å¾ˆå¤šä¸ª runJobï¼Œä½†æœ€åéƒ½æ˜¯è°ƒç”¨ DAGScheduler çš„ runJob // registerAM private def registerAM(uiAddress: String, securityMgr: SecurityManager) = { ..... allocator = client.register(yarnConf, if (sc != null) sc.getConf else sparkConf, if (sc != null) sc.preferredNodeLocationData else Map(), uiAddress, historyAddress, securityMgr) //ä¸ºexectoråˆ†é…èµ„æº allocator.allocateResources() reporterThread = launchReporterThread() ...... }","categories":[{"name":"Spark-On-Yarn","slug":"Spark-On-Yarn","permalink":"http://gangtieguo.cn/categories/Spark-On-Yarn/"}],"tags":[{"name":"åŸç†","slug":"åŸç†","permalink":"http://gangtieguo.cn/tags/åŸç†/"},{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"}]},{"title":"Spark-on-Yarnæºç è§£æ(äºŒ)Spark-Submitè§£æ","slug":"Spark-on-Yarnæºç è§£æâ‘¡Spark-Submitè§£æ","date":"2018-09-04T08:03:50.700Z","updated":"2019-03-11T07:44:48.567Z","comments":true,"path":"2018/09/04/Spark-on-Yarnæºç è§£æâ‘¡Spark-Submitè§£æ/","link":"","permalink":"http://gangtieguo.cn/2018/09/04/Spark-on-Yarnæºç è§£æâ‘¡Spark-Submitè§£æ/","excerpt":"[TOC] spark-on-yarnç³»åˆ— Spark-on-Yarn æºç è§£æâ‘ Yarn ä»»åŠ¡è§£æSpark-on-Yarn æºç è§£æâ‘¡Spark-Submit è§£æSpark-on-Yarn æºç è§£æâ‘¢client åšçš„äº‹æƒ…Spark-on-Yarn æºç è§£æâ‘£Spark ä¸šåŠ¡ä»£ç çš„æ‰§è¡ŒåŠå…¶ä»»åŠ¡åˆ†é…è°ƒåº¦ stage åˆ’åˆ† ä¸Šæ–‡æˆ‘ä»¬äº†è§£åˆ°äº†yarnçš„æ¶æ„å’Œæ‰§è¡Œä»»åŠ¡çš„æµç¨‹ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹ spark-submitå‘½ä»¤$SPARK_HOME/bin/spark-submit \\ --master yarn \\ //æäº¤æ¨¡å¼ yarn --deploy-mode cluster \\ //è¿è¡Œçš„æ¨¡å¼ï¼Œè¿˜æœ‰ä¸€ç§clientæ¨¡å¼ï¼Œä½†å¤§å¤šç”¨äºè°ƒè¯•ï¼Œæ­¤å¤„ä½¿ç”¨clusteræ¨¡å¼ --class me.yao.spark.me.yao.spark.WordCount \\ //æäº¤çš„ä»»åŠ¡ --name &quot;wc&quot; \\ //ä»»åŠ¡åå­— --queue root.default \\ //æäº¤çš„é˜Ÿåˆ— --driver-memory 3g \\ //ä¸ºdriverç”³è¯·çš„å†…å­˜ --num-executors 1 \\ //executorsçš„æ•°é‡ï¼Œå¯ä»¥ç†è§£ä¸ºçº¿ç¨‹æ•°ï¼Œå¯¹åº”yarnä¸­çš„Containerä¸ªæ•° --executor-memory 6g \\ //ä¸ºæ¯ä¸€ä¸ªexecutorç”³è¯·çš„å†…å­˜ --executor-cores 4 \\ //ä¸ºæ¯ä¸€ä¸ªexecutorç”³è¯·çš„core --conf spark.yarn.driver.memoryOverhead=1g \\ //driverå¯ä½¿ç”¨çš„éå †å†…å­˜ï¼Œè¿™äº›å†…å­˜ç”¨äºå¦‚VMï¼Œå­—ç¬¦ ä¸²å¸¸é‡æ± ä»¥åŠå…¶ä»–é¢å¤–æœ¬åœ°å¼€é”€ç­‰ --conf spark.yarn.executor.memoryOverhead=2g \\ //æ¯ä¸ªexecutorå¯ä½¿ç”¨çš„éå †å†…å­˜ï¼Œè¿™äº›å†…å­˜ç”¨äºå¦‚ VMï¼Œå­—ç¬¦ä¸²å¸¸é‡æ± ä»¥åŠå…¶ä»–é¢å¤–æœ¬åœ°å¼€é”€ç­‰ è¿™æ˜¯é€šå¸¸æˆ‘ä»¬æäº¤sparkç¨‹åºçš„submitå‘½ä»¤ï¼Œä»¥æ­¤ä¸ºåˆ‡å…¥ç‚¹ï¼Œå¯¹sparkç¨‹åºçš„è¿è¡Œæµç¨‹åšä¸€ä¸ªè·Ÿè¸ªå’Œåˆ†æã€‚","text":"[TOC] spark-on-yarnç³»åˆ— Spark-on-Yarn æºç è§£æâ‘ Yarn ä»»åŠ¡è§£æSpark-on-Yarn æºç è§£æâ‘¡Spark-Submit è§£æSpark-on-Yarn æºç è§£æâ‘¢client åšçš„äº‹æƒ…Spark-on-Yarn æºç è§£æâ‘£Spark ä¸šåŠ¡ä»£ç çš„æ‰§è¡ŒåŠå…¶ä»»åŠ¡åˆ†é…è°ƒåº¦ stage åˆ’åˆ† ä¸Šæ–‡æˆ‘ä»¬äº†è§£åˆ°äº†yarnçš„æ¶æ„å’Œæ‰§è¡Œä»»åŠ¡çš„æµç¨‹ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹ spark-submitå‘½ä»¤$SPARK_HOME/bin/spark-submit \\ --master yarn \\ //æäº¤æ¨¡å¼ yarn --deploy-mode cluster \\ //è¿è¡Œçš„æ¨¡å¼ï¼Œè¿˜æœ‰ä¸€ç§clientæ¨¡å¼ï¼Œä½†å¤§å¤šç”¨äºè°ƒè¯•ï¼Œæ­¤å¤„ä½¿ç”¨clusteræ¨¡å¼ --class me.yao.spark.me.yao.spark.WordCount \\ //æäº¤çš„ä»»åŠ¡ --name &quot;wc&quot; \\ //ä»»åŠ¡åå­— --queue root.default \\ //æäº¤çš„é˜Ÿåˆ— --driver-memory 3g \\ //ä¸ºdriverç”³è¯·çš„å†…å­˜ --num-executors 1 \\ //executorsçš„æ•°é‡ï¼Œå¯ä»¥ç†è§£ä¸ºçº¿ç¨‹æ•°ï¼Œå¯¹åº”yarnä¸­çš„Containerä¸ªæ•° --executor-memory 6g \\ //ä¸ºæ¯ä¸€ä¸ªexecutorç”³è¯·çš„å†…å­˜ --executor-cores 4 \\ //ä¸ºæ¯ä¸€ä¸ªexecutorç”³è¯·çš„core --conf spark.yarn.driver.memoryOverhead=1g \\ //driverå¯ä½¿ç”¨çš„éå †å†…å­˜ï¼Œè¿™äº›å†…å­˜ç”¨äºå¦‚VMï¼Œå­—ç¬¦ ä¸²å¸¸é‡æ± ä»¥åŠå…¶ä»–é¢å¤–æœ¬åœ°å¼€é”€ç­‰ --conf spark.yarn.executor.memoryOverhead=2g \\ //æ¯ä¸ªexecutorå¯ä½¿ç”¨çš„éå †å†…å­˜ï¼Œè¿™äº›å†…å­˜ç”¨äºå¦‚ VMï¼Œå­—ç¬¦ä¸²å¸¸é‡æ± ä»¥åŠå…¶ä»–é¢å¤–æœ¬åœ°å¼€é”€ç­‰ è¿™æ˜¯é€šå¸¸æˆ‘ä»¬æäº¤sparkç¨‹åºçš„submitå‘½ä»¤ï¼Œä»¥æ­¤ä¸ºåˆ‡å…¥ç‚¹ï¼Œå¯¹sparkç¨‹åºçš„è¿è¡Œæµç¨‹åšä¸€ä¸ªè·Ÿè¸ªå’Œåˆ†æã€‚æŸ¥çœ‹spark-submitè„šæœ¬ æŸ¥çœ‹spark-submitè„šæœ¬çš„ä¿¡æ¯ï¼Œåˆæ­¥å¯ä»¥çœ‹åˆ°submitå¯åŠ¨çš„ç±»ä¸ºorg.apache.spark.deploy.SparkSubmitï¼Œæ›´å¤šç»†èŠ‚å…¶å®ä¸é‡è¦ï¼ˆå¼€ä¸ªå¼€ç©ï¼Œæå®¢å¯ä»¥æ±‚ç”šè§£ï¼‰å¦‚æœè§‰å¾—è¦æ·±ç©¶ä¸€ä¸‹ä¸ºä»€ä¹ˆæ˜¯submitçš„mainæ–¹æ³•çš„å¯ä»¥å‚è€ƒä¸€ä¸‹spark on yarn ä½œä¸šæäº¤æºç åˆ†æ æ¥ä¸‹æ¥æŸ¥çœ‹è¯¥ç±»å†…éƒ¨çš„å¤„ç†é€»è¾‘ SparkSumbmitçš„ç±»ï¼ˆä¸ºäº†ç®€æ´å’Œæ–‡ç« ç¯‡å¹…ï¼Œåªä¿ç•™äº†å…³é”®æµç¨‹çš„ä¿¡æ¯ï¼‰ def main(args: Array[String]): Unit = { val appArgs = new SparkSubmitArguments(args) if (appArgs.verbose) { printStream.println(appArgs) } appArgs.action match { case SparkSubmitAction.SUBMIT =&gt; submit(appArgs) case SparkSubmitAction.KILL =&gt; kill(appArgs) case SparkSubmitAction.REQUEST_STATUS =&gt; requestStatus(appArgs) } } ...... private[spark] def submit(args: SparkSubmitArguments): Unit = { val (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args) ..... ..... runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose) } private[spark] def prepareSubmitEnvironment(args: SparkSubmitArguments) : (Seq[String], Seq[String], Map[String, String], String) = { ...... // In yarn-cluster mode, use yarn.Client as a wrapper around the user class if (isYarnCluster) { childMainClass = &quot;org.apache.spark.deploy.yarn.Client&quot; ....... } //åœ¨submitæ–¹æ³•ä¸­æœ€ç»ˆè°ƒç”¨çš„æ˜¯ runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose) try { mainClass = Class.forName(childMainClass, true, loader) } catch { ...... System.exit(CLASS_NOT_FOUND_EXIT_STATUS) } // SPARK-4170 private def runMain( childArgs: Seq[String], childClasspath: Seq[String], sysProps: Map[String, String], childMainClass: String, verbose: Boolean): Unit = { ... ... mainClass = Class.forName(childMainClass, true, loader) ... ... val mainMethod = mainClass.getMethod(&quot;main&quot;, new Array[String](0).getClass) ... ... mainMethod.invoke(null, childArgs.toArray) ... ... } é€šè¿‡ä¸Šé¢çš„æµç¨‹å¯ä»¥çœ‹åˆ°ï¼Œè¿™æ ·ä¸€ä¸ªè°ƒç”¨é“¾(æœªç‰¹æ®Šè¡¨æ˜ç±»åï¼Œè¡¨æ˜ä¸ºè¯¥æ­¥ä¸Šä¸€æ­¥çš„åŒä¸€ç±»)ï¼Œæˆ‘ä»¬ä»£ç ç®€åŒ–ä¸€ä¸‹ï¼Œçœ‹å¾—èˆ’å¿ƒæ˜äº†ï¼Œå†é…ä¸Šè§£è¯´ submit.main() -&gt;submit()æ¨¡å¼åŒ¹é…åˆ°è¯¥æ–¹æ³•ï¼Œå› ä¸ºæˆ‘ä»¬å°±æ˜¯submitæäº¤ä»»åŠ¡ -&gt;prepareSubmitEnvironment()è¯¥æ–¹æ³•ä¸­æŒ‡æ˜äº†è¦å¯åŠ¨çš„ç±»ï¼Œå°±æ˜¯å¤§æ˜æ¹–ç•”çš„Client -&gt;runMain()é€šè¿‡ä¸Šæ­¥æŒ‡å®šçš„ç±»ï¼Œç„¶åé€šè¿‡åå°„è°ƒç”¨mainæ–¹æ³• æ—¢ç„¶æˆ‘ä»¬çš„çº¿è·¯èµ°åˆ°org.apache.spark.deploy.yarn.Client ï¼Œé‚£æˆ‘ä»¬å†å»è¿™ä¸ªç±»ä¸€çœ‹ç©¶ç«Ÿï¼Œä¸”å¬ä¸‹å›åˆ†è§£","categories":[{"name":"Spark-On-Yarn","slug":"Spark-On-Yarn","permalink":"http://gangtieguo.cn/categories/Spark-On-Yarn/"}],"tags":[{"name":"åŸç†","slug":"åŸç†","permalink":"http://gangtieguo.cn/tags/åŸç†/"},{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"}]},{"title":"Spark-on-Yarnæºç è§£æ(ä¸€)Yarnä»»åŠ¡è§£æ","slug":"Spark-on-Yarnæºç è§£æâ‘ Yarnä»»åŠ¡è§£æ","date":"2018-09-04T07:44:58.661Z","updated":"2019-06-17T04:40:09.397Z","comments":true,"path":"2018/09/04/Spark-on-Yarnæºç è§£æâ‘ Yarnä»»åŠ¡è§£æ/","link":"","permalink":"http://gangtieguo.cn/2018/09/04/Spark-on-Yarnæºç è§£æâ‘ Yarnä»»åŠ¡è§£æ/","excerpt":"[TOC] spark-on-yarnç³»åˆ—Spark-on-Yarn æºç è§£æâ‘ Yarn ä»»åŠ¡è§£æSpark-on-Yarn æºç è§£æâ‘¡Spark-Submit è§£æSpark-on-Yarn æºç è§£æâ‘¢client åšçš„äº‹æƒ…Spark-on-Yarn æºç è§£æâ‘£Spark ä¸šåŠ¡ä»£ç çš„æ‰§è¡ŒåŠå…¶ä»»åŠ¡åˆ†é…è°ƒåº¦ stage åˆ’åˆ† äº†è§£spark-on-yarn,é¦–å…ˆæˆ‘ä»¬äº†è§£ä¸€ä¸‹yarnæäº¤çš„æµç¨‹ï¼Œä¿—è¯è¯´ï¼Œæ¬²ç»ƒæ­¤åŠŸï¼Œé”™äº†ï¼Œæˆ‘ä»¬è¿˜æ˜¯å…ˆçœ‹å§ yarnä»»åŠ¡çš„æäº¤YARN çš„åŸºæœ¬æ¶æ„å’Œå·¥ä½œæµç¨‹ YARN çš„åŸºæœ¬æ¶æ„å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œç”±ä¸‰å¤§åŠŸèƒ½æ¨¡å—ç»„æˆï¼Œåˆ†åˆ«æ˜¯ 1) RM (ResourceManager) 2) NM (Node Manager) 3) AM(Application Master)","text":"[TOC] spark-on-yarnç³»åˆ—Spark-on-Yarn æºç è§£æâ‘ Yarn ä»»åŠ¡è§£æSpark-on-Yarn æºç è§£æâ‘¡Spark-Submit è§£æSpark-on-Yarn æºç è§£æâ‘¢client åšçš„äº‹æƒ…Spark-on-Yarn æºç è§£æâ‘£Spark ä¸šåŠ¡ä»£ç çš„æ‰§è¡ŒåŠå…¶ä»»åŠ¡åˆ†é…è°ƒåº¦ stage åˆ’åˆ† äº†è§£spark-on-yarn,é¦–å…ˆæˆ‘ä»¬äº†è§£ä¸€ä¸‹yarnæäº¤çš„æµç¨‹ï¼Œä¿—è¯è¯´ï¼Œæ¬²ç»ƒæ­¤åŠŸï¼Œé”™äº†ï¼Œæˆ‘ä»¬è¿˜æ˜¯å…ˆçœ‹å§ yarnä»»åŠ¡çš„æäº¤YARN çš„åŸºæœ¬æ¶æ„å’Œå·¥ä½œæµç¨‹ YARN çš„åŸºæœ¬æ¶æ„å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œç”±ä¸‰å¤§åŠŸèƒ½æ¨¡å—ç»„æˆï¼Œåˆ†åˆ«æ˜¯ 1) RM (ResourceManager) 2) NM (Node Manager) 3) AM(Application Master)ä½œä¸šæäº¤ ç”¨æˆ·é€šè¿‡ Client å‘ ResourceManager æäº¤ Applicationï¼Œ ResourceManager æ ¹æ®ç”¨æˆ·è¯·æ±‚åˆ†é…åˆé€‚çš„ Container, ç„¶ååœ¨æŒ‡å®šçš„ NodeManager ä¸Šè¿è¡Œ Container ä»¥å¯åŠ¨ ApplicationMaster ApplicationMaster å¯åŠ¨å®Œæˆåï¼Œå‘ ResourceManager æ³¨å†Œè‡ªå·± å¯¹äºç”¨æˆ·çš„ Taskï¼ŒApplicationMaster éœ€è¦é¦–å…ˆè·Ÿ ResourceManager è¿›è¡Œåå•†ä»¥è·å–è¿è¡Œç”¨æˆ· Task æ‰€éœ€è¦çš„ Containerï¼Œåœ¨è·å–æˆåŠŸåï¼ŒApplicationMaster å°†ä»»åŠ¡å‘é€ç»™æŒ‡å®šçš„ NodeManager NodeManager å¯åŠ¨ç›¸åº”çš„ Containerï¼Œå¹¶è¿è¡Œç”¨æˆ· Task Spark-On-Yarnçš„æµç¨‹æäº¤åœ¨ yarn-cluster æ¨¡å¼ä¸‹ï¼ŒSpark driver è¿è¡Œåœ¨ application master è¿›ç¨‹ä¸­ï¼Œè¿™ä¸ªè¿›ç¨‹è¢«é›†ç¾¤ä¸­çš„ YARN æ‰€ç®¡ç†ï¼Œå®¢æˆ·ç«¯ä¼šåœ¨åˆå§‹åŒ–åº”ç”¨ç¨‹åº ä¹‹åå…³é—­ã€‚åœ¨ yarn-client æ¨¡å¼ä¸‹ï¼Œdriver è¿è¡Œåœ¨å®¢æˆ·ç«¯è¿›ç¨‹ä¸­ï¼Œapplication master ä»…ä»…ç”¨æ¥å‘ YARN è¯·æ±‚èµ„æº Spark Driveré¦–å…ˆä½œä¸ºä¸€ä¸ªApplicationMasteråœ¨YARNé›†ç¾¤ä¸­å¯åŠ¨ï¼Œå®¢æˆ·ç«¯æäº¤ç»™ResourceManagerçš„æ—¶å€™ï¼Œæ¯ä¸€ä¸ªjobéƒ½ä¼šåœ¨é›†ç¾¤çš„NodeManagerèŠ‚ç‚¹ä¸Šåˆ†é…ä¸€ä¸ªå”¯ä¸€çš„ApplicationMasterï¼Œç”±è¯¥ApplicationMasterç®¡ç†å…¨ç”Ÿå‘½å‘¨æœŸçš„åº”ç”¨ã€‚å…·ä½“è¿‡ç¨‹ï¼š ç”±clientå‘ResourceManageræäº¤è¯·æ±‚ï¼Œå¹¶ä¸Šä¼ jaråˆ°HDFSä¸Šè¿™æœŸé—´åŒ…æ‹¬å››ä¸ªæ­¥éª¤ï¼ša).è¿æ¥åˆ°RMb).ä»RMçš„ASMï¼ˆApplicationsManager ï¼‰ä¸­è·å¾—metricã€queueå’Œresourceç­‰ä¿¡æ¯ã€‚c). upload app jar and spark-assembly jard).è®¾ç½®è¿è¡Œç¯å¢ƒå’Œcontainerä¸Šä¸‹æ–‡ï¼ˆlaunch-container.shç­‰è„šæœ¬) ResouceManagerå‘NodeManagerç”³è¯·èµ„æºï¼Œåˆ›å»ºSpark ApplicationMasterï¼ˆæ¯ä¸ªSparkContextéƒ½æœ‰ä¸€ä¸ªApplicationMasterï¼‰ NodeManagerå¯åŠ¨ApplicationMasterï¼Œå¹¶å‘ResourceManager AsMæ³¨å†Œ ApplicationMasterä»HDFSä¸­æ‰¾åˆ°jaræ–‡ä»¶ï¼Œå¯åŠ¨SparkContextã€DAGschedulerå’ŒYARN Cluster Scheduler ResourceManagerå‘ResourceManager AsMæ³¨å†Œç”³è¯·containerèµ„æº ResourceManageré€šçŸ¥NodeManageråˆ†é…Containerï¼Œè¿™æ—¶å¯ä»¥æ”¶åˆ°æ¥è‡ªASMå…³äºcontainerçš„æŠ¥å‘Šã€‚ï¼ˆæ¯ä¸ªcontainerå¯¹åº”ä¸€ä¸ªexecutorï¼‰ Spark ApplicationMasterç›´æ¥å’Œcontainerï¼ˆexecutorï¼‰è¿›è¡Œäº¤äº’ï¼Œå®Œæˆè¿™ä¸ªåˆ†å¸ƒå¼ä»»åŠ¡ã€‚ ApplicationMasterå’ŒDriverçš„åŒºåˆ«é¦–å…ˆåŒºåˆ†ä¸‹ AppMaster å’Œ Driverï¼Œä»»ä½•ä¸€ä¸ª yarn ä¸Šè¿è¡Œçš„ä»»åŠ¡éƒ½å¿…é¡»æœ‰ä¸€ä¸ª AppMasterï¼Œè€Œä»»ä½•ä¸€ä¸ª Spark ä»»åŠ¡éƒ½ä¼šæœ‰ä¸€ä¸ª Driverï¼ŒDriver å°±æ˜¯è¿è¡Œ SparkContext(å®ƒä¼šæ„å»º TaskScheduler å’Œ DAGScheduler) çš„è¿›ç¨‹ï¼Œå½“ç„¶åœ¨ Driver ä¸Šä½ ä¹Ÿå¯ä»¥åšå¾ˆå¤šé Spark çš„äº‹æƒ…ï¼Œè¿™äº›äº‹æƒ…åªä¼šåœ¨ Driver ä¸Šé¢æ‰§è¡Œï¼Œè€Œç”± SparkContext ä¸Šç‰µå¼•å‡ºæ¥çš„ä»£ç åˆ™ä¼šç”± DAGScheduler åˆ†æï¼Œå¹¶å½¢æˆ Job å’Œ Stage äº¤ç”± TaskSchedulerï¼Œå†ç”± TaskScheduler äº¤ç”±å„ Executor åˆ†å¸ƒå¼æ‰§è¡Œã€‚ æ‰€ä»¥ Driver å’Œ AppMaster æ˜¯ä¸¤ä¸ªå®Œå…¨ä¸åŒçš„ä¸œè¥¿ï¼ŒDriver æ˜¯æ§åˆ¶ Spark è®¡ç®—å’Œä»»åŠ¡èµ„æºçš„ï¼Œè€Œ AppMaster æ˜¯æ§åˆ¶ yarn app è¿è¡Œå’Œä»»åŠ¡èµ„æºçš„ï¼Œåªä¸è¿‡åœ¨ Spark on Yarn ä¸Šï¼Œè¿™ä¸¤è€…å°±å‡ºç°äº†äº¤å‰ï¼Œè€Œåœ¨ standalone æ¨¡å¼ä¸‹ï¼Œèµ„æºåˆ™ç”± Driver ç®¡ç†ã€‚åœ¨ Spark on Yarn ä¸Šï¼ŒDriver ä¼šå’Œ AppMaster é€šä¿¡ï¼Œèµ„æºçš„ç”³è¯·ç”± AppMaster æ¥å®Œæˆï¼Œè€Œä»»åŠ¡çš„è°ƒåº¦å’Œæ‰§è¡Œåˆ™ç”± Driver å®Œæˆï¼ŒDriver ä¼šé€šè¿‡ä¸ AppMaster é€šä¿¡æ¥è®© Executor çš„æ‰§è¡Œå…·ä½“çš„ä»»åŠ¡ã€‚ Spark on Yarn","categories":[{"name":"Spark-On-Yarn","slug":"Spark-On-Yarn","permalink":"http://gangtieguo.cn/categories/Spark-On-Yarn/"}],"tags":[{"name":"åŸç†","slug":"åŸç†","permalink":"http://gangtieguo.cn/tags/åŸç†/"},{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"},{"name":"Yarn","slug":"Yarn","permalink":"http://gangtieguo.cn/tags/Yarn/"}]},{"title":"MapReduceä¸­Shuffleä¸­çš„æœºåˆ¶","slug":"MapReduceä¸­Shuffleä¸­çš„æœºåˆ¶","date":"2018-08-20T02:12:26.791Z","updated":"2019-06-17T04:40:09.393Z","comments":true,"path":"2018/08/20/MapReduceä¸­Shuffleä¸­çš„æœºåˆ¶/","link":"","permalink":"http://gangtieguo.cn/2018/08/20/MapReduceä¸­Shuffleä¸­çš„æœºåˆ¶/","excerpt":"[TOC] å®˜æ–¹çš„shuffleæµç¨‹ shuffleåŸç†æåˆ°MapReduceï¼Œå°±ä¸å¾—ä¸æä¸€ä¸‹shuffleã€‚ MapReduce æ¡†æ¶çš„æ ¸å¿ƒæ­¥éª¤ä¸»è¦åˆ†ä¸¤éƒ¨åˆ†ï¼šMap å’ŒReduceï¼Œä¸€ä¸ªæ˜¯ç‹¬ç«‹å¹¶å‘ï¼Œä¸€ä¸ªæ˜¯æ±‡èšã€‚å½“ä½ å‘MapReduce æ¡†æ¶æäº¤ä¸€ä¸ªè®¡ç®—ä½œä¸šæ—¶ï¼Œå®ƒä¼šé¦–å…ˆæŠŠè®¡ç®—ä½œä¸šæ‹†åˆ†æˆè‹¥å¹²ä¸ªMap ä»»åŠ¡ï¼Œç„¶ååˆ†é…åˆ°ä¸åŒçš„èŠ‚ç‚¹ä¸Šå»æ‰§è¡Œï¼Œæ¯ä¸€ä¸ªMap ä»»åŠ¡å¤„ç†è¾“å…¥æ•°æ®ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œå½“Map ä»»åŠ¡å®Œæˆåï¼Œå®ƒä¼šç”Ÿæˆä¸€äº›ä¸­é—´æ–‡ä»¶ï¼Œè¿™äº›ä¸­é—´æ–‡ä»¶å°†ä¼šä½œä¸ºReduce ä»»åŠ¡çš„è¾“å…¥æ•°æ®ã€‚Reduce ä»»åŠ¡çš„ä¸»è¦ç›®æ ‡å°±æ˜¯æŠŠå‰é¢è‹¥å¹²ä¸ªMap çš„è¾“å‡ºæ±‡æ€»åˆ°ä¸€èµ·å¹¶è¾“å‡ºã€‚","text":"[TOC] å®˜æ–¹çš„shuffleæµç¨‹ shuffleåŸç†æåˆ°MapReduceï¼Œå°±ä¸å¾—ä¸æä¸€ä¸‹shuffleã€‚ MapReduce æ¡†æ¶çš„æ ¸å¿ƒæ­¥éª¤ä¸»è¦åˆ†ä¸¤éƒ¨åˆ†ï¼šMap å’ŒReduceï¼Œä¸€ä¸ªæ˜¯ç‹¬ç«‹å¹¶å‘ï¼Œä¸€ä¸ªæ˜¯æ±‡èšã€‚å½“ä½ å‘MapReduce æ¡†æ¶æäº¤ä¸€ä¸ªè®¡ç®—ä½œä¸šæ—¶ï¼Œå®ƒä¼šé¦–å…ˆæŠŠè®¡ç®—ä½œä¸šæ‹†åˆ†æˆè‹¥å¹²ä¸ªMap ä»»åŠ¡ï¼Œç„¶ååˆ†é…åˆ°ä¸åŒçš„èŠ‚ç‚¹ä¸Šå»æ‰§è¡Œï¼Œæ¯ä¸€ä¸ªMap ä»»åŠ¡å¤„ç†è¾“å…¥æ•°æ®ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œå½“Map ä»»åŠ¡å®Œæˆåï¼Œå®ƒä¼šç”Ÿæˆä¸€äº›ä¸­é—´æ–‡ä»¶ï¼Œè¿™äº›ä¸­é—´æ–‡ä»¶å°†ä¼šä½œä¸ºReduce ä»»åŠ¡çš„è¾“å…¥æ•°æ®ã€‚Reduce ä»»åŠ¡çš„ä¸»è¦ç›®æ ‡å°±æ˜¯æŠŠå‰é¢è‹¥å¹²ä¸ªMap çš„è¾“å‡ºæ±‡æ€»åˆ°ä¸€èµ·å¹¶è¾“å‡ºã€‚ æˆ‘ä»¬çŸ¥é“æ¯ä¸ªreduce taskè¾“å…¥çš„keyéƒ½æ˜¯æŒ‰ç…§keyæ’åºçš„ã€‚ä½†æ˜¯æ¯ä¸ªmapçš„è¾“å‡ºåªæ˜¯ç®€å•çš„key-valueè€Œékey-valuelistï¼Œæ‰€ä»¥shuffleçš„å·¥ä½œå°±æ˜¯å°†mapè¾“å‡ºè½¬åŒ–ä¸ºreducerçš„è¾“å…¥çš„è¿‡ç¨‹ã€‚Shuffleè¿‡ç¨‹æ˜¯æŒ‡mapäº§ç”Ÿè¾“å‡ºç»“æœå¼€å§‹ï¼ŒåŒ…æ‹¬ç³»ç»Ÿæ‰§è¡Œåˆ†åŒºpartitionï¼Œæ’åºsortï¼ŒèšåˆCombinerï¼ˆå¦‚æœ‰ï¼‰ä»¥åŠä¼ é€mapçš„è¾“å‡ºåˆ°Reducerä½œä¸ºè¾“å…¥çš„è¿‡ç¨‹ã€‚ shuffleè¿‡ç¨‹åˆ†æ mapç«¯ ä»mapæ®µå¼€å§‹åˆ†æï¼Œå½“Mapå¼€å§‹äº§ç”Ÿè¾“å‡ºçš„æ—¶å€™ï¼Œå¹¶ä¸æ˜¯ç®€å•å§æ•°æ®å†™åˆ°ç£ç›˜ï¼Œå› ä¸ºé¢‘ç¹çš„æ“ä½œä¼šå¯¼è‡´æ€§èƒ½ä¸¥é‡ä¸‹é™ï¼Œé¦–å…ˆå°†æ•°æ®å†™å…¥åˆ°ä¸€ä¸ªç¯å½¢ç¼“å†²åŒºï¼ˆæ¯ä¸ªmaptaskéƒ½ä¼šæœ‰ä¸€ä¸ªç¯å½¢ç¼“å†²åŒºï¼Œé»˜è®¤100Mï¼Œå¯ä»¥é€šè¿‡io.sort.mbå±æ€§æ¥è®¾ç½®å…·ä½“çš„å¤§å°ï¼‰å¹¶åšä¸€äº›é¢„æ’åºï¼Œä»¥æå‡æ•ˆç‡ï¼Œå½“ç¼“å†²åŒºä¸­çš„æ•°æ®é‡è¾¾åˆ°ä¸€ä¸ªç‰¹å®šçš„é˜€å€¼(io.sort.mb * io.sort.spill.percentï¼Œå…¶ä¸­io.sort.spill.percent é»˜è®¤æ˜¯0.80ï¼Œå³é»˜è®¤ä¸º 80MBï¼‰ï¼Œæº¢å†™çº¿ç¨‹å¯åŠ¨ã€‚ ç³»ç»Ÿå°†ä¼šå¯åŠ¨ä¸€ä¸ªåå°çº¿ç¨‹æŠŠç¼“å†²åŒºä¸­çš„å†…å®¹spill åˆ°ç£ç›˜ã€‚å³ä¼šé”å®šè¿™80MBçš„å†…å­˜ï¼Œæ‰§è¡Œæº¢å†™è¿‡ç¨‹ã€‚Map taskçš„è¾“å‡ºç»“æœè¿˜å¯ä»¥å¾€å‰©ä¸‹çš„20MBå†…å­˜ä¸­å†™ï¼Œäº’ä¸å½±å“ã€‚åœ¨spillè¿‡ç¨‹ä¸­ï¼ŒMapçš„è¾“å‡ºå°†ä¼šç»§ç»­å†™å…¥åˆ°ç¼“å†²åŒºï¼Œä½†å¦‚æœç¼“å†²åŒºå·²ç»æ»¡äº†ï¼ŒMapå°±ä¼šè¢«é˜»å¡ç›´åˆ°spillå®Œæˆã€‚spillçº¿ç¨‹åœ¨æŠŠç¼“å†²åŒºçš„æ•°æ®å†™åˆ°ç£ç›˜å‰ï¼Œä¼šå¯¹ä»– è¿›è¡Œä¸€ä¸ªäºŒæ¬¡æ’åºï¼Œé¦–å…ˆæ ¹æ®æ•°æ®æ‰€å±çš„partitionæ’åºï¼ˆå¿«é€Ÿæ’åºï¼‰ï¼Œç„¶åæ¯ä¸ªpartitionä¸­å†æŒ‰Keyæ’åºã€‚è¾“å‡ºåŒ…æ‹¬ä¸€ä¸ªç´¢å¼•æ–‡ä»¶å’Œæ•°æ®æ–‡ä»¶ã€‚ å¦‚æœè®¾å®šäº†Combinerï¼Œå°†åœ¨æ’åºè¾“å‡ºçš„åŸºç¡€ä¸Šè¿›è¡Œã€‚Combinerå°±æ˜¯ä¸€ä¸ªMini Reducerï¼Œå®ƒåœ¨æ‰§è¡ŒMapä»»åŠ¡çš„èŠ‚ç‚¹æœ¬èº«è¿è¡Œï¼Œå…ˆå¯¹Mapçš„è¾“å‡ºä½œä¸€æ¬¡ç®€å•çš„Reduceï¼Œæœ‰äº›æ•°æ®å¯èƒ½åƒè¿™æ ·ï¼šâ€œaâ€/1ï¼Œ â€œaâ€/1ï¼Œ â€œaâ€/1ï¼Œä¼šåˆå¹¶æˆ â€œaâ€/3ï¼Œä½¿å¾—æ›´å°‘çš„æ•°æ®ä¼šè¢«å†™å…¥ç£ç›˜å’Œä¼ é€åˆ°Reducerã€‚ Spillæ–‡ä»¶ä¿å­˜åœ¨ç”±mapred.local.diræŒ‡å®šçš„ç›®å½•ä¸­ï¼ŒMapä»»åŠ¡ç»“æŸååˆ é™¤ã€‚æ¯å½“å†…å­˜ä¸­çš„æ•°æ®è¾¾åˆ°spillé˜€å€¼çš„æ—¶å€™ï¼Œéƒ½ä¼šäº§ç”Ÿä¸€ä¸ªæ–°çš„spillæ–‡ä»¶ï¼Œæ‰€ä»¥åœ¨Mapä»»åŠ¡å†™å®Œä»–çš„æœ€åä¸€ä¸ªè¾“å‡ºè®°å½•çš„æ—¶å€™ï¼Œå¯èƒ½ä¼šæœ‰å¤šä¸ªspillæ–‡ä»¶ï¼Œåœ¨Mapä»»åŠ¡å®Œæˆå‰ï¼Œæ‰€æœ‰çš„spillæ–‡ä»¶å°†ä¼šè¢«å½’å¹¶æ’åºä¸ºä¸€ä¸ªç´¢å¼•æ–‡ä»¶å’Œæ•°æ®æ–‡ä»¶ã€‚è¿™æ˜¯ä¸€ä¸ªå¤šè·¯å½’å¹¶è¿‡ç¨‹ï¼Œæœ€å¤§å½’å¹¶è·¯æ•°ç”±io.sort.factor æ§åˆ¶(é»˜è®¤æ˜¯10)ã€‚æ¯”å¦‚ï¼šâ€œaâ€ä»æŸä¸ªmap taskè¯»å–è¿‡æ¥æ—¶å€¼æ˜¯7ï¼Œä»å¦å¤–ä¸€ä¸ªmap è¯»å–æ—¶å€¼æ˜¯6ï¼Œå› ä¸ºå®ƒä»¬æœ‰ç›¸åŒçš„keyï¼Œæ‰€ä»¥å¾—mergeæˆgroupã€‚ä»€ä¹ˆæ˜¯groupã€‚å¯¹äºâ€œaâ€å°±æ˜¯åƒè¿™æ ·çš„ï¼š{â€œaâ€, [7, 6, 2, â€¦]}ï¼Œæ•°ç»„ä¸­çš„å€¼å°±æ˜¯ä»ä¸åŒæº¢å†™æ–‡ä»¶ä¸­è¯»å–å‡ºæ¥çš„ï¼Œç„¶åå†æŠŠè¿™äº›å€¼åŠ èµ·æ¥ã€‚è¯·æ³¨æ„ï¼Œå› ä¸ºmergeæ˜¯å°†å¤šä¸ªæº¢å†™æ–‡ä»¶åˆå¹¶åˆ°ä¸€ä¸ªæ–‡ä»¶ï¼Œæ‰€ä»¥å¯èƒ½ä¹Ÿæœ‰ç›¸åŒçš„keyå­˜åœ¨ã€‚å¦‚æœè®¾å®šäº†Combinerï¼Œä¼šä½¿ç”¨combineræ¥åˆå¹¶ç›¸åŒkeyï¼Œè¿™æ˜¯mapç«¯çš„ç»“æœã€‚ mapç«¯ä¸reduceç«¯çš„äº¤äº’Reduceæ˜¯æ€ä¹ˆçŸ¥é“ä»å“ªäº›TaskTrackersä¸­è·å–Mapçš„è¾“å‡ºå‘¢ï¼Ÿå½“Mapä»»åŠ¡å®Œæˆä¹‹åï¼Œä¼šé€šçŸ¥ä»–ä»¬çš„çˆ¶TaskTrackerï¼Œå‘ŠçŸ¥çŠ¶æ€æ›´æ–°ï¼Œç„¶åTaskTrackerå†è½¬å‘ŠJobTrackerï¼Œè¿™äº›é€šçŸ¥ä¿¡æ¯æ˜¯é€šè¿‡å¿ƒè·³é€šä¿¡æœºåˆ¶ä¼ è¾“çš„ï¼Œå› æ­¤é’ˆå¯¹ä»¥ä¸€ä¸ªç‰¹å®šçš„ä½œä¸šï¼ŒjobtrackerçŸ¥é“Mapè¾“å‡ºä¸tasktrackersçš„æ˜ å°„å…³ç³»ã€‚Reducerä¸­æœ‰ä¸€ä¸ªçº¿ç¨‹ä¼šé—´æ­‡çš„å‘JobTrackerè¯¢é—®Mapè¾“å‡ºçš„åœ°å€ï¼Œç›´åˆ°æŠŠæ‰€æœ‰çš„æ•°æ®éƒ½å–åˆ°ã€‚åœ¨Reducerå–èµ°äº†Mapè¾“å‡ºä¹‹åï¼ŒTaskTrackerä¸ä¼šç«‹å³åˆ é™¤è¿™äº›æ•°æ®ï¼Œå› ä¸ºReducerå¯èƒ½ä¼šå¤±è´¥ï¼Œä»–ä»¬ä¼šåœ¨æ•´ä¸ªä½œä¸šå®Œæˆä¹‹åï¼ŒJobTrackerå‘ŠçŸ¥ä»–ä»¬è¦åˆ é™¤çš„æ—¶å€™æ‰å»åˆ é™¤ mapç«¯çš„æ‰€æœ‰å·¥ä½œç»“æŸåï¼Œæœ€ç»ˆç”Ÿæˆçš„è¿™ä¸ªæ–‡ä»¶ä¹Ÿå­˜æ”¾åœ¨TaskTrackerå¤Ÿå¾—ç€çš„æŸä¸ªæœ¬åœ°ç›®å½•å†…ã€‚æ¯ä¸ªreduce taskä¸æ–­åœ°é€šè¿‡RPCä»JobTrackeré‚£é‡Œè·å–map taskæ˜¯å¦å®Œæˆçš„ä¿¡æ¯ï¼Œå¦‚æœreduce taskå¾—åˆ°é€šçŸ¥ï¼Œè·çŸ¥æŸå°TaskTrackerä¸Šçš„map taskæ‰§è¡Œå®Œæˆï¼ŒShuffleçš„ååŠæ®µè¿‡ç¨‹å¼€å§‹å¯åŠ¨ã€‚ç®€å•åœ°è¯´ï¼Œreduce taskåœ¨æ‰§è¡Œä¹‹å‰çš„å·¥ä½œå°±æ˜¯ä¸æ–­åœ°æ‹‰å–å½“å‰jobé‡Œæ¯ä¸ªmap taskçš„æœ€ç»ˆç»“æœï¼Œç„¶åå¯¹ä»ä¸åŒåœ°æ–¹æ‹‰å–è¿‡æ¥çš„æ•°æ®ä¸æ–­åœ°åšmergeï¼Œä¹Ÿæœ€ç»ˆå½¢æˆä¸€ä¸ªæ–‡ä»¶ä½œä¸ºreduce taskçš„è¾“å…¥æ–‡ä»¶ã€‚ reduceç«¯è¿‡ç¨‹ Copyè¿‡ç¨‹ï¼Œç®€å•åœ°æ‹‰å–æ•°æ®ã€‚Reduceè¿›ç¨‹å¯åŠ¨ä¸€äº›æ•°æ®copyçº¿ç¨‹(Fetcher)ï¼Œé€šè¿‡HTTPæ–¹å¼è¯·æ±‚map taskæ‰€åœ¨çš„TaskTrackerè·å–map taskçš„è¾“å‡ºæ–‡ä»¶ã€‚å› ä¸ºmap taskæ—©å·²ç»“æŸï¼Œè¿™äº›æ–‡ä»¶å°±å½’TaskTrackerç®¡ç†åœ¨æœ¬åœ°ç£ç›˜ä¸­ã€‚ Mergeé˜¶æ®µã€‚è¿™é‡Œçš„mergeå¦‚mapç«¯çš„mergeåŠ¨ä½œï¼Œåªæ˜¯æ•°ç»„ä¸­å­˜æ”¾çš„æ˜¯ä¸åŒmapç«¯copyæ¥çš„æ•°å€¼ã€‚Copyè¿‡æ¥çš„æ•°æ®ä¼šå…ˆæ”¾å…¥å†…å­˜ç¼“å†²åŒºä¸­ï¼Œè¿™é‡Œçš„ç¼“å†²åŒºå¤§å°è¦æ¯”mapç«¯çš„æ›´ä¸ºçµæ´»ï¼Œå®ƒåŸºäºJVMçš„heap sizeè®¾ç½®ï¼Œå› ä¸ºShuffleé˜¶æ®µReducerä¸è¿è¡Œï¼Œæ‰€ä»¥åº”è¯¥æŠŠç»å¤§éƒ¨åˆ†çš„å†…å­˜éƒ½ç»™Shuffleç”¨ã€‚è¿™é‡Œéœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œmergeæœ‰ä¸‰ç§å½¢å¼ï¼š1)å†…å­˜åˆ°å†…å­˜ ï¼ˆé»˜è®¤ä¸å¯ç”¨ï¼‰ 2)å†…å­˜åˆ°ç£ç›˜ 3)ç£ç›˜åˆ°ç£ç›˜ã€‚å½“å†…å­˜ä¸­çš„æ•°æ®é‡åˆ°è¾¾ä¸€å®šé˜ˆå€¼ï¼Œå°±å¯åŠ¨å†…å­˜åˆ°ç£ç›˜çš„mergeï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­å¦‚æœä½ è®¾ç½®æœ‰Combinerï¼Œä¹Ÿæ˜¯ä¼šå¯ç”¨çš„ï¼Œç„¶ååœ¨ç£ç›˜ä¸­ç”Ÿæˆäº†ä¼—å¤šçš„æº¢å†™æ–‡ä»¶ã€‚ç¬¬äºŒç§mergeæ–¹å¼ä¸€ç›´åœ¨è¿è¡Œï¼Œç›´åˆ°æ²¡æœ‰mapç«¯çš„æ•°æ®æ—¶æ‰ç»“æŸï¼Œç„¶åå¯åŠ¨ç¬¬ä¸‰ç§ç£ç›˜åˆ°ç£ç›˜çš„mergeæ–¹å¼ç”Ÿæˆæœ€ç»ˆçš„é‚£ä¸ªæ–‡ä»¶ã€‚ Reducerçš„è¾“å…¥æ–‡ä»¶ã€‚ä¸æ–­åœ°mergeåï¼Œæœ€åä¼šç”Ÿæˆä¸€ä¸ªæ–‡ä»¶ã€‚è¿™ä¸ªæ–‡ä»¶å¯èƒ½å­˜åœ¨äºç£ç›˜ä¸Šï¼Œä¹Ÿå¯èƒ½å­˜åœ¨äºå†…å­˜ä¸­ã€‚å°±è¯»å–é€Ÿåº¦æ¥è¯´å¯¹äºå†…å­˜ä¸­ï¼Œç›´æ¥ä½œä¸ºReducerçš„è¾“å…¥ï¼Œä½†é»˜è®¤æƒ…å†µä¸‹ï¼Œè¿™ä¸ªæ–‡ä»¶æ˜¯å­˜æ”¾äºç£ç›˜ä¸­çš„ã€‚å½“Reducerçš„è¾“å…¥æ–‡ä»¶å·²å®šï¼Œæ•´ä¸ªShuffleæ‰æœ€ç»ˆç»“æŸã€‚ç„¶åå°±æ˜¯Reduceræ‰§è¡Œï¼Œä¸€èˆ¬æ˜¯æŠŠç»“æœæ”¾åˆ°HDFSä¸Šã€‚ Speculative Executionæ˜¯æŒ‡å½“ä¸€ä¸ªjobçš„æ‰€æœ‰taskéƒ½åœ¨runningçš„æ—¶å€™ï¼Œå½“æŸä¸ªtaskçš„è¿›åº¦æ¯”å¹³å‡è¿›åº¦æ…¢æ—¶æ‰ä¼šå¯åŠ¨ä¸€ä¸ªå’Œå½“å‰Taskä¸€æ¨¡ä¸€æ ·çš„ä»»åŠ¡ï¼Œå½“å…¶ä¸­ä¸€ä¸ªtaskå®Œæˆä¹‹åå¦å¤–ä¸€ä¸ªä¼šè¢«ä¸­æ­¢ï¼Œæ‰€ä»¥Speculative Taskä¸æ˜¯é‡å¤Taskè€Œæ˜¯å¯¹Taskæ‰§è¡Œæ—¶å€™çš„ä¸€ç§ä¼˜åŒ–ç­–ç•¥ ä»»åŠ¡åˆ†ç‰‡ä¸hdfsæ–‡ä»¶å¤§å°åŠæ–‡ä»¶å—ä¹‹é—´çš„å…³ç³»åœ¨ä¸Šé¢mapReduceä¸­å¯ä»¥çœ‹åˆ°ï¼Œä»»åŠ¡çš„æ‰§è¡Œæ˜¯åŸºäºhdfsæ–‡ä»¶çš„ï¼Œä»»åŠ¡åˆ†ç‰‡å’Œæ–‡ä»¶å¤§å°ï¼Œæ–‡ä»¶å—å¤§å°éƒ½æœ‰ä¸€å®šçš„è”ç³»ã€‚ ä»»åŠ¡åˆ‡ç‰‡ï¼šå°†ä»»åŠ¡åˆ’åˆ†æˆåˆ‡ç‰‡ï¼Œä¸€ä¸ªåˆ‡ç‰‡äº¤ç»™ä¸€ä¸ªtaskå®ä¾‹å¤„ç†ï¼Œåªæ˜¯ä¸€ä¸ªé€»è¾‘çš„åç§»é‡åˆ’åˆ†è€Œå·² 1.åœ¨map taskæ‰§è¡Œæ—¶ï¼Œå®ƒçš„è¾“å…¥æ•°æ®æ¥æºäºHDFSçš„blockï¼Œå½“ç„¶åœ¨MapReduceæ¦‚å¿µä¸­ï¼Œmap taskåªè¯»å–splitã€‚Splitä¸blockçš„å¯¹åº”å…³ç³»å¯èƒ½æ˜¯å¤šå¯¹ä¸€ï¼Œé»˜è®¤æ˜¯ä¸€å¯¹ä¸€ã€‚ é‡‡ç”¨çš„ç®—æ³•æ˜¯ï¼š åˆ†ç‰‡å¤§å°èŒƒå›´å¯ä»¥åœ¨ mapred-site.xml ä¸­è®¾ç½®ï¼Œmapred.min.split.size mapred.max.split.sizeï¼ŒminSplitSize å¤§å°é»˜è®¤ä¸º 1Bï¼ŒmaxSplitSize å¤§å°é»˜è®¤ä¸º Long.MAX_VALUE = 9223372036854775807 123miniSize = 1maxSize = Long.MAXVALUEsplitSize = Math.max(miniSize,Math.min(maxSize,blockSize)) é»˜è®¤æƒ…å†µä¸‹ï¼Œä»»åŠ¡åˆ†ç‰‡çš„å¤§å°ä¸ºhdfsçš„blocksize ä¹Ÿå°±æ˜¯å—å¤§å° æ‰€ä»¥åœ¨æˆ‘ä»¬æ²¡æœ‰è®¾ç½®åˆ†ç‰‡çš„èŒƒå›´çš„æ—¶å€™ï¼Œåˆ†ç‰‡å¤§å°æ˜¯ç”± block å—å¤§å°å†³å®šçš„ï¼Œå’Œå®ƒçš„å¤§å°ä¸€æ ·ã€‚æ¯”å¦‚æŠŠä¸€ä¸ª 258MB çš„æ–‡ä»¶ä¸Šä¼ åˆ° HDFS ä¸Šï¼Œå‡è®¾ block å—å¤§å°æ˜¯ 128MBï¼Œé‚£ä¹ˆå®ƒå°±ä¼šè¢«åˆ†æˆä¸‰ä¸ª block å—ï¼Œä¸ä¹‹å¯¹åº”äº§ç”Ÿä¸‰ä¸ª splitï¼Œæ‰€ä»¥æœ€ç»ˆä¼šäº§ç”Ÿä¸‰ä¸ª map taskã€‚æˆ‘åˆå‘ç°äº†å¦ä¸€ä¸ªé—®é¢˜ï¼Œç¬¬ä¸‰ä¸ª block å—é‡Œå­˜çš„æ–‡ä»¶å¤§å°åªæœ‰ 2MBï¼Œè€Œå®ƒçš„ block å—å¤§å°æ˜¯ 128MBï¼Œé‚£å®ƒå®é™…å ç”¨ Linux file system çš„å¤šå¤§ç©ºé—´ï¼Ÿ** ç­”æ¡ˆæ˜¯å®é™…çš„æ–‡ä»¶å¤§å°ï¼Œè€Œéä¸€ä¸ªå—çš„å¤§å° åˆ‡ç‰‡çš„æµç¨‹ éå†è¾“å…¥ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼Œå¾—åˆ°æ–‡ä»¶é›†åˆ list éå†æ–‡ä»¶é›†åˆlistï¼Œå¾ªç¯æ“ä½œé›†åˆä¸‹çš„æ–‡ä»¶ è·å–æ–‡ä»¶çš„blocksize æ–‡ä»¶å—ï¼Œè·å–æ–‡ä»¶çš„é•¿åº¦ï¼Œå¾—åˆ°åˆ‡ç‰‡ä¿¡æ¯ï¼ˆsplit[æ–‡ä»¶è·¯å¾„,åˆ‡ç‰‡ç¼–å·,åç§»é‡èŒƒå›´]ï¼‰,å°†å„åˆ‡ç‰‡å¯¹è±¡æ”¾å…¥åˆ°ä¸€ä¸ªsplitListé‡Œé¢ éå†å®Œæˆåï¼Œå°†åˆ‡ç‰‡ä¿¡æ¯splitListåºåˆ—åŒ–åˆ°ä¸€ä¸ªsplitæè¿°æ–‡ä»¶ä¸­ é»˜è®¤æƒ…å†µä¸‹ï¼Œé»˜è®¤çš„TextInputFormatå¯¹ä»»åŠ¡åˆ‡ç‰‡æ˜¯æŒ‰æ–‡ä»¶è§„åˆ’åˆ‡ç‰‡ï¼Œä¸ç®¡æ–‡ä»¶å¤šå°ï¼Œéƒ½ä¼šæ˜¯ä¸€ä¸ªå•ç‹¬çš„åˆ‡ç‰‡ï¼Œè¿™æ ·å¦‚æœæœ‰å¤§é‡å°æ–‡ä»¶ï¼Œå°±ä¼šäº§ç”Ÿå¤§é‡çš„maptaskï¼Œå¤„ç†æ•ˆç‡æå…¶ä½ä¸‹ å¯¹äºä»¥ä¸ŠåŸç†ï¼Œå¦‚æœè¯»å–çš„æ˜¯å¾ˆå¤šå°æ–‡ä»¶ï¼Œä¼šäº§ç”Ÿå¤§é‡çš„å°åˆ‡ç‰‡ï¼Œé€ æˆå¤§é‡çš„maptaskè¿è¡Œï¼Œå¯¹åº”çš„è§£å†³æ–¹æ³•ï¼š å°†å°æ–‡ä»¶åˆå¹¶ä¹‹åå†ä¸Šä¼ åˆ°hdfs å¦‚æœå°æ–‡ä»¶å·²ç»ä¸Šä¼ äº†ï¼Œå¯ä»¥å†™MapReduceç¨‹åºå°†å°æ–‡ä»¶åˆå¹¶ å¯ä»¥ç”¨å¦ä¸€ç§InputFormatï¼šCombineInputFormat(å®ƒå¯ä»¥å°†å¤šä¸ªæ–‡ä»¶åˆ’åˆ†åˆ°ä¸€ä¸ªåˆ‡ç‰‡ä¸­)","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gangtieguo.cn/tags/Hadoop/"},{"name":"åŸç†","slug":"åŸç†","permalink":"http://gangtieguo.cn/tags/åŸç†/"}]},{"title":"SparkSQLä»‹ç»","slug":"SparkSQLä»‹ç»","date":"2018-08-15T18:04:32.317Z","updated":"2018-08-20T01:38:13.848Z","comments":true,"path":"2018/08/16/SparkSQLä»‹ç»/","link":"","permalink":"http://gangtieguo.cn/2018/08/16/SparkSQLä»‹ç»/","excerpt":"[TOC] Hiveï¼Œå®ƒæ˜¯å°†Hive SQLè½¬æ¢æˆMapReduceç„¶åæäº¤åˆ°é›†ç¾¤ä¸Šæ‰§è¡Œï¼Œå¤§å¤§ç®€åŒ–äº†ç¼–å†™MapReduceçš„ç¨‹åºçš„å¤æ‚æ€§ï¼Œç”±äºMapReduceè¿™ç§è®¡ç®—æ¨¡å‹æ‰§è¡Œæ•ˆç‡æ¯”è¾ƒæ…¢ã€‚æ‰€æœ‰Spark SQLçš„åº”è¿è€Œç”Ÿï¼Œå®ƒæ˜¯å°†Spark SQLè½¬æ¢æˆRDDï¼Œç„¶åæäº¤åˆ°é›†ç¾¤æ‰§è¡Œï¼Œæ‰§è¡Œæ•ˆç‡éå¸¸å¿«ï¼","text":"[TOC] Hiveï¼Œå®ƒæ˜¯å°†Hive SQLè½¬æ¢æˆMapReduceç„¶åæäº¤åˆ°é›†ç¾¤ä¸Šæ‰§è¡Œï¼Œå¤§å¤§ç®€åŒ–äº†ç¼–å†™MapReduceçš„ç¨‹åºçš„å¤æ‚æ€§ï¼Œç”±äºMapReduceè¿™ç§è®¡ç®—æ¨¡å‹æ‰§è¡Œæ•ˆç‡æ¯”è¾ƒæ…¢ã€‚æ‰€æœ‰Spark SQLçš„åº”è¿è€Œç”Ÿï¼Œå®ƒæ˜¯å°†Spark SQLè½¬æ¢æˆRDDï¼Œç„¶åæäº¤åˆ°é›†ç¾¤æ‰§è¡Œï¼Œæ‰§è¡Œæ•ˆç‡éå¸¸å¿«ï¼ éœ€è¦å°†hive-site.xmlæ‹·åˆ°sparkçš„é…ç½®æ–‡ä»¶å¤¹ hiveåªè®¤latin1ç¼–ç linuxä¸‹çš„mysqlç¼–ç æ˜¯latin1windowsä¸‹çš„ä¹Ÿè®¾ç½®æˆlatin1.å¦‚æœè¦å’Œhiveæ­é…ä½¿ç”¨çš„è¯ è¿›å…¥sparksqlå’Œhiveè¿æ¥çš„å‘½ä»¤1/home/bigdata/apps/spark/bin/spark-sql --master spark://bigdata1:7077 --driver-class-path /home/bigdata/apps/hive/lib/mysql-connector-java-5.1.31-bin.jar éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œéœ€è¦æ˜¯é›†ç¾¤æ¨¡å¼ï¼Œâ€“master ç­‰ç­‰ï¼Œè¿˜è¦æŒ‡å®šä¸€ä¸ªjdbcçš„è¿æ¥é©±åŠ¨sparksqlä¹Ÿä¼šèµ°hiveçš„å…ƒæ•°æ®åº“ hiveè¯­æ³•åœ¨spark-sqlä¸‹ 1'&gt;create table person(id bigint,name string,age int) row format delimited fields terminated by ','; åœ¨hiveä¸‹ï¼š 1load data inpath \"hdfs://master:9000/person.txt\" into table person;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"},{"name":"SparkSQL","slug":"SparkSQL","permalink":"http://gangtieguo.cn/tags/SparkSQL/"}]},{"title":"Spark-On-Yarnæ¨¡å¼","slug":"Spark-On-yarn","date":"2018-08-15T17:58:01.513Z","updated":"2019-06-17T04:40:09.396Z","comments":true,"path":"2018/08/16/Spark-On-yarn/","link":"","permalink":"http://gangtieguo.cn/2018/08/16/Spark-On-yarn/","excerpt":"[TOC] SparkOnYarnä¸¤ç§æ¨¡å¼åŒºåˆ«clusteræ¨¡å¼ï¼šDriverç¨‹åºåœ¨YARNä¸­è¿è¡Œï¼Œåº”ç”¨çš„è¿è¡Œç»“æœä¸èƒ½åœ¨å®¢æˆ·ç«¯æ˜¾ç¤ºï¼Œæ‰€ä»¥æœ€å¥½è¿è¡Œé‚£äº›å°†ç»“æœæœ€ç»ˆä¿å­˜åœ¨å¤–éƒ¨å­˜å‚¨ä»‹è´¨ï¼ˆå¦‚HDFSã€Redisã€Mysqlï¼‰è€Œéstdoutè¾“å‡ºçš„åº”ç”¨ç¨‹åºï¼Œå®¢æˆ·ç«¯çš„ç»ˆç«¯æ˜¾ç¤ºçš„ä»…æ˜¯ä½œä¸ºYARNçš„jobçš„ç®€å•è¿è¡ŒçŠ¶å†µã€‚ 12345678910111213141516171819./bin/spark-submit --class org.apache.spark.examples.SparkPi \\--master yarn \\--deploy-mode cluster \\--driver-memory 1g \\--executor-memory 1g \\--executor-cores 2 \\--queue default \\lib/spark-examples*.jar \\10./bin/spark-submit --class cn.itcast.spark.day1.WordCount \\--master yarn \\--deploy-mode cluster \\--driver-memory 1g \\--executor-memory 1g \\--executor-cores 2 \\--queue default \\/home/bigdata/hello-spark-1.0.jar \\hdfs://master:9000/wc hdfs://master:9000/out-yarn-1","text":"[TOC] SparkOnYarnä¸¤ç§æ¨¡å¼åŒºåˆ«clusteræ¨¡å¼ï¼šDriverç¨‹åºåœ¨YARNä¸­è¿è¡Œï¼Œåº”ç”¨çš„è¿è¡Œç»“æœä¸èƒ½åœ¨å®¢æˆ·ç«¯æ˜¾ç¤ºï¼Œæ‰€ä»¥æœ€å¥½è¿è¡Œé‚£äº›å°†ç»“æœæœ€ç»ˆä¿å­˜åœ¨å¤–éƒ¨å­˜å‚¨ä»‹è´¨ï¼ˆå¦‚HDFSã€Redisã€Mysqlï¼‰è€Œéstdoutè¾“å‡ºçš„åº”ç”¨ç¨‹åºï¼Œå®¢æˆ·ç«¯çš„ç»ˆç«¯æ˜¾ç¤ºçš„ä»…æ˜¯ä½œä¸ºYARNçš„jobçš„ç®€å•è¿è¡ŒçŠ¶å†µã€‚ 12345678910111213141516171819./bin/spark-submit --class org.apache.spark.examples.SparkPi \\--master yarn \\--deploy-mode cluster \\--driver-memory 1g \\--executor-memory 1g \\--executor-cores 2 \\--queue default \\lib/spark-examples*.jar \\10./bin/spark-submit --class cn.itcast.spark.day1.WordCount \\--master yarn \\--deploy-mode cluster \\--driver-memory 1g \\--executor-memory 1g \\--executor-cores 2 \\--queue default \\/home/bigdata/hello-spark-1.0.jar \\hdfs://master:9000/wc hdfs://master:9000/out-yarn-1 clientæ¨¡å¼ï¼šDriverè¿è¡Œåœ¨Clientä¸Šï¼Œåº”ç”¨ç¨‹åºè¿è¡Œç»“æœä¼šåœ¨å®¢æˆ·ç«¯æ˜¾ç¤ºï¼Œæ‰€æœ‰é€‚åˆè¿è¡Œç»“æœæœ‰è¾“å‡ºçš„åº”ç”¨ç¨‹åºï¼ˆå¦‚spark-shellï¼‰ 12345678910111213clientæ¨¡å¼./bin/spark-submit --class org.apache.spark.examples.SparkPi \\--master yarn \\--deploy-mode client \\--driver-memory 1g \\--executor-memory 1g \\--executor-cores 2 \\--queue default \\lib/spark-examples*.jar \\10spark-shellå¿…é¡»ä½¿ç”¨clientæ¨¡å¼./bin/spark-shell --master yarn --deploy-mode client åŸç†clusteræ¨¡å¼ï¼š Spark Driveré¦–å…ˆä½œä¸ºä¸€ä¸ªApplicationMasteråœ¨YARNé›†ç¾¤ä¸­å¯åŠ¨ï¼Œå®¢æˆ·ç«¯æäº¤ç»™ResourceManagerçš„æ¯ä¸€ä¸ªjobéƒ½ä¼šåœ¨é›†ç¾¤çš„NodeManagerèŠ‚ç‚¹ä¸Šåˆ†é…ä¸€ä¸ªå”¯ä¸€çš„ApplicationMasterï¼Œç”±è¯¥ApplicationMasterç®¡ç†å…¨ç”Ÿå‘½å‘¨æœŸçš„åº”ç”¨ã€‚å…·ä½“è¿‡ç¨‹ï¼š ç”±clientå‘ResourceManageræäº¤è¯·æ±‚ï¼Œå¹¶ä¸Šä¼ jaråˆ°HDFSä¸Šè¿™æœŸé—´åŒ…æ‹¬å››ä¸ªæ­¥éª¤ï¼ša).è¿æ¥åˆ°RMb).ä»RMçš„ASMï¼ˆApplicationsManager ï¼‰ä¸­è·å¾—metricã€queueå’Œresourceç­‰ä¿¡æ¯ã€‚c). upload app jar and spark-assembly jard).è®¾ç½®è¿è¡Œç¯å¢ƒå’Œcontainerä¸Šä¸‹æ–‡ï¼ˆlaunch-container.shç­‰è„šæœ¬) ResouceManagerå‘NodeManagerç”³è¯·èµ„æºï¼Œåˆ›å»ºSpark ApplicationMasterï¼ˆæ¯ä¸ªSparkContextéƒ½æœ‰ä¸€ä¸ªApplicationMasterï¼‰ NodeManagerå¯åŠ¨ApplicationMasterï¼Œå¹¶å‘ResourceManager AsMæ³¨å†Œ ApplicationMasterä»HDFSä¸­æ‰¾åˆ°jaræ–‡ä»¶ï¼Œå¯åŠ¨SparkContextã€DAGschedulerå’ŒYARN Cluster Scheduler ResourceManagerå‘ResourceManager AsMæ³¨å†Œç”³è¯·containerèµ„æº ResourceManageré€šçŸ¥NodeManageråˆ†é…Containerï¼Œè¿™æ—¶å¯ä»¥æ”¶åˆ°æ¥è‡ªASMå…³äºcontainerçš„æŠ¥å‘Šã€‚ï¼ˆæ¯ä¸ªcontainerå¯¹åº”ä¸€ä¸ªexecutorï¼‰ Spark ApplicationMasterç›´æ¥å’Œcontainerï¼ˆexecutorï¼‰è¿›è¡Œäº¤äº’ï¼Œå®Œæˆè¿™ä¸ªåˆ†å¸ƒå¼ä»»åŠ¡ã€‚ clientæ¨¡å¼ åœ¨clientæ¨¡å¼ä¸‹ï¼ŒDriverè¿è¡Œåœ¨Clientä¸Šï¼Œé€šè¿‡ApplicationMasterå‘RMè·å–èµ„æºã€‚æœ¬åœ°Driverè´Ÿè´£ä¸æ‰€æœ‰çš„executor containerè¿›è¡Œäº¤äº’ï¼Œå¹¶å°†æœ€åçš„ç»“æœæ±‡æ€»ã€‚ç»“æŸæ‰ç»ˆç«¯ï¼Œç›¸å½“äºkillæ‰è¿™ä¸ªsparkåº”ç”¨ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœè¿è¡Œçš„ç»“æœä»…ä»…è¿”å›åˆ°terminalä¸Šæ—¶éœ€è¦é…ç½®è¿™ä¸ªã€‚ å®¢æˆ·ç«¯çš„Driverå°†åº”ç”¨æäº¤ç»™Yarnåï¼ŒYarnä¼šå…ˆåå¯åŠ¨ApplicationMasterå’Œexecutorï¼Œå¦å¤–ApplicationMasterå’Œexecutoréƒ½ æ˜¯è£…è½½åœ¨containeré‡Œè¿è¡Œï¼Œcontaineré»˜è®¤çš„å†…å­˜æ˜¯1Gï¼ŒApplicationMasteråˆ†é…çš„å†…å­˜æ˜¯driver- memoryï¼Œexecutoråˆ†é…çš„å†…å­˜æ˜¯executor-memoryã€‚åŒæ—¶ï¼Œå› ä¸ºDriveråœ¨å®¢æˆ·ç«¯ï¼Œæ‰€ä»¥ç¨‹åºçš„è¿è¡Œç»“æœå¯ä»¥åœ¨å®¢æˆ·ç«¯æ˜¾ ç¤ºï¼ŒDriverä»¥è¿›ç¨‹åä¸ºSparkSubmitçš„å½¢å¼å­˜åœ¨ã€‚","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"},{"name":"ä½¿ç”¨","slug":"ä½¿ç”¨","permalink":"http://gangtieguo.cn/tags/ä½¿ç”¨/"}]},{"title":"SparkStreamingä»‹ç»","slug":"SparkStreamingä»‹ç»","date":"2018-08-15T17:50:47.842Z","updated":"2019-06-17T04:40:09.399Z","comments":true,"path":"2018/08/16/SparkStreamingä»‹ç»/","link":"","permalink":"http://gangtieguo.cn/2018/08/16/SparkStreamingä»‹ç»/","excerpt":"[TOC] å¤§æ•°æ®é¢†åŸŸï¼Œåˆ†ä¸ºç¦»çº¿è®¡ç®—å’Œå®æ—¶è®¡ç®—","text":"[TOC] å¤§æ•°æ®é¢†åŸŸï¼Œåˆ†ä¸ºç¦»çº¿è®¡ç®—å’Œå®æ—¶è®¡ç®— Streamingå’ŒStormæ¯”è¾ƒåœ¨æ—¶æ•ˆæ€§ä¸Šæ¯”stormå¼±ï¼Œåœ¨ååé‡ä¸Šæ¯”stormå¤§streamingéœ€è¦è®¾ç½®æ—¶é—´é—´éš”ï¼Œè®¾ç½®å¤šé•¿æ—¶é—´äº§ç”Ÿä¸€ä¸ªæ‰¹æ¬¡è®°å½•åˆ°streamingæ”¾åœ¨RDDé‡Œé¢æ¯”å¦‚è®¾ç½®5sï¼Œæ¯éš”5så°±ä¼šäº§ç”Ÿä¸€ä¸ªRDDRDDéœ€è¦æ˜¯æœ‰åºçš„","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"åŸç†","slug":"åŸç†","permalink":"http://gangtieguo.cn/tags/åŸç†/"},{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"},{"name":"SparkStreaming","slug":"SparkStreaming","permalink":"http://gangtieguo.cn/tags/SparkStreaming/"}]},{"title":"SparkRDDä»‹ç»","slug":"SparkRDDä»‹ç»","date":"2018-08-15T16:49:49.734Z","updated":"2019-06-17T04:40:09.398Z","comments":true,"path":"2018/08/16/SparkRDDä»‹ç»/","link":"","permalink":"http://gangtieguo.cn/2018/08/16/SparkRDDä»‹ç»/","excerpt":"[TOC] 1sc.textfile(\"hdfs://master:9000/wc\").flatMap(_.split(\"åˆ†éš”ç¬¦\")).map((_,1)).reduceByKey(_+_).saveAsTextFile(\"hdfs://master:9000/wcResult\")","text":"[TOC] 1sc.textfile(\"hdfs://master:9000/wc\").flatMap(_.split(\"åˆ†éš”ç¬¦\")).map((_,1)).reduceByKey(_+_).saveAsTextFile(\"hdfs://master:9000/wcResult\") å½“rddå½¢æˆè¿‡ç¨‹ä¸­ï¼Œworkerçš„åˆ†åŒºä¸­åªæ˜¯é¢„ç•™äº†å­˜æ”¾æ•°æ®çš„ä½ç½®ï¼Œåªæœ‰å½“actionè§¦å‘çš„æ—¶å€™ï¼Œworkerçš„åˆ†åŒºä¸­æ‰ä¼šå­˜åœ¨æ•°æ®ï¼ŒsparkSubmit submitçš„å‘½ä»¤è¡Œé»˜è®¤çš„æ˜¯driver ï¼ŒRDDçš„åˆ›å»ºéƒ½æ˜¯åœ¨åœ¨driverä¸Šåˆ›å»ºçš„ sparkçš„åˆ†åŒºä¸hdfsæ•°æ®å—çš„å…³ç³»Partitionerå‡½æ•°ä¸ä½†å†³å®šäº†RDDæœ¬èº«çš„åˆ†ç‰‡æ•°é‡ï¼Œä¹Ÿå†³å®šäº†parent RDD Shuffleè¾“å‡ºæ—¶çš„åˆ†ç‰‡æ•°é‡ã€‚ SparkRDDRDDï¼ˆResilientDistributed Datasetï¼‰å«åšåˆ†å¸ƒå¼æ•°æ®é›†ï¼Œæ˜¯Sparkä¸­æœ€åŸºæœ¬çš„æ•°æ®æŠ½è±¡ï¼Œå®ƒä»£è¡¨ä¸€ä¸ªä¸å¯å˜ã€å¯åˆ†åŒºã€é‡Œé¢çš„å…ƒç´ å¯å¹¶è¡Œè®¡ç®—çš„é›†åˆã€‚RDDå…·æœ‰æ•°æ®æµæ¨¡å‹çš„ç‰¹ç‚¹ï¼šè‡ªåŠ¨å®¹é”™ã€ä½ç½®æ„ŸçŸ¥æ€§è°ƒåº¦å’Œå¯ä¼¸ç¼©æ€§ã€‚RDDå…è®¸ç”¨æˆ·åœ¨æ‰§è¡Œå¤šä¸ªæŸ¥è¯¢æ—¶æ˜¾å¼åœ°å°†å·¥ä½œé›†ç¼“å­˜åœ¨å†…å­˜ä¸­ï¼Œåç»­çš„æŸ¥è¯¢èƒ½å¤Ÿé‡ç”¨å·¥ä½œé›†ï¼Œè¿™æå¤§åœ°æå‡äº†æŸ¥è¯¢é€Ÿåº¦ã€‚ 1ï¼‰ä¸€ç»„åˆ†ç‰‡ï¼ˆPartitionï¼‰ï¼Œå³æ•°æ®é›†çš„åŸºæœ¬ç»„æˆå•ä½ã€‚å¯¹äºRDDæ¥è¯´ï¼Œæ¯ä¸ªåˆ†ç‰‡éƒ½ä¼šè¢«ä¸€ä¸ªè®¡ç®—ä»»åŠ¡å¤„ç†ï¼Œå¹¶å†³å®šå¹¶è¡Œè®¡ç®—çš„ç²’åº¦ã€‚ç”¨æˆ·å¯ä»¥åœ¨åˆ›å»ºRDDæ—¶æŒ‡å®šRDDçš„åˆ†ç‰‡ä¸ªæ•°ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œé‚£ä¹ˆå°±ä¼šé‡‡ç”¨é»˜è®¤å€¼ã€‚é»˜è®¤å€¼å°±æ˜¯ç¨‹åºæ‰€åˆ†é…åˆ°çš„CPU Coreçš„æ•°ç›®ã€‚ 2ï¼‰ä¸€ä¸ªè®¡ç®—æ¯ä¸ªåˆ†åŒºçš„å‡½æ•°ã€‚Sparkä¸­RDDçš„è®¡ç®—æ˜¯ä»¥åˆ†ç‰‡ä¸ºå•ä½çš„ï¼Œæ¯ä¸ªRDDéƒ½ä¼šå®ç°computeå‡½æ•°ä»¥è¾¾åˆ°è¿™ä¸ªç›®çš„ã€‚computeå‡½æ•°ä¼šå¯¹è¿­ä»£å™¨è¿›è¡Œå¤åˆï¼Œä¸éœ€è¦ä¿å­˜æ¯æ¬¡è®¡ç®—çš„ç»“æœã€‚ 3ï¼‰RDDä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚RDDçš„æ¯æ¬¡è½¬æ¢éƒ½ä¼šç”Ÿæˆä¸€ä¸ªæ–°çš„RDDï¼Œæ‰€ä»¥RDDä¹‹é—´å°±ä¼šå½¢æˆç±»ä¼¼äºæµæ°´çº¿ä¸€æ ·çš„å‰åä¾èµ–å…³ç³»ã€‚åœ¨éƒ¨åˆ†åˆ†åŒºæ•°æ®ä¸¢å¤±æ—¶ï¼ŒSparkå¯ä»¥é€šè¿‡è¿™ä¸ªä¾èµ–å…³ç³»é‡æ–°è®¡ç®—ä¸¢å¤±çš„åˆ†åŒºæ•°æ®ï¼Œè€Œä¸æ˜¯å¯¹RDDçš„æ‰€æœ‰åˆ†åŒºè¿›è¡Œé‡æ–°è®¡ç®—ã€‚ 4ï¼‰ä¸€ä¸ªPartitionerï¼Œå³RDDçš„åˆ†ç‰‡å‡½æ•°ã€‚å½“å‰Sparkä¸­å®ç°äº†ä¸¤ç§ç±»å‹çš„åˆ†ç‰‡å‡½æ•°ï¼Œä¸€ä¸ªæ˜¯åŸºäºå“ˆå¸Œçš„HashPartitionerï¼Œå¦å¤–ä¸€ä¸ªæ˜¯åŸºäºèŒƒå›´çš„RangePartitionerã€‚åªæœ‰å¯¹äºäºkey-valueçš„RDDï¼Œæ‰ä¼šæœ‰Partitionerï¼Œékey-valueçš„RDDçš„Parititionerçš„å€¼æ˜¯Noneã€‚Partitionerå‡½æ•°ä¸ä½†å†³å®šäº†RDDæœ¬èº«çš„åˆ†ç‰‡æ•°é‡ï¼Œä¹Ÿå†³å®šäº†parent RDD Shuffleè¾“å‡ºæ—¶çš„åˆ†ç‰‡æ•°é‡ã€‚ 5ï¼‰ä¸€ä¸ªåˆ—è¡¨ï¼Œå­˜å‚¨å­˜å–æ¯ä¸ªPartitionçš„ä¼˜å…ˆä½ç½®ï¼ˆpreferredlocationï¼‰ã€‚å¯¹äºä¸€ä¸ªHDFSæ–‡ä»¶æ¥è¯´ï¼Œè¿™ä¸ªåˆ—è¡¨ä¿å­˜çš„å°±æ˜¯æ¯ä¸ª**Partitionæ‰€åœ¨çš„å—çš„ä½ç½®**ã€‚æŒ‰ç…§â€œç§»åŠ¨æ•°æ®ä¸å¦‚ç§»åŠ¨è®¡ç®—â€çš„ç†å¿µï¼ŒSparkåœ¨è¿›è¡Œä»»åŠ¡è°ƒåº¦çš„æ—¶å€™ï¼Œä¼šå°½å¯èƒ½åœ°å°†è®¡ç®—ä»»åŠ¡åˆ†é…åˆ°å…¶æ‰€è¦å¤„ç†æ•°æ®å—çš„å­˜å‚¨ä½ç½®ã€‚ è¡€ç¼˜ä¾èµ– RDD 5ä¸ªç‰¹æ€§ä¸€ä¸ªfunctionä½œç”¨ä¸€ä¸ªpartitionå¦‚æœæ˜¯key-valueæ ¼å¼çš„æœ‰ä¸€ä¸ªé»˜è®¤çš„partitioner é»˜è®¤æ˜¯hashpartitionerå¦‚æœæ˜¯ä»hdfsè¿™ç§æ–‡ä»¶ç³»ç»Ÿç±»å‹è¯»å–çš„æ•°æ®ï¼Œä¼šæœ‰ä¸€ä¸ªprefered locationï¼Œå› ä¸ºåœ¨å¤§æ•°æ®é¢†åŸŸå®æ„¿ç§»åŠ¨è®¡ç®—ï¼Œä¹Ÿä¸æ„¿ç§»åŠ¨æ•°æ®ï¼Œé€šå¸¸å«åšæ•°æ®æœ¬åœ°åŒ–ï¼Œ RDDæ•°æ®è¯»å–rddå‘hdfsä¸­è¯»å–æ•°æ®æ˜¯ä¸€è¡Œä¸€è¡Œè¯»å–æ”¾åœ¨è¿­ä»£å™¨é‡Œé¢ï¼Œè€Œä¸æ˜¯ä¸€ä¸‹å­å…¨éƒ¨è¯»å–æ•°æ® rddå‘hdfsä¸­è¯»å–æ•°æ®ï¼Œhdfsæ–‡ä»¶æœ‰å‡ ä¸ªæ•°æ®å—å°±ä¼šåˆ›å»ºå‡ ä¸ªåˆ†åŒº è¯»å–æ•°æ®è¿˜æ˜¯ç”¨çš„hadoopçš„inputFormatæ¥è¯»å–çš„ RDDçš„ç”Ÿæˆæ–¹å¼RDDç®—å­TransformationRDDä¸­çš„æ‰€æœ‰è½¬æ¢éƒ½æ˜¯å»¶è¿ŸåŠ è½½çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¬å¹¶ä¸ä¼šç›´æ¥è®¡ç®—ç»“æœã€‚ç›¸åçš„ï¼Œå®ƒä»¬åªæ˜¯è®°ä½è¿™äº›åº”ç”¨åˆ°åŸºç¡€æ•°æ®é›†ï¼ˆä¾‹å¦‚ä¸€ä¸ªæ–‡ä»¶ï¼‰ä¸Šçš„è½¬æ¢åŠ¨ä½œã€‚åªæœ‰å½“å‘ç”Ÿä¸€ä¸ªè¦æ±‚è¿”å›ç»“æœç»™Driverçš„åŠ¨ä½œæ—¶ï¼Œè¿™äº›è½¬æ¢æ‰ä¼šçœŸæ­£è¿è¡Œã€‚è¿™ç§è®¾è®¡è®©Sparkæ›´åŠ æœ‰æ•ˆç‡åœ°è¿è¡Œã€‚ å¸¸ç”¨çš„Transformationï¼š è½¬æ¢ å«ä¹‰ map(func) è¿”å›ä¸€ä¸ªæ–°çš„RDDï¼Œè¯¥RDDç”±æ¯ä¸€ä¸ªè¾“å…¥å…ƒç´ ç»è¿‡funcå‡½æ•°è½¬æ¢åç»„æˆ filter(func) è¿”å›ä¸€ä¸ªæ–°çš„RDDï¼Œè¯¥RDDç”±ç»è¿‡funcå‡½æ•°è®¡ç®—åè¿”å›å€¼ä¸ºtrueçš„è¾“å…¥å…ƒç´ ç»„æˆ flatMap(func) ç±»ä¼¼äºmapï¼Œä½†æ˜¯æ¯ä¸€ä¸ªè¾“å…¥å…ƒç´ å¯ä»¥è¢«æ˜ å°„ä¸º0æˆ–å¤šä¸ªè¾“å‡ºå…ƒç´ ï¼ˆæ‰€ä»¥funcåº”è¯¥è¿”å›ä¸€ä¸ªåºåˆ—ï¼Œè€Œä¸æ˜¯å•ä¸€å…ƒç´ ï¼‰ mapPartitions(func) ç±»ä¼¼äºmapï¼Œä½†ç‹¬ç«‹åœ°åœ¨RDDçš„æ¯ä¸€ä¸ªåˆ†ç‰‡ä¸Šè¿è¡Œï¼Œå› æ­¤åœ¨ç±»å‹ä¸ºTçš„RDDä¸Šè¿è¡Œæ—¶ï¼Œfuncçš„å‡½æ•°ç±»å‹å¿…é¡»æ˜¯Iterator[T] =&gt; Iterator[U] mapPartitionsWithIndex(func) ç±»ä¼¼äºmapPartitionsï¼Œä½†funcå¸¦æœ‰ä¸€ä¸ªæ•´æ•°å‚æ•°è¡¨ç¤ºåˆ†ç‰‡çš„ç´¢å¼•å€¼ï¼Œå› æ­¤åœ¨ç±»å‹ä¸ºTçš„RDDä¸Šè¿è¡Œæ—¶ï¼Œfuncçš„å‡½æ•°ç±»å‹å¿…é¡»æ˜¯ (Int, Interator[T]) =&gt; Iterator[U] sample(withReplacement, fraction, seed) æ ¹æ®fractionæŒ‡å®šçš„æ¯”ä¾‹å¯¹æ•°æ®è¿›è¡Œé‡‡æ ·ï¼Œå¯ä»¥é€‰æ‹©æ˜¯å¦ä½¿ç”¨éšæœºæ•°è¿›è¡Œæ›¿æ¢ï¼Œseedç”¨äºæŒ‡å®šéšæœºæ•°ç”Ÿæˆå™¨ç§å­ union(otherDataset) å¯¹æºRDDå’Œå‚æ•°RDDæ±‚å¹¶é›†åè¿”å›ä¸€ä¸ªæ–°çš„RDD intersection(otherDataset) å¯¹æºRDDå’Œå‚æ•°RDDæ±‚äº¤é›†åè¿”å›ä¸€ä¸ªæ–°çš„RDD distinct([numTasks])) å¯¹æºRDDè¿›è¡Œå»é‡åè¿”å›ä¸€ä¸ªæ–°çš„RDD groupByKey([numTasks]) åœ¨ä¸€ä¸ª(K,V)çš„RDDä¸Šè°ƒç”¨ï¼Œè¿”å›ä¸€ä¸ª(K, Iterator[V])çš„RDD reduceByKey(func, [numTasks]) åœ¨ä¸€ä¸ª(K,V)çš„RDDä¸Šè°ƒç”¨ï¼Œè¿”å›ä¸€ä¸ª(K,V)çš„RDDï¼Œä½¿ç”¨æŒ‡å®šçš„reduceå‡½æ•°ï¼Œå°†ç›¸åŒkeyçš„å€¼èšåˆåˆ°ä¸€èµ·ï¼Œä¸groupByKeyç±»ä¼¼ï¼Œreduceä»»åŠ¡çš„ä¸ªæ•°å¯ä»¥é€šè¿‡ç¬¬äºŒä¸ªå¯é€‰çš„å‚æ•°æ¥è®¾ç½® aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) sortByKey([ascending], [numTasks]) åœ¨ä¸€ä¸ª(K,V)çš„RDDä¸Šè°ƒç”¨ï¼ŒKå¿…é¡»å®ç°Orderedæ¥å£ï¼Œè¿”å›ä¸€ä¸ªæŒ‰ç…§keyè¿›è¡Œæ’åºçš„(K,V)çš„RDD sortBy(func,[ascending], [numTasks]) ä¸sortByKeyç±»ä¼¼ï¼Œä½†æ˜¯æ›´çµæ´» join(otherDataset, [numTasks]) åœ¨ç±»å‹ä¸º(K,V)å’Œ(K,W)çš„RDDä¸Šè°ƒç”¨ï¼Œè¿”å›ä¸€ä¸ªç›¸åŒkeyå¯¹åº”çš„æ‰€æœ‰å…ƒç´ å¯¹åœ¨ä¸€èµ·çš„(K,(V,W))çš„RDD cogroup(otherDataset, [numTasks]) åœ¨ç±»å‹ä¸º(K,V)å’Œ(K,W)çš„RDDä¸Šè°ƒç”¨ï¼Œè¿”å›ä¸€ä¸ª(K,(Iterable,Iterable))ç±»å‹çš„RDD cartesian(otherDataset) ç¬›å¡å°”ç§¯ pipe(command, [envVars]) coalesce(numPartitions) repartition(numPartitions) repartitionAndSortWithinPartitions(partitioner) Action åŠ¨ä½œ å«ä¹‰ reduce(func) é€šè¿‡funcå‡½æ•°èšé›†RDDä¸­çš„æ‰€æœ‰å…ƒç´ ï¼Œè¿™ä¸ªåŠŸèƒ½å¿…é¡»æ˜¯å¯äº¤æ¢ä¸”å¯å¹¶è”çš„ collect() åœ¨é©±åŠ¨ç¨‹åºä¸­ï¼Œä»¥æ•°ç»„çš„å½¢å¼è¿”å›æ•°æ®é›†çš„æ‰€æœ‰å…ƒç´  count() è¿”å›RDDçš„å…ƒç´ ä¸ªæ•° first() è¿”å›RDDçš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆç±»ä¼¼äºtake(1)ï¼‰ take(n) è¿”å›ä¸€ä¸ªç”±æ•°æ®é›†çš„å‰nä¸ªå…ƒç´ ç»„æˆçš„æ•°ç»„ takeSample(withReplacement,num, [seed]) è¿”å›ä¸€ä¸ªæ•°ç»„ï¼Œè¯¥æ•°ç»„ç”±ä»æ•°æ®é›†ä¸­éšæœºé‡‡æ ·çš„numä¸ªå…ƒç´ ç»„æˆï¼Œå¯ä»¥é€‰æ‹©æ˜¯å¦ç”¨éšæœºæ•°æ›¿æ¢ä¸è¶³çš„éƒ¨åˆ†ï¼Œseedç”¨äºæŒ‡å®šéšæœºæ•°ç”Ÿæˆå™¨ç§å­ takeOrdered(n, [ordering]) saveAsTextFile(path) å°†æ•°æ®é›†çš„å…ƒç´ ä»¥textfileçš„å½¢å¼ä¿å­˜åˆ°HDFSæ–‡ä»¶ç³»ç»Ÿæˆ–è€…å…¶ä»–æ”¯æŒçš„æ–‡ä»¶ç³»ç»Ÿï¼Œå¯¹äºæ¯ä¸ªå…ƒç´ ï¼ŒSparkå°†ä¼šè°ƒç”¨toStringæ–¹æ³•ï¼Œå°†å®ƒè£…æ¢ä¸ºæ–‡ä»¶ä¸­çš„æ–‡æœ¬ saveAsSequenceFile(path) å°†æ•°æ®é›†ä¸­çš„å…ƒç´ ä»¥Hadoop sequencefileçš„æ ¼å¼ä¿å­˜åˆ°æŒ‡å®šçš„ç›®å½•ä¸‹ï¼Œå¯ä»¥ä½¿HDFSæˆ–è€…å…¶ä»–Hadoopæ”¯æŒçš„æ–‡ä»¶ç³»ç»Ÿã€‚ saveAsObjectFile(path) countByKey() é’ˆå¯¹(K,V)ç±»å‹çš„RDDï¼Œè¿”å›ä¸€ä¸ª(K,Int)çš„mapï¼Œè¡¨ç¤ºæ¯ä¸€ä¸ªkeyå¯¹åº”çš„å…ƒç´ ä¸ªæ•°ã€‚ å®½ä¾èµ–çª„ä¾èµ–åŒºåˆ† çª„ä¾èµ– narrow dependenciesä¸‰ä¸ªå°åˆ†å—æ˜¯RDDçš„åˆ†åŒºï¼Œç»„åˆèµ·æ¥çš„å¤§æ¡†æ˜¯RDDï¼Œåé¢çš„æ˜¯å­rddçš„åˆ†åŒºï¼Œä¸€ä¸ªçˆ¶rddçš„åˆ†åŒºåªå¯¹åº”ä¸€ä¸ªå­rddçš„åˆ†åŒºï¼ˆç±»æ¯”ç‹¬ç”Ÿå­å¥³ï¼‰ ï¼Œä¸€ä¸ªå­å¯ä»¥å¯¹åº”å¤šä¸ªçˆ¶åˆ†åŒºï¼ˆå¯ä»¥ç±»æ¯”çˆ¶æ¯åˆ†åŒºï¼‰ å¦‚mapï¼Œfilterï¼Œunionç­‰ç®—å­éƒ½æ˜¯æ“ä½œçš„åŸæ¥åˆ†åŒºé‡Œé¢çš„æ•°æ®,æ“ä½œä¹‹åä¹Ÿåœ¨åŸæ¥çš„åˆ†åŒºjoinå¤§å¤šæ•°æƒ…å†µä¸‹æ˜¯å®½ä¾èµ–ï¼Œåœ¨ä¸€ç§ç‰¹æ®Šæƒ…å†µä¸‹æ˜¯çª„ä¾èµ– (joinæ˜¯é’ˆå¯¹key valueå½¢å¼çš„rddï¼Œç›¸åŒkeyçš„ä¼šjoinåœ¨ä¸€èµ·) å®½ä¾èµ– wide dependenciesçˆ¶rddä¸€ä¸ªåˆ†åŒºä¼šæµå‘å¤šä¸ªå­rddçš„åˆ†åŒºç±»æ¯”å¤šå­å¥³æƒ…å†µ groupBy ï¼ŒreduceByKey ï¼Œjoinç­‰ ä¸‹å›¾båˆ°gä¸æ˜¯ä¸€ä¸ªstageæ˜¯å› ä¸ºï¼Œæå‰å·²ç»åˆ†å¥½ç»„ï¼Œæ‰€ä»¥æ˜¯çª„ä¾èµ–ï¼Œæ²¡æœ‰stage LineageRDDåªæ”¯æŒç²—ç²’åº¦è½¬æ¢ï¼Œå³åœ¨å¤§é‡è®°å½•ä¸Šæ‰§è¡Œçš„å•ä¸ªæ“ä½œã€‚å°†åˆ›å»ºRDDçš„ä¸€ç³»åˆ—Lineageï¼ˆå³è¡€ç»Ÿï¼‰è®°å½•ä¸‹æ¥ï¼Œä»¥ä¾¿æ¢å¤ä¸¢å¤±çš„åˆ†åŒºã€‚RDDçš„Lineageä¼šè®°å½•RDDçš„å…ƒæ•°æ®ä¿¡æ¯å’Œè½¬æ¢è¡Œä¸ºï¼Œå½“è¯¥RDDçš„éƒ¨åˆ†åˆ†åŒºæ•°æ®ä¸¢å¤±æ—¶ï¼Œå®ƒå¯ä»¥æ ¹æ®è¿™äº›ä¿¡æ¯æ¥é‡æ–°è¿ç®—å’Œæ¢å¤ä¸¢å¤±çš„æ•°æ®åˆ†åŒºã€‚ RDDçš„ç¼“å­˜Sparké€Ÿåº¦éå¸¸å¿«çš„åŸå› ä¹‹ä¸€ï¼Œå°±æ˜¯åœ¨ä¸åŒæ“ä½œä¸­å¯ä»¥åœ¨å†…å­˜ä¸­æŒä¹…åŒ–æˆ–ç¼“å­˜ä¸ªæ•°æ®é›†ã€‚å½“æŒä¹…åŒ–æŸä¸ªRDDåï¼Œæ¯ä¸€ä¸ªèŠ‚ç‚¹éƒ½å°†æŠŠè®¡ç®—çš„åˆ†ç‰‡ç»“æœä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œå¹¶åœ¨å¯¹æ­¤RDDæˆ–è¡ç”Ÿå‡ºçš„RDDè¿›è¡Œçš„å…¶ä»–åŠ¨ä½œä¸­é‡ç”¨ã€‚è¿™ä½¿å¾—åç»­çš„åŠ¨ä½œå˜å¾—æ›´åŠ è¿…é€Ÿã€‚RDDç›¸å…³çš„æŒä¹…åŒ–å’Œç¼“å­˜ï¼Œæ˜¯Sparkæœ€é‡è¦çš„ç‰¹å¾ä¹‹ä¸€ã€‚å¯ä»¥è¯´ï¼Œç¼“å­˜æ˜¯Sparkæ„å»ºè¿­ä»£å¼ç®—æ³•å’Œå¿«é€Ÿäº¤äº’å¼æŸ¥è¯¢çš„å…³é”®ã€‚ ç¼“å­˜æ–¹å¼RDDé€šè¿‡persistæ–¹æ³•æˆ–cacheæ–¹æ³•å¯ä»¥å°†å‰é¢çš„è®¡ç®—ç»“æœç¼“å­˜ï¼Œä½†æ˜¯å¹¶ä¸æ˜¯è¿™ä¸¤ä¸ªæ–¹æ³•è¢«è°ƒç”¨æ—¶ç«‹å³ç¼“å­˜ï¼Œè€Œæ˜¯è§¦å‘åé¢çš„actionæ—¶ï¼Œè¯¥RDDå°†ä¼šè¢«ç¼“å­˜åœ¨è®¡ç®—èŠ‚ç‚¹çš„å†…å­˜ä¸­ï¼Œå¹¶ä¾›åé¢é‡ç”¨ã€‚ é€šè¿‡æŸ¥çœ‹æºç å‘ç°cacheæœ€ç»ˆä¹Ÿæ˜¯è°ƒç”¨äº†persistæ–¹æ³•ï¼Œé»˜è®¤çš„å­˜å‚¨çº§åˆ«éƒ½æ˜¯ä»…åœ¨å†…å­˜å­˜å‚¨ä¸€ä»½ï¼ŒSparkçš„å­˜å‚¨çº§åˆ«è¿˜æœ‰å¥½å¤šç§ï¼Œå­˜å‚¨çº§åˆ«åœ¨object StorageLevelä¸­å®šä¹‰çš„ã€‚ ç¼“å­˜æœ‰å¯èƒ½ä¸¢å¤±ï¼Œæˆ–è€…å­˜å‚¨å­˜å‚¨äºå†…å­˜çš„æ•°æ®ç”±äºå†…å­˜ä¸è¶³è€Œè¢«åˆ é™¤ï¼ŒRDDçš„ç¼“å­˜å®¹é”™æœºåˆ¶ä¿è¯äº†å³ä½¿ç¼“å­˜ä¸¢å¤±ä¹Ÿèƒ½ä¿è¯è®¡ç®—çš„æ­£ç¡®æ‰§è¡Œã€‚é€šè¿‡åŸºäºRDDçš„ä¸€ç³»åˆ—è½¬æ¢ï¼Œä¸¢å¤±çš„æ•°æ®ä¼šè¢«é‡ç®—ï¼Œç”±äºRDDçš„å„ä¸ªPartitionæ˜¯ç›¸å¯¹ç‹¬ç«‹çš„ï¼Œå› æ­¤åªéœ€è¦è®¡ç®—ä¸¢å¤±çš„éƒ¨åˆ†å³å¯ï¼Œå¹¶ä¸éœ€è¦é‡ç®—å…¨éƒ¨Partitionã€‚ RDDç¼“å­˜val rdd = sc.textFile(â€œhdfs:hadoop1:9000/yaoâ€).cache();cacheæ˜¯trancsformationä¹Ÿæ˜¯æ‡’åŠ è½½ï¼Œé‡åˆ°action å¦‚count.collectæ‰ä¼šï¼Œç¼“å­˜åˆ°å†…å­˜é‡Œé¢,è€Œä¸æ˜¯æ–‡ä»¶ç³»ç»Ÿä¸­è¯»å–cache()è°ƒç”¨çš„persist() rdd.unpersist() å°±ä¼šå°†å†…å­˜ä¸­çš„ç¼“å­˜é‡Šæ”¾æ‰rdd.unpersist(true) CheckPointçš„èƒŒæ™¯checkpointå±äºtransaction äº‘è®¡ç®—ä¸€è¾¹è¦å°†ä¸­é—´ç»“æœè¿›è¡Œäº§ç”Ÿå¤šä¸ªRDDå’Œå¤šæ¬¡è¿ç®—ï¼Œç‰¹åˆ«æ˜¯æœºå™¨å­¦ä¹ ï¼Œéœ€è¦ä¸­é—´ç»“æœè®¡ç®—å¾ˆå¤šå¾ˆå¤šæ¬¡è¿­ä»£ï¼Œæœ‰å¯èƒ½ä¸Šç™¾æ¬¡è¿™æ ·å°±éœ€è¦å°†ä¸­é—´RDDç»“æœä¿å­˜ä¸‹æ¥ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬çš„checkpointï¼Œä¸€èˆ¬ä¿å­˜åœ¨é«˜å¯ç”¨ä¸­ï¼Œæ¯”å¦‚hdfså°±æ˜¯é«˜å¯ç”¨çš„ã€‚ åªæœ‰rddæ‰èƒ½checkPointç¼“å­˜cacheåˆ°å†…å­˜ä¸­ï¼Œç›´æ¥åˆ°å†…å­˜ä¸­æ‹¿checkPointæ˜¯åˆ°hdfs CheckPointå‘½ä»¤è®¾å®šç›®å½•ï¼Œåˆ›å»ºç›®å½• ï¼Œå¿…é¡»æŒ‡å®šç¼“å­˜åˆ°å“ªä¸ªç›®å½• 1234val rdd = sc.setCheckpointDir(\"hdfs://master:9000/ckpoint\")val rdd = sc.textFile(\"hdfs://master:9000/yao\")rdd.checkpointrdd.count ä¼šè§¦å‘ä¸¤ä¸ªä»»åŠ¡ï¼Œä¸€ä¸ªä»»åŠ¡è®¡ç®—ï¼Œä¸€ä¸ªä»»åŠ¡å†™å…¥åˆ°ckæŒ‡å®šçš„hdfsç›®å½• ä¸ºå‡å°æŒä¹…åŒ–çš„æ•°æ®é‡ï¼Œæœ€å¥½å°†RDDè¿‡æ»¤å‡ºæœ‰èŠ‚ç‚¹æ„ä¹‰çš„æ•°æ®å†è¿›è¡Œckæ“ä½œï¼Œç›´æ¥ckä¼šæŠŠæ–‡ä»¶è®°å½•èµ·æ¥åˆ°hdfsä¸­ ï¼Œä½†æ˜¯countäº§ç”Ÿçš„æ•°æ®ä¸èƒ½ckï¼Œå› ä¸ºè¿”å›çš„æ˜¯Longç±»å‹çš„ï¼Œå•æ•°æ®ç±»å‹çš„æ•°æ®ä¸èƒ½checkpoint åœ¨ckæ“ä½œä»¥åï¼ŒRDDå’Œæ•°æ®çš„å…³è”éƒ½å–æ¶ˆäº†ï¼ŒckæˆåŠŸä»¥åï¼Œæ•°æ®ç›´æ¥ä»ckpointé‡Œé¢è¯»å–å³å¯ï¼Œç”±äºckå±äºtransactionæ•…ckå¿…é¡»åœ¨è§¦å‘actionä¹‹å‰æ‰§è¡Œ å¦‚æœæŠŠRDDç¼“å­˜åˆ°å†…å­˜ï¼ˆå³åœ¨ckä¹‹å‰æœ‰cache rddåˆ°å†…å­˜çš„æ“ä½œï¼‰å°±ä¸ä¼šå¦èµ·ä¸€ä¸ªä½œä¸šä¸€æ­¥ä¸€æ­¥ä»åŸå§‹æ•°æ®è¿è¡Œï¼Œç„¶åå†ckåˆ°hdfsç›®å½•ï¼Œè€Œæ˜¯ç›´æ¥ä»å†…å­˜ä¸­è¯»å–æ•°æ® å¹¿æ’­å˜é‡ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæ¯”å¦‚mapreduce ä½¿ç”¨joinã€‚å½“mapæ®µæ‰€éœ€è¦çš„æ•°æ®é‡ä¸æ˜¯å¾ˆå¤§ï¼Œé¿å…ç½‘ç»œæµªè´¹ï¼Œä½¿ç”¨mapAsJoinæŠŠè§„åˆ™åŠ å…¥mapç«¯å†…å­˜å½“ä¸­ï¼Œè¿™æ ·mapreduceåœ¨mapç«¯å¯ä»¥ç›´æ¥åœ¨ç¼“å­˜ä¸­æ‹¿åˆ°è§„åˆ™ï¼Œè¿™æ ·å¯ä»¥æé«˜æ•ˆç‡ã€‚å¹¿æ’­å˜é‡çš„åŸç†ä¹Ÿæ˜¯å¦‚æ­¤ 1val bd = sc.broadcast(ruleArray) å¹¿æ’­å‡ºå» å¹¿æ’­ä¹‹åæ‰€æœ‰çš„executeréƒ½èƒ½æ”¶åˆ°ï¼Œè€Œä¸”æ˜¯ç›¸å½“äºåœ¨æ¯ä¸ªexecutorä¸­éƒ½å­˜æœ‰è¿™ä¸€å°éƒ¨åˆ†æ•°æ®ï¼Œä¸ç”¨é€šè¿‡ç½‘ç»œä¼ è¾“ï¼Œæé«˜æ•ˆç‡ åœ¨rddä¸­æ‹¿åˆ°å¹¿æ’­ä¸­çš„æ•°æ®ï¼Œ 123val arr = bd.value //å°†æ•°æ®å±•ç¤ºarr.toBuffer","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"åŸç†","slug":"åŸç†","permalink":"http://gangtieguo.cn/tags/åŸç†/"},{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"},{"name":"RDD","slug":"RDD","permalink":"http://gangtieguo.cn/tags/RDD/"}]},{"title":"Hadoopé›¶ç¢çŸ¥è¯†ç‚¹","slug":"Hadoopé›¶ç¢çŸ¥è¯†ç‚¹","date":"2018-08-15T16:34:55.243Z","updated":"2019-06-17T04:40:09.377Z","comments":true,"path":"2018/08/16/Hadoopé›¶ç¢çŸ¥è¯†ç‚¹/","link":"","permalink":"http://gangtieguo.cn/2018/08/16/Hadoopé›¶ç¢çŸ¥è¯†ç‚¹/","excerpt":"[TOC] æŸ¥çœ‹å…ƒæ•°æ®ä¿¡æ¯å¯ä»¥é€šè¿‡hdfsçš„ä¸€ä¸ªå·¥å…·æ¥æŸ¥çœ‹editsä¸­çš„ä¿¡æ¯ 12bin/hdfs oev -i edits -o edits.xmlbin/hdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml","text":"[TOC] æŸ¥çœ‹å…ƒæ•°æ®ä¿¡æ¯å¯ä»¥é€šè¿‡hdfsçš„ä¸€ä¸ªå·¥å…·æ¥æŸ¥çœ‹editsä¸­çš„ä¿¡æ¯ 12bin/hdfs oev -i edits -o edits.xmlbin/hdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml æŸ¥çœ‹ç›®å½•æ ‘12hdfsä¼šåœ¨é…ç½®æ–‡ä»¶ä¸­é…ç½®ä¸€ä¸ªdatanodeçš„å·¥ä½œç›®å½•å…ƒæ•°æ® æŸ¥çœ‹ç›®å½•ç»“æ„ tree hddata/ å®‰å…¨æ¨¡å¼ è¿™æ˜¯å› ä¸ºåœ¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿå¯åŠ¨çš„æ—¶å€™ï¼Œå¼€å§‹çš„æ—¶å€™ä¼šæœ‰å®‰å…¨æ¨¡å¼ï¼Œå½“åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿå¤„äºå®‰å…¨æ¨¡å¼çš„æƒ…å†µä¸‹ï¼Œæ–‡ä»¶ç³»ç»Ÿä¸­çš„å†…å®¹ä¸å…è®¸ä¿®æ”¹ä¹Ÿä¸å…è®¸åˆ é™¤ï¼Œç›´åˆ°å®‰å…¨æ¨¡å¼ç»“æŸã€‚å®‰å…¨æ¨¡å¼ä¸»è¦æ˜¯ä¸ºäº†ç³»ç»Ÿå¯åŠ¨çš„æ—¶å€™æ£€æŸ¥å„ä¸ªDataNodeä¸Šæ•°æ®å—çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶æ ¹æ®ç­–ç•¥å¿…è¦çš„å¤åˆ¶æˆ–è€…åˆ é™¤éƒ¨åˆ†æ•°æ®å—ã€‚è¿è¡ŒæœŸé€šè¿‡å‘½ä»¤ä¹Ÿå¯ä»¥è¿›å…¥å®‰å…¨æ¨¡å¼ã€‚åœ¨å®è·µè¿‡ç¨‹ä¸­ï¼Œç³»ç»Ÿå¯åŠ¨çš„æ—¶å€™å»ä¿®æ”¹å’Œåˆ é™¤æ–‡ä»¶ä¹Ÿä¼šæœ‰å®‰å…¨æ¨¡å¼ä¸å…è®¸ä¿®æ”¹çš„å‡ºé”™æç¤ºï¼Œåªéœ€è¦ç­‰å¾…ä¸€ä¼šå„¿å³å¯ã€‚ å®‰å…¨æ¨¡å¼çš„å‡ºç°ä¸ºç”šä¹ˆé›†ç¾¤å¯åŠ¨çš„æ—¶å€™è¦è¿›å…¥å®‰å…¨æ¨¡å¼ï¼Œå› ä¸ºä¸è¿›å…¥å®‰å…¨æ¨¡å¼ï¼ŒnameNodeä¸çŸ¥é“æ•°æ®å—å­˜æ”¾åœ¨å“ªå„¿ ä¸ºä»€ä¹ˆä¸€å¼€å§‹ï¼Œnamenodeä¸è®°å½•ä¸‹è¿™äº›æ•°æ®çš„åœ°å€å‘¢ï¼Ÿå› ä¸ºæ•°æ®ä½ç½®ä¿¡æ¯ä¼šå‘ç”Ÿå˜åŒ– è¿™äº›æ•°æ®è®°å½•åœ¨datanodeçš„blockå—ä¸‹çš„å…ƒæ•°æ®ä¸‹ã€‚fodarationæ¨¡å¼æ˜¯è®°å½•åœ¨juoinrnodeä¸‹çš„ juoinrnodeä¹Ÿæ˜¯ä¸€ä¸ªç¡¬ç›˜ç‰ˆçš„zookeeper åœ¨hdfså¯åŠ¨çš„æ—¶å€™ï¼Œå¤„äºå®‰å…¨æ¨¡å¼ï¼Œdatanodeå‘namenodeæ±‡æŠ¥è‡ªå·±çš„ipå’ŒæŒæœ‰çš„blockå—ä¿¡æ¯ï¼Œè¿™æ ·ï¼Œå®‰å…¨æ¨¡å¼ç»“æŸåï¼Œæ–‡ä»¶å—å’Œdatanodeçš„ipå…³è”ä¸Šï¼Œå­˜æ”¾æ•°æ®çš„ä½ç½®ä¹Ÿå°±å¯ä»¥æ‰¾åˆ° å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥æ‰‹åŠ¨ç¦»å¼€å®‰å…¨æ¨¡å¼ï¼š 1bin/hadoop dfsadmin -safemode leave ç”¨æˆ·å¯ä»¥é€šè¿‡dfsadmin -safemode value æ¥æ“ä½œå®‰å…¨æ¨¡å¼ï¼Œå‚æ•°valueçš„è¯´æ˜å¦‚ä¸‹ï¼šenter - è¿›å…¥å®‰å…¨æ¨¡å¼leave - å¼ºåˆ¶NameNodeç¦»å¼€å®‰å…¨æ¨¡å¼get - è¿”å›å®‰å…¨æ¨¡å¼æ˜¯å¦å¼€å¯çš„ä¿¡æ¯wait - ç­‰å¾…ï¼Œä¸€ç›´åˆ°å®‰å…¨æ¨¡å¼ç»“æŸã€‚ è¿˜æœ‰å¦å¤–ä¸€ç§æƒ…å†µå½“namenodeå‘ç°é›†ç¾¤ä¸­çš„blockä¸¢å¤±æ•°é‡è¾¾åˆ°ä¸€ä¸ªé˜€å€¼æ—¶ï¼Œnamenodeå°±è¿›å…¥å®‰å…¨æ¨¡å¼çŠ¶æ€ï¼Œä¸å†æ¥å—å®¢æˆ·ç«¯çš„æ•°æ®æ›´æ–°è¯·æ±‚ åœ¨æ­£å¸¸æƒ…å†µä¸‹ï¼Œnamenodeä¹Ÿæœ‰å¯èƒ½è¿›å…¥å®‰å…¨æ¨¡å¼ï¼šé›†ç¾¤å¯åŠ¨æ—¶ï¼ˆnamenodeå¯åŠ¨æ—¶ï¼‰å¿…å®šä¼šè¿›å…¥å®‰å…¨æ¨¡å¼ï¼Œç„¶åè¿‡ä¸€æ®µæ—¶é—´ä¼šè‡ªåŠ¨é€€å‡ºå®‰å…¨æ¨¡å¼ï¼ˆåŸå› æ˜¯datanodeæ±‡æŠ¥çš„è¿‡ç¨‹æœ‰ä¸€æ®µæŒç»­æ—¶é—´ï¼‰ ä¹Ÿç¡®å®æœ‰å¼‚å¸¸æƒ…å†µä¸‹å¯¼è‡´çš„å®‰å…¨æ¨¡å¼åŸå› ï¼šblockç¡®å®æœ‰ç¼ºå¤±æªæ–½ï¼šå¯ä»¥æ‰‹åŠ¨è®©namenodeé€€å‡ºå®‰å…¨æ¨¡å¼ï¼Œbin/hdfs dfsadmin -safemode leaveæˆ–è€…ï¼šè°ƒæ•´safemodeé—¨é™å€¼ï¼š dfs.safemode.threshold.pct=0.999f Hadoopçš„è°ƒåº¦å™¨FIFO: é»˜è®¤ï¼Œå…ˆè¿›å…ˆå‡ºçš„åŸåˆ™ Capacityï¼š è®¡ç®—èƒ½åŠ›è°ƒåº¦å™¨ï¼Œé€‰æ‹©å ç”¨æœ€å°ã€ä¼˜å…ˆçº§é«˜çš„æ‰§è¡Œ Fair: å…¬å¹³è°ƒåº¦ï¼Œæ‰€æœ‰çš„jobå…·æœ‰ç›¸åŒçš„èµ„æº ï¼ˆå¯ä»¥åœ¨core-site.xmlä¸­é…ç½®ï¼Œsparkæ˜¯æœ‰ä¸¤ç§è°ƒåº¦å™¨ï¼Œfifo å…¬å¹³è°ƒåº¦ï¼‰","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gangtieguo.cn/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://gangtieguo.cn/tags/HDFS/"},{"name":"åŸç†","slug":"åŸç†","permalink":"http://gangtieguo.cn/tags/åŸç†/"}]},{"title":"HDFSå…ƒæ•°æ®å¤‡ä»½æµç¨‹","slug":"HDFSå…ƒæ•°æ®å¤‡ä»½æµç¨‹","date":"2018-08-15T15:56:42.315Z","updated":"2018-12-25T03:53:58.236Z","comments":true,"path":"2018/08/15/HDFSå…ƒæ•°æ®å¤‡ä»½æµç¨‹/","link":"","permalink":"http://gangtieguo.cn/2018/08/15/HDFSå…ƒæ•°æ®å¤‡ä»½æµç¨‹/","excerpt":"hdfså…ƒæ•°æ®ç®¡ç†namenodeå¯¹æ•°æ®çš„ç®¡ç†é‡‡ç”¨äº†ä¸‰ç§å­˜å‚¨å½¢å¼ï¼š å†…å­˜å…ƒæ•°æ®(NameSystem) ç£ç›˜å…ƒæ•°æ®é•œåƒæ–‡ä»¶fsimagefsimageä¿å­˜ç€æ–‡ä»¶çš„åå­—ã€idã€åˆ†å—ä¿¡æ¯ã€å¤§å°ç­‰ï¼Œä½†æ˜¯ä¸ä¿å­˜datanodeåç§°å¯¹åº”çš„ip æ•°æ®æ“ä½œæ—¥å¿—æ–‡ä»¶editsï¼ˆå¯é€šè¿‡æ—¥å¿—è¿ç®—å‡ºå…ƒæ•°æ®ï¼‰","text":"hdfså…ƒæ•°æ®ç®¡ç†namenodeå¯¹æ•°æ®çš„ç®¡ç†é‡‡ç”¨äº†ä¸‰ç§å­˜å‚¨å½¢å¼ï¼š å†…å­˜å…ƒæ•°æ®(NameSystem) ç£ç›˜å…ƒæ•°æ®é•œåƒæ–‡ä»¶fsimagefsimageä¿å­˜ç€æ–‡ä»¶çš„åå­—ã€idã€åˆ†å—ä¿¡æ¯ã€å¤§å°ç­‰ï¼Œä½†æ˜¯ä¸ä¿å­˜datanodeåç§°å¯¹åº”çš„ip æ•°æ®æ“ä½œæ—¥å¿—æ–‡ä»¶editsï¼ˆå¯é€šè¿‡æ—¥å¿—è¿ç®—å‡ºå…ƒæ•°æ®ï¼‰ datanodeå’Œnamenodeäº¤äº’åœ¨hdfså¯åŠ¨çš„æ—¶å€™ï¼Œå¤„äºå®‰å…¨æ¨¡å¼ï¼Œdatanodeå‘namenodeæ±‡æŠ¥è‡ªå·±çš„ipå’ŒæŒæœ‰çš„blockå—ä¿¡æ¯ï¼Œè¿™æ ·ï¼Œå®‰å…¨æ¨¡å¼ç»“æŸåï¼Œæ–‡ä»¶å—å’Œdatanodeçš„ipå…³è”ä¸Šï¼Œå­˜æ”¾æ•°æ®çš„ä½ç½®ä¹Ÿå°±å¯ä»¥æ‰¾åˆ° å…ƒæ•°æ®ç®¡ç†æµç¨‹ ç£ç›˜æ–‡ä»¶æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹namenodeçš„ç›®å½•ç»“æ„çœ‹åˆ°ï¼š Aã€å†…å­˜ä¸­æœ‰ä¸€ä»½å®Œæ•´çš„å…ƒæ•°æ®(å†…å­˜meta data)Bã€ç£ç›˜æœ‰ä¸€ä¸ªâ€œå‡†å®Œæ•´â€çš„å…ƒæ•°æ®é•œåƒï¼ˆfsimageï¼‰æ–‡ä»¶(åœ¨namenodeçš„å·¥ä½œç›®å½•ä¸­)Cã€ç”¨äºè¡”æ¥å†…å­˜metadataå’ŒæŒä¹…åŒ–å…ƒæ•°æ®é•œåƒfsimageä¹‹é—´çš„æ“ä½œæ—¥å¿—ï¼ˆeditsæ–‡ä»¶æ³¨ï¼šå½“å®¢æˆ·ç«¯å¯¹hdfsä¸­çš„æ–‡ä»¶è¿›è¡Œæ–°å¢æˆ–è€…ä¿®æ”¹æ“ä½œï¼Œæ“ä½œè®°å½•é¦–å…ˆè¢«è®°å…¥editsæ—¥å¿—æ–‡ä»¶ä¸­ï¼Œå½“å®¢æˆ·ç«¯æ“ä½œæˆåŠŸåï¼Œç›¸åº”çš„å…ƒæ•°æ®ä¼šæ›´æ–°åˆ°å†…å­˜meta.dataä¸­ namenodeç®¡ç†å…ƒæ•°æ®è§£ænamenodeä¼šå°†å…ƒæ•°æ®æ”¾åœ¨å†…å­˜é‡Œé¢ï¼Œè¿™æ ·æ–¹ä¾¿å¿«é€Ÿå¯¹æ•°æ®çš„è¯·æ±‚ï¼Œç”±äºæ”¾åœ¨å†…å­˜ä¸­æ˜¯ä¸å®‰å…¨çš„ï¼Œæ‰€ä»¥å°±åºåˆ—åŒ–åˆ°fsimageé‡Œé¢ï¼Œå°±åƒjvmä¸­dumpï¼Œå°†å†…å­˜ä¸­æ‰€æœ‰æ•°æ®dumpå‡ºå»,ç”±äºä¸€æ¡å…ƒæ•°æ®å¤§å°ä¸º150byteï¼Œhdfsä¸åˆ©äºå­˜å‚¨å°å—æ–‡ä»¶ï¼Œæ‰€ä»¥å­˜å¤§æ–‡ä»¶åˆ’ç®—ï¼Œå› ä¸ºå…ƒæ•°æ®æ¶ˆè€—çš„å†…å­˜éƒ½æ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯å†…å­˜ä¸­çš„æ•°æ®é‡å¤ªå¤§ï¼Œä¸å¯èƒ½ç»å¸¸åºåˆ—åŒ–ï¼Œæ‰€ä»¥éœ€è¦å®šæ—¶åºåˆ—åŒ– æ‰€ä»¥å¼•å…¥äº†secondaryNameNodeæ›´æ–°å…ƒæ•°æ®çš„æ—¶å€™ï¼Œä¸å¯èƒ½å»ç›´æ¥è·Ÿæ›´æ”¹å…ƒæ•°æ®fsimageæ–‡ä»¶ï¼Œå› ä¸ºæ–‡ä»¶æ˜¯çº¿æ€§ç»“æ„ï¼Œå‡å¦‚é‡åˆ°æ›´æ”¹ä¸­é—´å†…å®¹ä¼šå¾ˆä¸æ–¹ä¾¿ï¼Œæ‰€æœ‰å°±å°†æ“ä½œä¿¡æ¯è®°å½•åœ¨editsæ—¥å¿—æ–‡ä»¶ä¸­ï¼Œåªæ˜¯è®°å½•æ“ä½œä¿¡æ¯ editsæ–‡ä»¶å®šæœŸè½¬ä¸ºå…ƒæ•°æ®ä¸ºäº†é˜²æ­¢editsè¿‡å¤šï¼Œå¯¼è‡´åœ¨å¯åŠ¨hdfsé›†ç¾¤datanodeçš„æ—¶å€™ä¼šå¾ˆæ…¢ï¼Œå› ä¸ºéœ€è¦å°†editsé€šè¿‡è½¬åŒ–å½¢æˆä¸ºå…ƒæ•°æ®fsimageæ–‡ä»¶ï¼Œæ‰€ä»¥åº”è¯¥å®šæœŸå°†editsæ–‡ä»¶è½¬æ¢ä¸ºfsimageå…ƒæ•°æ®ï¼Œç„¶åå°†fsimageæ›¿æ¢æ‰ secondaryNameNodeçš„å‡ºç°å¦‚æœnameNodeæ¥åšä¸Šé¢çš„editsè½¬æ¢ä¸ºå…ƒæ•°æ®çš„è¯ï¼Œç”±äºæ¶ˆè€—çš„èµ„æºå¤ªå¤§ï¼Œå°±ä¸èƒ½ä¸ºå…¶ä»–æ¯”å¦‚ä»hdfsä¸­è¯»å–æ•°æ®æœåŠ¡æä¾›èµ„æºï¼Œæˆ–è€…æä¾›æœåŠ¡çš„æ•ˆæœä¸å¥½æ‰€ä»¥è¿™ä¸ªæ—¶å€™å°±æŠŠåˆå¹¶æ“ä½œäº¤ç»™secondNameNodeæ¥åšè¿™ä¸ªè¿‡ç¨‹å«åšcheckpoint namenodeå’ŒsecondaryNameNodeæœ€å¥½ä¸è¦æ”¾åœ¨ä¸€å°æœºå™¨ä¸Š å®•æœºå¯èƒ½å¯¼è‡´æ•°æ®ä¸èƒ½æ¢å¤æµ‹è¯•ç¯å¢ƒæˆ–è€…å­¦ä¹ ç¯å¢ƒå¯ä»¥å¼„åœ¨ä¸€å°æœºå™¨ä¸Š ä¸ºäº†é˜²æ­¢namenodeå®•æœºå¯¼è‡´äº†æ•°æ®ä¸¢å¤±æ‰€ä½œé…ç½®å¯ä»¥åœ¨hdfs-site.xmlæ–‡ä»¶ä¸­åœ¨å¤šä¸ªæœºå™¨ä¸Šçš„ç›®å½•æ¥ä¿å­˜nameçš„editsï¼Œfsimageæ–‡ä»¶ 1234&lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/home/bigdata/names1,/home/bigdata/names2&lt;/value&gt;&lt;/property&gt; é…ç½®çš„å¤šä¸ªçš„è¯ï¼Œä¼šåŒæ—¶å¾€è¿™ä¸¤ä¸ªç›®å½•ä¸­å†™ å¦‚æœä¸é…ç½®è¿™ä¸ªé»˜è®¤çš„ç›®å½•æ˜¯core-site.xmlæ–‡ä»¶ä¸­é…ç½®çš„hadoopçš„ä¸´æ—¶æ–‡ä»¶ 1234&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/bigdata/apps/hadoop-2.6.4/tmp&lt;/value&gt; &lt;/property&gt;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gangtieguo.cn/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://gangtieguo.cn/tags/HDFS/"},{"name":"åŸç†","slug":"åŸç†","permalink":"http://gangtieguo.cn/tags/åŸç†/"}]},{"title":"Hdfsç»“æ„æ€§èƒ½åˆ†æåŠè¯»å†™æµç¨‹","slug":"Hdfsç»“æ„æ€§èƒ½åˆ†æåŠè¯»å†™æµç¨‹","date":"2018-08-15T15:55:15.892Z","updated":"2018-12-25T03:53:56.755Z","comments":true,"path":"2018/08/15/Hdfsç»“æ„æ€§èƒ½åˆ†æåŠè¯»å†™æµç¨‹/","link":"","permalink":"http://gangtieguo.cn/2018/08/15/Hdfsç»“æ„æ€§èƒ½åˆ†æåŠè¯»å†™æµç¨‹/","excerpt":"[TOC] hdfsçš„è®¾è®¡æ€æƒ³åˆ†è€Œæ²»ä¹‹ï¼šå°†å¤§æ–‡ä»¶ã€å¤§æ‰¹é‡æ–‡ä»¶ï¼Œåˆ†å¸ƒå¼å­˜æ”¾åœ¨å¤§é‡æœåŠ¡å™¨ä¸Šï¼Œä»¥ä¾¿äºé‡‡å–åˆ†è€Œæ²»ä¹‹çš„æ–¹å¼å¯¹æµ·é‡æ•°æ®è¿›è¡Œè¿ç®—åˆ†æé¦–å…ˆï¼Œå®ƒæ˜¯ä¸€ä¸ªæ–‡ä»¶ç³»ç»Ÿï¼Œç”¨äºå­˜å‚¨æ–‡ä»¶ï¼Œé€šè¿‡ç»Ÿä¸€çš„å‘½åç©ºé—´â€”â€”ç›®å½•æ ‘æ¥å®šä½æ–‡ä»¶å…¶æ¬¡ï¼Œå®ƒæ˜¯åˆ†å¸ƒå¼çš„ï¼Œç”±å¾ˆå¤šæœåŠ¡å™¨è”åˆèµ·æ¥å®ç°å…¶åŠŸèƒ½ï¼Œé›†ç¾¤ä¸­çš„æœåŠ¡å™¨æœ‰å„è‡ªçš„è§’è‰²ï¼›","text":"[TOC] hdfsçš„è®¾è®¡æ€æƒ³åˆ†è€Œæ²»ä¹‹ï¼šå°†å¤§æ–‡ä»¶ã€å¤§æ‰¹é‡æ–‡ä»¶ï¼Œåˆ†å¸ƒå¼å­˜æ”¾åœ¨å¤§é‡æœåŠ¡å™¨ä¸Šï¼Œä»¥ä¾¿äºé‡‡å–åˆ†è€Œæ²»ä¹‹çš„æ–¹å¼å¯¹æµ·é‡æ•°æ®è¿›è¡Œè¿ç®—åˆ†æé¦–å…ˆï¼Œå®ƒæ˜¯ä¸€ä¸ªæ–‡ä»¶ç³»ç»Ÿï¼Œç”¨äºå­˜å‚¨æ–‡ä»¶ï¼Œé€šè¿‡ç»Ÿä¸€çš„å‘½åç©ºé—´â€”â€”ç›®å½•æ ‘æ¥å®šä½æ–‡ä»¶å…¶æ¬¡ï¼Œå®ƒæ˜¯åˆ†å¸ƒå¼çš„ï¼Œç”±å¾ˆå¤šæœåŠ¡å™¨è”åˆèµ·æ¥å®ç°å…¶åŠŸèƒ½ï¼Œé›†ç¾¤ä¸­çš„æœåŠ¡å™¨æœ‰å„è‡ªçš„è§’è‰²ï¼› hdfsåŠŸèƒ½å’Œç‰¹ç‚¹hdfsçš„é‡è¦ç‰¹æ€§å¦‚ä¸‹ï¼š HDFSä¸­çš„æ–‡ä»¶åœ¨ç‰©ç†ä¸Šæ˜¯åˆ†å—å­˜å‚¨ï¼ˆblockï¼‰ï¼Œå—çš„å¤§å°å¯ä»¥é€šè¿‡é…ç½®å‚æ•°( dfs.blocksize)æ¥è§„å®šï¼Œé»˜è®¤å¤§å°åœ¨hadoop2.xç‰ˆæœ¬ä¸­æ˜¯128Mï¼Œè€ç‰ˆæœ¬ä¸­æ˜¯64M HDFSæ–‡ä»¶ç³»ç»Ÿä¼šç»™å®¢æˆ·ç«¯æä¾›ä¸€ä¸ªç»Ÿä¸€çš„æŠ½è±¡ç›®å½•æ ‘ï¼Œå®¢æˆ·ç«¯é€šè¿‡è·¯å¾„æ¥è®¿é—®æ–‡ä»¶ï¼Œå½¢å¦‚ï¼šhdfs://namenode:port/dir-a/dir-b/dir-c/file.data ç›®å½•ç»“æ„åŠæ–‡ä»¶åˆ†å—ä¿¡æ¯(å…ƒæ•°æ®)çš„ç®¡ç†ç”±namenodeèŠ‚ç‚¹æ‰¿æ‹…â€”â€”namenodeæ˜¯HDFSé›†ç¾¤ä¸»èŠ‚ç‚¹ï¼Œè´Ÿè´£ç»´æŠ¤æ•´ä¸ªhdfsæ–‡ä»¶ç³»ç»Ÿçš„ç›®å½•æ ‘ï¼Œä»¥åŠæ¯ä¸€ä¸ªè·¯å¾„ï¼ˆæ–‡ä»¶ï¼‰æ‰€å¯¹åº”çš„blockå—ä¿¡æ¯ï¼ˆblockçš„idï¼ŒåŠæ‰€åœ¨çš„datanodeæœåŠ¡å™¨ï¼‰ æ–‡ä»¶çš„å„ä¸ªblockçš„å­˜å‚¨ç®¡ç†ç”±datanodeèŠ‚ç‚¹æ‰¿æ‹…â€”- datanodeæ˜¯HDFSé›†ç¾¤ä»èŠ‚ç‚¹ï¼Œæ¯ä¸€ä¸ªblockéƒ½å¯ä»¥åœ¨å¤šä¸ªdatanodeä¸Šå­˜å‚¨å¤šä¸ªå‰¯æœ¬ï¼ˆå‰¯æœ¬æ•°é‡ä¹Ÿå¯ä»¥é€šè¿‡å‚æ•°è®¾ç½®dfs.replicationï¼‰ HDFSæ˜¯è®¾è®¡æˆé€‚åº”ä¸€æ¬¡å†™å…¥ï¼Œå¤šæ¬¡è¯»å‡ºçš„åœºæ™¯ï¼Œä¸”ä¸æ”¯æŒæ–‡ä»¶çš„ä¿®æ”¹ Hdfsçš„ç»“æ„1.HDFSé›†ç¾¤åˆ†ä¸ºä¸¤å¤§è§’è‰²ï¼šNameNodeã€DataNode ï¼ˆsecondary NameNodeï¼‰2.NameNodeè´Ÿè´£ç®¡ç†æ•´ä¸ªæ–‡ä»¶ç³»ç»Ÿçš„å…ƒæ•°æ®ï¼šè®°å½•æ–‡ä»¶åœ¨å“ªé‡Œ3.DataNode è´Ÿè´£ç®¡ç†ç”¨æˆ·çš„æ–‡ä»¶æ•°æ®å—ï¼šä¸è´Ÿè´£åˆ‡å—ï¼Œè´Ÿè´£ä¿ç®¡4.æ–‡ä»¶ä¼šæŒ‰ç…§å›ºå®šçš„å¤§å°ï¼ˆblocksizeï¼‰åˆ‡æˆè‹¥å¹²å—ååˆ†å¸ƒå¼å­˜å‚¨åœ¨è‹¥å¹²å°datanodeä¸Š5.æ¯ä¸€ä¸ªæ–‡ä»¶å—å¯ä»¥æœ‰å¤šä¸ªå‰¯æœ¬ï¼Œå¹¶å­˜æ”¾åœ¨ä¸åŒçš„datanodeä¸Šå‰¯æœ¬ä¸ä¼šæ”¾åœ¨åŒä¸€ä¸ªæœºå™¨ä¸Šï¼Œå› ä¸ºå‰¯æœ¬å°±æ˜¯é˜²æ­¢å®•æœºï¼Œ6.Datanodeä¼šå®šæœŸå‘Namenodeæ±‡æŠ¥è‡ªèº«æ‰€ä¿å­˜çš„æ–‡ä»¶blockä¿¡æ¯ï¼Œè€Œnamenodeåˆ™ä¼šè´Ÿè´£ä¿æŒæ–‡ä»¶çš„å‰¯æœ¬æ•°é‡å› ä¸ºdatanodeå¦‚æœå®•æœºçš„è¯ï¼Œnameè¯¥æœºå™¨ä¸Šçš„å¯¹åº”çš„å‰¯æœ¬æ•°æ®å°†ä¼šæ¶ˆå¤±ï¼Œè¿™æ ·éœ€è¦å°†å…¶åœ¨å…¶ä»–æœºå™¨ä¸Šè¿›è¡Œæ¢å¤ï¼Œæ¢å¤çš„è¯ï¼Œå°±éœ€è¦ä¸Šé¢å°±éœ€è¦æ•°æ®å’Œæœªå®•æœºæ—¶çš„æ•°æ®å°½é‡ä¿æŒä¸€è‡´ï¼Œæ‰€ä»¥éœ€è¦ä¾èµ–äºdatanodeå®šæœŸæ±‡æŠ¥ï¼Œä¸ç„¶å·®è·çš„æ•°æ®ä¼šå¾ˆå¤§7.HDFSçš„å†…éƒ¨å·¥ä½œæœºåˆ¶å¯¹å®¢æˆ·ç«¯ä¿æŒé€æ˜ï¼Œå®¢æˆ·ç«¯è¯·æ±‚è®¿é—®HDFSéƒ½æ˜¯é€šè¿‡å‘namenodeç”³è¯·æ¥è¿›è¡Œ Hdfså†™æ“ä½œ è¯¦ç»†æ­¥éª¤è§£æ 1ã€æ ¹namenodeé€šä¿¡è¯·æ±‚ä¸Šä¼ æ–‡ä»¶ï¼Œnamenodeæ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œçˆ¶ç›®å½•æ˜¯å¦å­˜åœ¨ ä¸å­˜åœ¨åˆ™ä¼šè¿”å›path not existå¼‚å¸¸ 2ã€namenodeè¿”å›æ˜¯å¦å¯ä»¥ä¸Šä¼  3ã€clientè¯·æ±‚ç¬¬ä¸€ä¸ª blockï¼ˆ0-128mï¼‰è¯¥ä¼ è¾“åˆ°å“ªäº›datanodeæœåŠ¡å™¨ä¸Š è¿”å›è¯¥blockå­˜æ”¾çš„ä½ç½®ï¼ŒåŠå…¶å‰¯æœ¬çš„ä¿¡æ¯å­˜æ”¾çš„ä½ç½® 4ã€namenodeè¿”å›3ä¸ªdatanodeæœåŠ¡å™¨ABC å‰¯æœ¬é€‰æ‹©ç­–ç•¥ï¼ˆå¦‚æœè®¾ç½®ä¸ºè¢«åˆ†æ•°ä¸º2çš„è¯ï¼‰ è€ƒè™‘ç©ºé—´å’Œè·ç¦»çš„å› ç´ ï¼Œç½‘ç»œè·³è½¬çš„è·³æ•°ï¼Œæ¯”å¦‚è¯´æœºæ¶çš„ä½ç½®ï¼Œ ç¬¬ä¸€å°æ˜¯çœ‹è°æ¯”è¾ƒè¿‘ï¼ˆæœºæ¶ï¼‰ï¼Œå› ä¸ºä¼ è¾“æ¯”è¾ƒå¿«ï¼Œå‰¯æœ¬åˆ™æ˜¯æ˜¯çœ‹è°æ¯”è¾ƒè¿œï¼Œé˜²æ­¢æœºæ¶å‡ºé—®é¢˜ï¼ˆå¦‚æ–­ç”µï¼‰ï¼Œå¹²æ‰°æ€§æ›´å° è€Œé›†ç¾¤å…¨çº¿å´©å¡Œ 5ã€clientè¯·æ±‚3å°dnä¸­çš„ä¸€å°Aä¸Šä¼ æ•°æ®ï¼ˆæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªRPCè°ƒç”¨ï¼Œå»ºç«‹pipelineï¼‰ï¼ŒAæ”¶åˆ°è¯·æ±‚ä¼šç»§ç»­è°ƒç”¨Bï¼Œç„¶åBè°ƒç”¨Cï¼Œå°†çœŸä¸ªpipelineå»ºç«‹å®Œæˆï¼Œé€çº§è¿”å›å®¢æˆ·ç«¯ è¿™æ ·æ˜¯é˜²æ­¢æ•´ä¸ªæµç¨‹å˜æ…¢ï¼ŒåŒæ—¶åˆ›å»ºé€šé“ï¼Œå…ˆå»ºç«‹é€šé“pipeline,é€šé“ 6ã€clientå¼€å§‹å¾€Aä¸Šä¼ ç¬¬ä¸€ä¸ªblockï¼ˆå…ˆä»ç£ç›˜è¯»å–æ•°æ®æ”¾åˆ°ä¸€ä¸ªæœ¬åœ°å†…å­˜ç¼“å­˜bytebufï¼‰ï¼Œä»¥packetä¸ºå•ä½ï¼ŒAæ”¶åˆ°ä¸€ä¸ªpacketå°±ä¼šä¼ ç»™Bï¼ŒBä¼ ç»™Cï¼›Aæ¯ä¼ ä¸€ä¸ªpacketä¼šæ”¾å…¥ä¸€ä¸ªåº”ç­”é˜Ÿåˆ—ç­‰å¾…åº”ç­” å› ä¸ºç­‰ä¸€ä¸ªblockå†™æ»¡ä¹‹åå†ä¼ é€ï¼Œé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼Œæ‰€ä»¥æ˜¯æ¥æ”¶ä¸€ä¸ªpacketå°±ä¼šå†™å…¥åˆ°ç®¡é“æµpipelineä¸­ã€‚ åªè¦ä¸Šä¼ ä¸€ä¸ªæˆåŠŸï¼Œåˆ™å®¢æˆ·ç«¯è§†ä¸ºä¸Šä¼ æˆåŠŸï¼Œå› ä¸ºå¦‚æœæ²¡ä¸Šä¼ æˆåŠŸï¼Œnamenodeä¼šè¿›è¡Œå¼‚æ­¥çš„å¤åˆ¶å‰¯æœ¬çš„ä¿¡æ¯ 7ã€å½“ä¸€ä¸ªblockä¼ è¾“å®Œæˆä¹‹åï¼Œclientå†æ¬¡è¯·æ±‚namenodeä¸Šä¼ ç¬¬äºŒä¸ªblockçš„æœåŠ¡å™¨ã€‚ æ³¨ï¼šå†™çš„è¿‡ç¨‹ä¸­ï¼Œnamenodeè®°å½•ä¸‹æ¥äº†æ–‡ä»¶è·¯å¾„ï¼Œæ–‡ä»¶æœ‰å‡ ä¸ªblockä¹Ÿè®°å½•ä¸‹æ¥äº†ï¼Œæ¯ä¸ªblockåˆ†é…åˆ°å“ªäº›æœºå™¨ä¸Šä¹Ÿè®°å½•ä¸‹åˆ°äº†ï¼ŒåŠå…¶æ¯ä¸ªblockçš„å‰¯æœ¬ä¿¡æ¯ï¼Œå‰¯æœ¬åœ¨é‚£å‡ ä¸ªæœºå™¨ä¸Šã€‚ æ ¡éªŒçš„æ—¶å€™ä¸æ˜¯ä¸€ä¸ªpacketï¼ˆä¸€æ‰¹chunkï¼Œå…±64kï¼‰æ ¡éªŒï¼Œè€Œæ˜¯ä»¥ä¸€ä¸ªchunkæ¥æ ¡éªŒï¼Œä¸€ä¸ªchunkæ˜¯512byteï¼ˆå­—èŠ‚ï¼‰ Hdfsè¯»æ“ä½œ å®¢æˆ·ç«¯å°†è¦è¯»å–çš„æ–‡ä»¶è·¯å¾„å‘é€ç»™namenodeï¼Œnamenodeè·å–æ–‡ä»¶çš„å…ƒä¿¡æ¯ï¼ˆä¸»è¦æ˜¯blockçš„å­˜æ”¾ä½ç½®ä¿¡æ¯ï¼‰è¿”å›ç»™å®¢æˆ·ç«¯ï¼Œå®¢æˆ·ç«¯æ ¹æ®è¿”å›çš„ä¿¡æ¯æ‰¾åˆ°ç›¸åº”datanodeé€ä¸ªè·å–æ–‡ä»¶çš„blockå¹¶åœ¨å®¢æˆ·ç«¯æœ¬åœ°è¿›è¡Œæ•°æ®è¿½åŠ åˆå¹¶ä»è€Œè·å¾—æ•´ä¸ªæ–‡ä»¶ 1ã€è·Ÿnamenodeé€šä¿¡æŸ¥è¯¢å…ƒæ•°æ®ï¼Œæ‰¾åˆ°æ–‡ä»¶å—æ‰€åœ¨çš„datanodeæœåŠ¡å™¨ 2ã€æŒ‘é€‰ä¸€å°datanodeï¼ˆå°±è¿‘åŸåˆ™ï¼Œç„¶åéšæœºï¼‰æœåŠ¡å™¨ï¼Œè¯·æ±‚å»ºç«‹socketæµ 3ã€datanodeå¼€å§‹å‘é€æ•°æ®ï¼ˆä»ç£ç›˜é‡Œé¢è¯»å–æ•°æ®æ”¾å…¥æµï¼Œä»¥packetä¸ºå•ä½æ¥åšæ ¡éªŒï¼‰ 4ã€å®¢æˆ·ç«¯ä»¥packetä¸ºå•ä½æ¥æ”¶ï¼Œç°åœ¨æœ¬åœ°ç¼“å­˜ï¼Œç„¶åå†™å…¥ç›®æ ‡æ–‡ä»¶ å¯¹æ­¤ï¼Œæˆ‘ä»¬äº†è§£äº†hdfsçš„è¯»å†™æµç¨‹ï¼Œé‚£ä¹ˆæˆ‘ä»¬å†æ¥çœ‹çœ‹hdfså…ƒæ•°æ®çš„ç®¡ç† å°è´´å£«hdfsä¸­datanodeçš„åˆå§‹åŒ– hdfsä¼šåœ¨é…ç½®æ–‡ä»¶ä¸­é…ç½®ä¸€ä¸ªnamenodeçš„å·¥ä½œç›®å½•å…ƒæ•°æ® æŸ¥çœ‹ç›®å½•ç»“æ„ tree $DATANODE/ datanodeçš„å·¥ä½œç›®å½•æ˜¯åœ¨datanodeå¯åŠ¨ååˆå§‹åŒ–çš„è€Œhadoop namenode format åªä¼šåˆå§‹nameçš„å·¥ä½œç›®å½•ï¼Œå’Œdatanodeæ²¡æœ‰å…³ç³» å¦‚ä½•æŠŠä¸€ä¸ªhdfsçš„ä¸€ä¸ªèŠ‚ç‚¹åŠ å…¥åˆ°å¦ä¸€ä¸ªé›†ç¾¤å› ä¸ºåœ¨åŸæ¥çš„ç›®å½•ä¸­ä¼šæœ‰åŸæ¥é›†ç¾¤çš„ä¿¡æ¯å¦‚ï¼šClusterID å¿…é¡»è¦å°†hdfs datanodeçš„å·¥ä½œç›®å½•åˆ é™¤ï¼Œä¸ç„¶æŒæœ‰ä¸Šä¸€ä¸ªé›†ç¾¤çš„datanodeçš„å·¥ä½œç›®å½•ï¼Œä¼šè®¤ä¸ºæ˜¯ä¸€ä¸ªè¯¯æ“ä½œï¼Œä¸ºäº†é˜²æ­¢ä¸¢å¤±æ•°æ®ï¼Œä¸ä¼šè®©å…¶è¿æ¥ä¸Š","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gangtieguo.cn/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://gangtieguo.cn/tags/HDFS/"},{"name":"åŸç†","slug":"åŸç†","permalink":"http://gangtieguo.cn/tags/åŸç†/"}]},{"title":"Sparkå¯åŠ¨æµç¨‹åŠä¸€äº›å°æ€»ç»“","slug":"Sparkå¯åŠ¨æµç¨‹åŠä¸€äº›å°æ€»ç»“","date":"2018-08-15T15:54:20.258Z","updated":"2019-06-17T04:40:09.400Z","comments":true,"path":"2018/08/15/Sparkå¯åŠ¨æµç¨‹åŠä¸€äº›å°æ€»ç»“/","link":"","permalink":"http://gangtieguo.cn/2018/08/15/Sparkå¯åŠ¨æµç¨‹åŠä¸€äº›å°æ€»ç»“/","excerpt":"[TOC] sparkçš„æ¶æ„æ¨¡å‹ è§’è‰²åŠŸèƒ½Driverï¼šä»¥spark-submitæäº¤ç¨‹åºä¸ºä¾‹ï¼Œæ‰§è¡Œè¯¥å‘½ä»¤çš„ä¸»æœºä¸ºdriverï¼ˆåœ¨ä»»æ„ä¸€å°å®‰è£…äº†sparkï¼ˆspark submitï¼‰çš„æœºå™¨ä¸Šå¯åŠ¨ä¸€ä¸ªä»»åŠ¡çš„å®¢æˆ·ç«¯ä¹Ÿå°±æ˜¯Driver ã€‚å®¢æˆ·ç«¯ä¸é›†ç¾¤éœ€è¦å»ºç«‹é“¾æ¥ï¼Œå»ºç«‹çš„è¿™ä¸ªé“¾æ¥å¯¹è±¡å«åšsparkContextï¼Œåªæœ‰è¿™ä¸ªå¯¹è±¡åˆ›å»ºæˆåŠŸæ‰æ ‡å¿—è¿™è¿™ä¸ªå®¢æˆ·ç«¯ä¸sparké›†ç¾¤é“¾æ¥æˆåŠŸã€‚SparkContextæ˜¯driverè¿›ç¨‹ä¸­çš„ä¸€ä¸ªå¯¹è±¡ï¼Œæäº¤ä»»åŠ¡çš„æ—¶å€™ï¼ŒæŒ‡å®šäº†æ¯å°æœºå™¨éœ€è¦å¤šå°‘ä¸ªæ ¸coresï¼Œéœ€è¦çš„å†…å­˜ ï¼‰ Master: ç»™ä»»åŠ¡æä¾›èµ„æºï¼Œåˆ†é…èµ„æºï¼Œmasterè·Ÿworkeré€šä¿¡ï¼ŒæŠ¥æ´»å’Œæ›´æ–°èµ„æº Worker: ä»¥å­è¿›ç¨‹çš„æ–¹å¼å¯åŠ¨executor Executorï¼šDriveræäº¤ç¨‹åºåˆ°executor(CoarseGrainedExecutorBankend)ï¼Œæ‰§è¡Œä»»åŠ¡çš„è¿›ç¨‹ï¼Œexectoræ˜¯taskè¿è¡Œçš„å®¹å™¨","text":"[TOC] sparkçš„æ¶æ„æ¨¡å‹ è§’è‰²åŠŸèƒ½Driverï¼šä»¥spark-submitæäº¤ç¨‹åºä¸ºä¾‹ï¼Œæ‰§è¡Œè¯¥å‘½ä»¤çš„ä¸»æœºä¸ºdriverï¼ˆåœ¨ä»»æ„ä¸€å°å®‰è£…äº†sparkï¼ˆspark submitï¼‰çš„æœºå™¨ä¸Šå¯åŠ¨ä¸€ä¸ªä»»åŠ¡çš„å®¢æˆ·ç«¯ä¹Ÿå°±æ˜¯Driver ã€‚å®¢æˆ·ç«¯ä¸é›†ç¾¤éœ€è¦å»ºç«‹é“¾æ¥ï¼Œå»ºç«‹çš„è¿™ä¸ªé“¾æ¥å¯¹è±¡å«åšsparkContextï¼Œåªæœ‰è¿™ä¸ªå¯¹è±¡åˆ›å»ºæˆåŠŸæ‰æ ‡å¿—è¿™è¿™ä¸ªå®¢æˆ·ç«¯ä¸sparké›†ç¾¤é“¾æ¥æˆåŠŸã€‚SparkContextæ˜¯driverè¿›ç¨‹ä¸­çš„ä¸€ä¸ªå¯¹è±¡ï¼Œæäº¤ä»»åŠ¡çš„æ—¶å€™ï¼ŒæŒ‡å®šäº†æ¯å°æœºå™¨éœ€è¦å¤šå°‘ä¸ªæ ¸coresï¼Œéœ€è¦çš„å†…å­˜ ï¼‰ Master: ç»™ä»»åŠ¡æä¾›èµ„æºï¼Œåˆ†é…èµ„æºï¼Œmasterè·Ÿworkeré€šä¿¡ï¼ŒæŠ¥æ´»å’Œæ›´æ–°èµ„æº Worker: ä»¥å­è¿›ç¨‹çš„æ–¹å¼å¯åŠ¨executor Executorï¼šDriveræäº¤ç¨‹åºåˆ°executor(CoarseGrainedExecutorBankend)ï¼Œæ‰§è¡Œä»»åŠ¡çš„è¿›ç¨‹ï¼Œexectoræ˜¯taskè¿è¡Œçš„å®¹å™¨ Driverç«¯driveråˆ›å»ºsparkContextäºsparké›†ç¾¤å»ºç«‹è¿æ¥ï¼Œç„¶åå‘masterç”³è¯·èµ„æºï¼Œmasteræ¥è°ƒåº¦å†³å®šå‘workerçš„å“ªå°æœºå™¨ä¸Šæ‰§è¡Œ ï¼Œåœ¨åˆé€‚çš„workerï¼ˆèµ„æºåˆ†é…ç­–ç•¥ï¼‰ä¸Šä»¥å­è¿›ç¨‹çš„æ–¹å¼å¯åŠ¨excutoræ¥æ‰§è¡Œ ï¼ˆClass.forName()ä¼šåªåœ¨è‡ªå·±çš„è¿›ç¨‹é‡Œé¢æ‰§è¡Œï¼Œåå°„æ˜¯åœ¨åŒä¸€è¿›ç¨‹ ï¼Œå¯åŠ¨javaå­è¿›ç¨‹çš„æ–¹æ³• æ˜¯å¦ä¸€ä¸ªè¿›ç¨‹ï¼Œdebugè·³ä¸åˆ°å­è¿›ç¨‹ä¸­å»ï¼Œè¿›ç¨‹ä¹‹é—´è¿›è¡Œæ–¹æ³•è°ƒç”¨åªæœ‰rpcæ‰å¯ä»¥ ï¼‰æ­¤è¿‡ç¨‹åªæ˜¯å¯åŠ¨executorï¼Œæœ€ç»ˆä»»åŠ¡çš„æ‰§è¡Œéœ€è¦ç­‰åˆ°actionè§¦å‘ï¼Œæäº¤ç¨‹åºæœ€ç»ˆä¹Ÿæ˜¯æäº¤åˆ°executorä¸Šï¼Œåœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œ driveræäº¤ä¸€ä¸ªapplicationæ—¶å°±ä¼šåœ¨driverä¸»æœºç”Ÿæˆä¸€ä¸ªsparkSubmitè¿›ç¨‹æ¥ç›‘æ§ä»»åŠ¡çš„æ‰§è¡Œæƒ…å†µã€‚actionè§¦å‘ç¨‹åºè¿è¡Œåï¼Œexecutoråªä¼šé€šè¿‡driverClinetä¸driveräº¤äº’ï¼ŒapplicationMaster ä¸è´Ÿè´£å…·ä½“æŸä¸ªä»»åŠ¡çš„è¿›åº¦ ,åªè´Ÿè´£èµ„æºçš„è°ƒåº¦å’Œç›‘æ§ã€‚driveråœ¨actionçš„æ—¶å€™æ‰ä¼šæŠŠä»»åŠ¡æäº¤ç»™executorã€‚ masterä¸workeräº¤äº’workerå’Œmasterä»¥è¿›ç¨‹æ–¹å¼å¯åŠ¨å ï¼Œworkerä¼šå‘masterè¿›è¡Œæ³¨å†Œ,workeråœ¨å¯åŠ¨çš„æ—¶å€™ï¼Œä¼šåœ¨spark envä¸­æŒ‡å®šworkerçš„èµ„æºï¼Œå¦‚æœæ²¡æŒ‡å®šçš„è¯ï¼Œä¼šé»˜è®¤ä½¿ç”¨æœºå™¨çš„æ‰€æœ‰æ ¸æ•°ï¼Œæ‰€æœ‰å†…å­˜-1Gçš„å†…å­˜ï¼Œé¢„ç•™1Gç»™æ“ä½œç³»ç»Ÿï¼Œç„¶åworkerä¼šå‘ŠçŸ¥masterè‡ªå·±æ‹¥æœ‰çš„èµ„æºï¼Œå¦‚æ ¸æ•°coresï¼Œå†…å­˜ç­‰ã€‚åœ¨ä¹‹åçš„è¿‡ç¨‹ä¸­ï¼Œworkerä¼šå®æ—¶å‘masterå‘é€å¿ƒè·³æŠ¥æ´»ã€‚ èµ„æºåˆ†é…ç­–ç•¥sparkä»»åŠ¡èµ„æºåˆ†é…æœ‰ä¸¤ç§ç­–ç•¥ï¼šå°½é‡æ‰“æ•£ å’Œå°½é‡é›†ä¸­ ä»¥task1éœ€è¦2coreä¸ºä¾‹ï¼š å°½é‡æ‰“æ•£æ˜¯å°½å¯èƒ½çš„å°†ä»»åŠ¡åˆ†æ•£åˆ°ä¸åŒçš„workeræ¥å¯åŠ¨exectorï¼Œæœºå™¨1åˆ†é…1coreï¼Œæœºå™¨2åˆ†é…1core å°½é‡é›†ä¸­æ˜¯å°½å¯èƒ½åœ¨æ›´å°‘çš„æœºå™¨ä¸Šæ¥å¯åŠ¨workerå¹¶å¯åŠ¨exectorï¼Œåœ¨åˆ†é…çš„æœºå™¨ä¸Šå°½å¯èƒ½å¤šçš„åˆ†é…èµ„æºï¼Œè¾¾åˆ°é›†ä¸­åœ¨æ›´å°‘çš„æœºå™¨ä¸Šçš„ç›®çš„ã€‚å¦‚åœ¨æœºå™¨1ä¸Šåˆ†é…2coreï¼ˆå‰ææ˜¯æœºå™¨1 å‰©ä½™coreæ•°&gt;=2ï¼‰ RDDæ•´ä¸ªè¿è¡Œæµç¨‹é¦–å…ˆrddäº§ç”Ÿè¿‡åï¼Œç»è¿‡ä¸€ç³»åˆ—çš„å¤„ç†ï¼Œæ„æˆç›¸äº’ä¾èµ–ï¼Œåœ¨æœ‰å®½ä¾èµ–çš„æƒ…å†µä¸‹ä¼šåˆ’åˆ†stageï¼Œæ„æˆä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ä¹Ÿå°±æ˜¯DAGï¼Œæ•´ä¸ªrddçš„æ„å»ºï¼Œå®Œæˆåœ¨actionè§¦å‘ä¹‹å‰ï¼Œactionè§¦å‘å°±ä¼šå°†taskæäº¤åˆ°executorå®¹å™¨ä¸­è¿è¡Œ Driverä¸executoräº¤äº’åœ¨ä»»åŠ¡æäº¤çš„æ—¶å€™ï¼Œä¼šå°†driverä¿¡æ¯å°è£…ï¼Œå…ˆå‘Šè¯‰ç»™masterï¼Œmasterå†å‘Šè¯‰ç»™workerï¼Œç„¶åworkerå¯åŠ¨executorè¿›ç¨‹çš„æ—¶å€™ï¼Œä¼šå°†ä¿¡æ¯å‘Šè¯‰ç»™executorï¼Œæ•…è€Œæœ€ç»ˆexectorèƒ½è·å–åˆ°driverçš„ä¿¡æ¯ åœ¨executoråˆ›å»ºæˆåŠŸåï¼Œå°±ä¼šå’Œdriverå»ºç«‹è¿æ¥ï¼Œè€Œä¸å†ä¸Masteré€šä¿¡ï¼Œè¿™æ ·æ˜¯ä¸ºäº†å‡å°‘Masterå‹åŠ›ï¼Œä¹Ÿä¸è®©Masteræˆä¸ºæ€§èƒ½ç“¶é¢ˆã€‚ ç”±äºexecutoræœ‰å¯èƒ½åœ¨å¾ˆå¤šå°æœºå™¨ä¸Šå¯åŠ¨ï¼Œdriveræ— æ³•çŸ¥æ™“åœ¨å“ªå°æœºå™¨ä¸Šå¯åŠ¨æœ‰exectorï¼Œæ‰€ä»¥éœ€è¦executorå’Œdriverè¿›è¡Œrpcé€šä¿¡ï¼ˆnettyæˆ–è€…akkaï¼‰ä¸»åŠ¨å»ºç«‹è¿æ¥ã€‚å»ºç«‹é€šä¿¡åexectorå‘driveræ±‡æŠ¥çŠ¶æ€åŠå…¶æ‰§è¡Œè¿›åº¦ï¼Œdriverå‘executoræäº¤è®¡ç®—ä»»åŠ¡ï¼Œç„¶ååœ¨executorä¸­æ‰§è¡Œï¼Œåªæœ‰å½“RDDè§¦å‘actionçš„æ—¶å€™ï¼Œdriveræ‰å°†tasksetä»¥stageçš„å½¢å¼æäº¤ä»»åŠ¡ç»™executorï¼Œä»»åŠ¡çš„æœ€ç»†ç²’åº¦æ˜¯task ç„¶åexecutorå°±è·Ÿdriverè¿›è¡Œé€šä¿¡ï¼ŒRddæ‰§è¡Œé€»è¾‘çš„æ—¶å€™å°±ä¸å†é€šè¿‡Masterï¼Œè¿™æ˜¯ä¸ºäº†å‡å°‘Masterå‹åŠ›ï¼Œä¹Ÿä¸è®©Masteræˆä¸ºæ€§èƒ½ç“¶é¢ˆã€‚ RDDçš„æ„å»ºè¯¦ç»†é˜¶æ®µ ç¬¬ä¸€é˜¶æ®µ rddåˆ›å»ºrddåˆ›å»ºéƒ½æ˜¯åœ¨driverä¸­æ‰§è¡Œï¼Œåªæœ‰è¿è¡Œæ—¶æœ‰æ•°æ®æµå‘rddæ‰ä¼šæäº¤åˆ°executorã€‚å¦‚hdfsä¸­è¯»å–æ•°æ®ï¼Œä¼šäº§ç”Ÿå¾ˆå¤šçš„RDDâ€¦.ï¼ŒRDDä¹‹é—´å­˜åœ¨ç€ä¾èµ–å…³ç³»ï¼Œ RDDä¹‹é—´çš„è½¬æ¢éƒ½æ˜¯ç”±transformationäº§ç”Ÿçš„ï¼ˆæ•…transformationè¿”å›å¯¹è±¡ä¸ºRDDï¼‰ï¼Œåœ¨actionè§¦å‘åï¼ŒDAGå°±ç¡®å®šäº†RDDå°±å½¢æˆäº†æ•°æ®çš„æµå‘hdfs-&gt;RDD1-&gt;RDD2-&gt;RDD3â€¦.-&gt;RDDn ç¬¬äºŒé˜¶æ®µ DAGSchedulerstageçš„åˆ’åˆ†ï¼Œä¹Ÿå°±æ˜¯DAGSchedulerçš„æ‰§è¡Œ stageåˆ‡åˆ†ä¾æ®ï¼šæœ‰å®½ä¾èµ–ï¼ˆåé¢å¯¹å®½çª„ä¾èµ–æœ‰è¯´æ˜ï¼‰å°±ä¼šåˆ‡åˆ†ï¼Œæ›´æ˜ç¡®çš„è¯´å°±æ˜¯æœ‰shuffleçš„æ—¶å€™åˆ‡åˆ† å®½ä¾èµ–ï¼šé‡åˆ°shuffleå°±æ˜¯å®½ä¾èµ–çª„ä¾èµ–ï¼šæ²¡æœ‰shuffleå°±æ˜¯çª„ä¾èµ– æŠŠDAGåˆ‡åˆ†æˆstageï¼Œç„¶åä»¥taskSet(æµæ°´çº¿taskçš„é›†åˆï¼Œæ‰€æœ‰çš„taskä¸šåŠ¡é€»è¾‘éƒ½æ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯è®¡ç®—çš„æ•°æ®å—ä¸åŒï¼Œå› ä¸ºæ¯ä¸€ä¸ªtaskåªè®¡ç®—ä¸€ä¸ªåˆ†åŒºï¼Œä¸”taskSeté›†åˆä¸­çš„ä»»åŠ¡ä¼šå¹¶è¡Œæ“ä½œï¼‰ taskç”±masterå†³å®šåœ¨å“ªä¸ªexecutorä¸Šè¿è¡Œï¼Œä¹‹åç”±driveræäº¤taskåˆ°executoræ¥æ‰§è¡Œ å‰é¢çš„stageå…ˆæäº¤ï¼Œå› ä¸ºstageçš„åˆ’åˆ†æ„å‘³ç€æœ‰shuffleï¼Œé‚£ä¹ˆåé¢stageä¼šä¾èµ–å‰é¢stageçš„æ•°æ®ï¼Œæ•…æ­¤å‰é¢stageå…ˆæäº¤ã€‚ Actionè§¦å‘çš„æ—¶å€™ï¼ŒDAGå°±å¯ä»¥ç¡®å®šäº†ï¼Œtransformationçš„è°ƒç”¨éƒ½æ˜¯åœ¨driverä¸­å®Œæˆçš„ï¼Œä¸€æ—¦è°ƒç”¨äº†actionï¼Œä¼šæäº¤è¿™ä¸ªä»»åŠ¡ã€‚ ç¬¬ä¸‰é˜¶æ®µ TaskSchedulerTaskScheduler cluster Manager ï¼šå°±æ˜¯Masterï¼Œå¯åŠ¨workä¸Šçš„executorstragling tasksï¼šä¾‹å­ï¼šå½“100ä¸ªä»»åŠ¡ï¼Œ99éƒ½å®Œæˆäº†ï¼Œå‰©ä¸‹äº†ä¸€ä¸ªä»»åŠ¡1ï¼Œä¼šå†å¯åŠ¨å’Œå‰©ä¸‹çš„ä¸€æ¨¡ä¸€æ ·çš„ä»»åŠ¡2ï¼Œä»»åŠ¡1å’Œä»»åŠ¡2è°å…ˆå®Œæˆï¼Œå°±ç”¨è°çš„ç»“æœ ç¬¬å››é˜¶æ®µ ä»»åŠ¡æ‰§è¡ŒBlock Managerç®¡ç†æ•°æ® ä¹Ÿå°±æ˜¯actionè§¦å‘ï¼Œæ•°æ®å¼€å§‹æµé€šï¼Œdriveræäº¤taskåˆ°workerä¸Šçš„executorï¼Œæ‰§è¡ŒçœŸæ­£çš„ä¸šåŠ¡é€»è¾‘ è®¡ç®—å®Œæˆä¹‹åï¼Œè‹¥å°†è¿™äº›ç»“æœæ•°æ®collectèšåˆï¼Œåˆ™ä¼šå°†æ‰€æœ‰executorçš„éƒ¨åˆ†ç»“æœèšåˆåœ¨ä¸€èµ·ï¼Œæ¯”å¦‚countã€sumçš„ï¼Œå¤šä¸ªworkerä¸­æ¯ä¸€ä¸ªworkeråªä¼šä¿å­˜ä¸€éƒ¨åˆ†æ•°æ®ï¼Œdriverä¿å­˜äº†æ‰€æœ‰æ•°æ®ï¼Œã€éƒ½æ˜¯åœ¨driveré‡Œé¢ taskä¸è¯»å–hdfsæ•°æ®çš„å…³ç³»taskæäº¤ä¹‹åï¼Œå¦‚æœå‘hdfsä¸­æ‹‰å–æ•°æ®ï¼Œdriverå·²ç»å°†è¦è·å–æ•°æ®çš„å…ƒæ•°æ®éƒ½å·²ç»è·å–åˆ°äº†ï¼Œtaskä¼šå’Œåˆ†åŒºæ•°æŒ‚é’©ï¼Œåœ¨executorä¸Šä¸€ä¸ªtaskè¯»å–ä¸€ä¸ªåˆ†åŒºã€‚taskè¯»å–æ•°æ®ä¸æ˜¯ä¸€ä¸‹å°†æ‰€æœ‰æ•°æ®åŠ è½½åˆ°å†…å­˜é‡Œé¢ï¼Œæ˜¯å…ˆæ„å»ºä¸€ä¸ªè¿­ä»£å™¨ï¼Œæ‹¿åˆ°ä¸€æ¡å¤„ç†ä¸€æ¡ã€‚å¤„ç†å®Œæˆä¹‹åä¼šå°†æ•°æ®ä¿å­˜åœ¨executorçš„å†…å­˜ä¸­ï¼Œå¦‚æœå†…å­˜æ”¾ä¸ä¸‹ï¼Œå°±å°†æ•°æ®æŒä¹…åŒ–åˆ°ç£ç›˜ä¸Š","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"åŸç†","slug":"åŸç†","permalink":"http://gangtieguo.cn/tags/åŸç†/"},{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"}]},{"title":"SparkStreamingæ¶ˆè´¹Kafkaæ•°æ®","slug":"SparkStreamingæ¶ˆè´¹Kafkaæ•°æ®","date":"2018-08-15T15:53:30.330Z","updated":"2018-08-20T01:39:18.490Z","comments":true,"path":"2018/08/15/SparkStreamingæ¶ˆè´¹Kafkaæ•°æ®/","link":"","permalink":"http://gangtieguo.cn/2018/08/15/SparkStreamingæ¶ˆè´¹Kafkaæ•°æ®/","excerpt":"[TOC] Streamingæ¶ˆè´¹Kafkaæœ‰ä¸¤ç§æ–¹å¼ 1ã€reciveræ–¹å¼æ ¹æ®æ—¶é—´æ¥åˆ’åˆ†æ‰¹æ¬¡ï¼Œç¼ºç‚¹ï¼šæœ‰å¯èƒ½ä¸€ä¸ªæ—¶é—´æ®µä¼šå‡ºç°æ•°æ®çˆ†ç‚¸ï¼Œæœ‰ä¿å­˜logåˆ°hdfsæœºåˆ¶ï¼Œä½†æ¶ˆè€—å¤§ï¼ˆzkæ¥ç®¡ç†åç§»é‡ï¼‰ 2ã€directæ–¹å¼ 1.3.6åæ¨å‡ºexecutorå’Œkafkaçš„partitionæ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼ˆæ˜¯rddçš„åˆ†åŒºå’Œkafkaå¯¹åº”ï¼Œå¦‚æœä¸€ä¸ªexecutorçš„rddæœ‰å¤šä¸ªåˆ†åŒºï¼Œé‚£ä¹ˆä¸€ä¸ªexecutorå¯ä»¥å¯¹åº”å¤šä¸ªpartitionï¼‰å¿…é¡»è‡ªå·±æ¥ç®¡ç†åç§»é‡ï¼Œæœ€å¥½æŠŠåç§»é‡å†™åœ¨zkæˆ–è€…å…¶ä»–ç¬¬ä¸‰æ–¹ä»‹è´¨é‡Œé¢","text":"[TOC] Streamingæ¶ˆè´¹Kafkaæœ‰ä¸¤ç§æ–¹å¼ 1ã€reciveræ–¹å¼æ ¹æ®æ—¶é—´æ¥åˆ’åˆ†æ‰¹æ¬¡ï¼Œç¼ºç‚¹ï¼šæœ‰å¯èƒ½ä¸€ä¸ªæ—¶é—´æ®µä¼šå‡ºç°æ•°æ®çˆ†ç‚¸ï¼Œæœ‰ä¿å­˜logåˆ°hdfsæœºåˆ¶ï¼Œä½†æ¶ˆè€—å¤§ï¼ˆzkæ¥ç®¡ç†åç§»é‡ï¼‰ 2ã€directæ–¹å¼ 1.3.6åæ¨å‡ºexecutorå’Œkafkaçš„partitionæ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼ˆæ˜¯rddçš„åˆ†åŒºå’Œkafkaå¯¹åº”ï¼Œå¦‚æœä¸€ä¸ªexecutorçš„rddæœ‰å¤šä¸ªåˆ†åŒºï¼Œé‚£ä¹ˆä¸€ä¸ªexecutorå¯ä»¥å¯¹åº”å¤šä¸ªpartitionï¼‰å¿…é¡»è‡ªå·±æ¥ç®¡ç†åç§»é‡ï¼Œæœ€å¥½æŠŠåç§»é‡å†™åœ¨zkæˆ–è€…å…¶ä»–ç¬¬ä¸‰æ–¹ä»‹è´¨é‡Œé¢ éç›´è¿æ–¹å¼ receiveæ–¹å¼spark clusterå…ˆä¸kafkaé›†ç¾¤å»ºç«‹è¿æ¥åœ¨æ¯ä¸€ä¸ªexecutorä¸­åˆ›å»ºä¸€ä¸ªReciverå½“executorå¯åŠ¨kafkaå¯åŠ¨åï¼Œreciverä¼šä¸€ç›´æ¶ˆè´¹kafkaä¸­çš„æ•°æ®ï¼Œä¸€ç›´æ‹‰ï¼Œç›´è¿æ˜¯æ¯éš”ä¸€ä¸ªæ—¶é—´æ®µå»æ‹‰å–è¿™ç§æ–¹å¼æ˜¯zookeeperæ¥ç®¡ç†æ•°æ®çš„åç§»é‡é—®é¢˜:åœ¨æ—¶é—´é—´éš”ä¸­ï¼Œexecutoræ¥æ”¶çš„æ•°æ®è¶…è¿‡äº†Executorçš„å†…å­˜æ•°ï¼Œä¼šé€ æˆæ•°æ®çš„ä¸¢å¤±ä¸ºäº†é˜²æ­¢æ•°æ®ä¸¢å¤±ï¼Œå¯ä»¥åšcheckpointï¼Œæˆ–è€…æ˜¯è®°å½•æ—¥å¿—å¯ä»¥å¤šä¸ªreciverï¼Œä¸€ä¸ªreciverå¯ä»¥æŒ‡å®šå¤šä¸ªçº¿ç¨‹å¯ä»¥è¯»è¯»æ–‡ç«  Kafka Intergration Guide å®˜ç½‘é“¾æ¥ ç›´è¿Direct Approach (No Receives)B-*ä»£è¡¨brokerï¼Œæ¯ä¸€ä¸ªbrokerä¸­æœ‰ä¸‰ä¸ªåˆ†åŒºï¼ˆå‡è®¾ï¼‰ï¼Œä½†æ˜¯æ¯ä¸ªbrokeré‡Œé¢åªæœ‰ä¸€ä¸ªåˆ†åŒºæ˜¯æ´»ç€çš„ç›´è¿æ˜¯æ¯éš”ä¸€ä¸ªæ—¶é—´æ®µå»æ‹‰å–å¯¹æ¶ˆè´¹æ•°æ®çš„ä½ç½®ä¿è¯ä¼šå®šæœŸå®æ—¶æŸ¥è¯¢kafka topic+partitionçš„åç§»é‡ï¼Œä¼šæ ¹æ®åç§»é‡çš„èŒƒå›´æ¥å¤„ç†æ¯ä¸€ä¸ªæ‰¹æ¬¡ï¼Œ executorä¸ä¼šæ¥æ”¶è¶…å‡ºæ¥æ”¶èŒƒå›´çš„æ•°æ®ï¼Œè€Œæ˜¯è®°å½•ä¸‹åç§»é‡ï¼Œä¸‹æ¬¡æ¥ç€æ‹‰å–ä¸€ä¸ªkafkaçš„partitonå¯¹åº”rddçš„ä¸€ä¸ªpartition é—®é¢˜ï¼šä¸€ä¸ªpartitionåªæœ‰ä¸€ä¸ªExecutorè¿æ¥ä¸Šï¼Œä¸èƒ½å¹¶è¡Œè¯»å»æ•°æ®ã€‚è§£å†³åŠæ³•ï¼Œå¯ä»¥repartitionï¼Œåˆ†æ•£åˆ°å¤šä¸ªpartitionä¸Šå»è¯»å–ã€‚æ€æ ·è®©executorè¯»å–kafkaåˆ†åŒºé‡Œé¢çš„æ•°æ®çš„é€Ÿåº¦å¿«ï¼Ÿå°†bokerçš„åˆ†åŒºæ•°åˆ›å»ºæˆå’Œworkerçš„æ•°ç›®ä¸€æ ·ï¼Œä¹Ÿå°±æ˜¯executorçš„æ•°ç›®ä¸€æ ·ï¼Œä¸€ä¸ªexecutoræ¶ˆè´¹ä¸€ä¸ªåˆ†åŒºï¼Œè¿™æ ·æ•°æ®è¯»å–æ¯”è¾ƒå¿«ã€‚å¹¶è¡Œè¯»å–æ•°æ®ï¼Œä¹Ÿå¯ä»¥æ§åˆ¶è¯»å–çš„é€Ÿåº¦ã€‚è¿™æ ·éœ€è¦è‡ªå·±ç®¡ç†åç§»é‡ï¼Œä»¥å‰çš„æ–¹å¼æ˜¯zkç®¡ç†åç§»é‡æœ€å¥½æ˜¯å°†åç§»é‡ä¿å­˜åˆ°zké‡Œé¢ï¼Œä¸è¿‡æ˜¯è‡ªå·±æ§åˆ¶çš„ï¼Œé˜²æ­¢å°†åç§»é‡ä¿å­˜åˆ°æœ¬åœ°å®•æœºæ— æ³•æ¢å¤ã€‚éœ€è¦è¯»åšå®¢ Spark streamingæ•´åˆkafkaå®˜æ–¹æ–‡æ¡£æ˜¾ç¤ºï¼Œä¸€ä¸ªæ•°æ®ä¿è¯åªä¼šæ¶ˆè´¹ä¸€æ¬¡ï¼Œä¸ä¼šé‡å¤æ¶ˆè´¹ï¼Œæ›´åŠ é«˜æ•ˆç®€åŒ–å¹¶è¡Œï¼Œä¸€å¯¹ä¸€æ¶ˆè´¹ï¼Œé«˜æ•ˆï¼Œæ²¡æœ‰reciverä½œä¸ºæ¶ˆè´¹è€…","categories":[],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"},{"name":"SparkStreaming","slug":"SparkStreaming","permalink":"http://gangtieguo.cn/tags/SparkStreaming/"}]},{"title":"Sparkç®—å­æ¡ˆä¾‹","slug":"Sparkç®—å­","date":"2018-08-15T15:52:40.133Z","updated":"2019-06-17T04:40:09.401Z","comments":true,"path":"2018/08/15/Sparkç®—å­/","link":"","permalink":"http://gangtieguo.cn/2018/08/15/Sparkç®—å­/","excerpt":"[TOC] HelloWordï¼ŸWorldCount1sc.textfile(\"hdfs://master:9000/wc\").flatMap(_.split(\"åˆ†éš”ç¬¦\")).map((_,1)).reduceByKey(_+_).saveAsTextFile(\"hdfs://master:9000/wcResult\") æ•°æ®æœ€å¼€å§‹åœ¨Driverï¼Œè®¡ç®—çš„æ—¶å€™æ•°æ®ä¼šæµå…¥workerå½“rddå½¢æˆè¿‡ç¨‹ä¸­ï¼Œworkerçš„åˆ†åŒºä¸­åªæ˜¯é¢„ç•™äº†å­˜æ”¾æ•°æ®çš„ä½ç½®ï¼Œåªæœ‰å½“actionè§¦å‘çš„æ—¶å€™ï¼Œworkerçš„åˆ†åŒºä¸­æ‰ä¼šå­˜åœ¨æ•°æ® Sparkçš„è¿ç®—éƒ½æ˜¯é€šè¿‡ç®—å­è¿›è¡ŒRDDçš„è½¬æ¢åŠè¿ç®—ï¼Œé‚£æˆ‘ä»¬å¯¹ç®—å­è¿›è¡Œç®€å•ç†Ÿæ‚‰å‚è€ƒRDDç®—å­å®ä¾‹","text":"[TOC] HelloWordï¼ŸWorldCount1sc.textfile(\"hdfs://master:9000/wc\").flatMap(_.split(\"åˆ†éš”ç¬¦\")).map((_,1)).reduceByKey(_+_).saveAsTextFile(\"hdfs://master:9000/wcResult\") æ•°æ®æœ€å¼€å§‹åœ¨Driverï¼Œè®¡ç®—çš„æ—¶å€™æ•°æ®ä¼šæµå…¥workerå½“rddå½¢æˆè¿‡ç¨‹ä¸­ï¼Œworkerçš„åˆ†åŒºä¸­åªæ˜¯é¢„ç•™äº†å­˜æ”¾æ•°æ®çš„ä½ç½®ï¼Œåªæœ‰å½“actionè§¦å‘çš„æ—¶å€™ï¼Œworkerçš„åˆ†åŒºä¸­æ‰ä¼šå­˜åœ¨æ•°æ® Sparkçš„è¿ç®—éƒ½æ˜¯é€šè¿‡ç®—å­è¿›è¡ŒRDDçš„è½¬æ¢åŠè¿ç®—ï¼Œé‚£æˆ‘ä»¬å¯¹ç®—å­è¿›è¡Œç®€å•ç†Ÿæ‚‰å‚è€ƒRDDç®—å­å®ä¾‹ reduceByKeyå…ˆè¿›è¡Œä¸€ä¸‹combineer ç§»åŠ¨è®¡ç®— groupByKeyä¸å¥½ reduceByKeyä¼šåœ¨å±€éƒ¨å…ˆè¿›è¡Œä¸€ä¸‹æ±‚å’Œ groupByKeyæ˜¯ä¼šå°†æ‰€æœ‰çš„æ•°æ®æ”¾åœ¨ä¸€ä¸ªå¤§é›†åˆé‡Œé¢ï¼Œç„¶åå†æ±‚å’Œ ï¼Œä¼šæ¶ˆè€—æ›´å¤šçš„ç½‘ç»œå¸¦å®½ï¼Œä¸ç¬¦åˆè®¡ç®—æœ¬åœ°åŒ– ä¸€ä¸‹ä¸€äº›RDDæ˜¯ç»™äºˆrdd1æ¥æ“ä½œçš„ 1val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2) mapPartitionsmap æ˜¯å¯¹ rdd ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ è¿›è¡Œæ“ä½œï¼Œè€Œ mapPartitions(foreachPartition) åˆ™æ˜¯å¯¹ rdd ä¸­çš„æ¯ä¸ªåˆ†åŒºçš„è¿­ä»£å™¨è¿›è¡Œæ“ä½œã€‚å¦‚æœåœ¨ map è¿‡ç¨‹ä¸­éœ€è¦é¢‘ç¹åˆ›å»ºé¢å¤–çš„å¯¹è±¡ (ä¾‹å¦‚å°† rdd ä¸­çš„æ•°æ®é€šè¿‡ jdbc å†™å…¥æ•°æ®åº“, map éœ€è¦ä¸ºæ¯ä¸ªå…ƒç´ åˆ›å»ºä¸€ä¸ªé“¾æ¥è€Œ mapPartition ä¸ºæ¯ä¸ª partition åˆ›å»ºä¸€ä¸ªé“¾æ¥), åˆ™ mapPartitions æ•ˆç‡æ¯” map é«˜çš„å¤šã€‚ SparkSql æˆ– DataFrame é»˜è®¤ä¼šå¯¹ç¨‹åºè¿›è¡Œ mapPartition çš„ä¼˜åŒ–ã€‚ mapPartitionsWithIndexmapPartitionWithIndexä¸mapPartitionç±»ä¼¼ï¼Œåªæ˜¯ä¼šå¸¦ä¸Šåˆ†åŒºçš„åºå· æŠŠæ¯ä¸ªpartitionä¸­çš„åˆ†åŒºå·å’Œå¯¹åº”çš„å€¼æ‹¿å‡ºæ¥, æºç ä¸­æ–¹æ³•çš„å½¢å¼ï¼š 123val func(index,Int,iter:Interator[(Int)]):Interator[String] = &#123;iter.toList.map(x =&gt; \"[partID:\" + index + \", val: \" + x + \"]\").iterator&#125; ä¼šè½¬æ¢æˆå‡½æ•°å‡½æ•°çš„å½¢å¼ 1234val func = (index: Int, iter: Iterator[(Int)]) =&gt; &#123; iter.toList.map(x =&gt; \"[partID:\" + index + \", val: \" + x + \"]\").iterator&#125;rdd1.mapPartitionsWithIndex(func).collect aggregate (action)aggregateæ˜¯ä¸€ä¸ªactionæ“ä½œ æºç å®šä¹‰ 1def aggregate[U](zeroValue: U)(seqOp: (U, T) â‡’ U, combOp: (U, U) â‡’ U)(implicit arg0: ClassTag[U]): U eqOp æ“ä½œä¼šèšåˆå„åˆ†åŒºä¸­çš„å…ƒç´ ï¼Œç„¶å combOp æ“ä½œæŠŠæ‰€æœ‰åˆ†åŒºçš„èšåˆç»“æœå†æ¬¡èšåˆï¼Œä¸¤ä¸ªæ“ä½œçš„åˆå§‹å€¼éƒ½æ˜¯ zeroValue. seqOp çš„æ“ä½œæ˜¯éå†åˆ†åŒºä¸­çš„æ‰€æœ‰å…ƒç´  (T)ï¼Œç¬¬ä¸€ä¸ª T è·Ÿ zeroValue åšæ“ä½œï¼Œç»“æœå†ä½œä¸ºä¸ç¬¬äºŒä¸ª T åšæ“ä½œçš„ zeroValueï¼Œç›´åˆ°éå†å®Œæ•´ä¸ªåˆ†åŒºã€‚combOp æ“ä½œæ˜¯æŠŠå„åˆ†åŒºèšåˆçš„ç»“æœï¼Œå†èšåˆã€‚aggregate å‡½æ•°è¿”å›ä¸€ä¸ªè·Ÿ RDD ä¸åŒç±»å‹çš„å€¼ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ä¸ªæ“ä½œ seqOp æ¥æŠŠåˆ†åŒºä¸­çš„å…ƒç´  T åˆå¹¶æˆä¸€ä¸ª Uï¼Œå¦å¤–ä¸€ä¸ªæ“ä½œ combOp æŠŠæ‰€æœ‰ U èšåˆã€‚ å‚è€ƒç†è§£ Spark RDD ä¸­çš„ aggregate å‡½æ•° ç¬¬ä¸€ä¸ªå‚æ•°ï¼šåˆå§‹å€¼ï¼ˆåœ¨è¿›è¡Œæ“ä½œçš„æ—¶å€™ï¼Œä¼šé»˜è®¤å¸¦å…¥è¯¥å€¼è¿›è¡Œï¼‰ç¬¬äºŒä¸ªå‚æ•°: æ˜¯ä¸¤ä¸ªå‡½æ•°[æ¯ä¸ªå‡½æ•°éƒ½æ˜¯2ä¸ªå‚æ•°(ç¬¬ä¸€ä¸ªå‡½æ•°:å…ˆå¯¹å„ä¸ªåˆ†åŒºè¿›è¡Œåˆå¹¶, ç¬¬äºŒä¸ªå‡½æ•°:å¯¹å„ä¸ªåˆ†åŒºåˆå¹¶åçš„ç»“æœå†è¿›è¡Œåˆå¹¶)] æœ€åå¾—åˆ°è¿”å›å€¼ rdd1ä¸ºä¸Šé¢çš„rdd1åˆ†åŒºå‡½æ•°çš„ç»“æœ 1rdd1.aggregate(0)(_+_, _+_) 0 + (0+1+2+3+4 + 0+5+6+7+8+9) 1rdd1.aggregate(7)(_+_, _+_) 7 + (7+1+2+3+4 + 7+5+6+7+8+9) 1rdd1.aggregate(0)(math.max(_, _), _ + _) 1rdd1.aggregate(5)(math.max(_, _), _ + _) 5å’Œ1æ¯”, å¾—5å†å’Œ234æ¯”å¾—5 â€“&gt; 5å’Œ6789æ¯”,å¾—9 â€“&gt; 5 + (5+9) 1val rdd2 = sc.parallelize(List(\"q\",\"w\",\"e\",\"r\",\"t\",\"y\",\"u\",\"i\",\"o\",\"p\"),2) å¯ä»¥ç”¨æ›´åŠ ç›´æ¥çš„æ–¹å¼éªŒè¯æ“ä½œ 123def func2(index: Int, iter: Iterator[(String)]) : Iterator[String] = &#123; iter.toList.map(x =&gt; \"[partID:\" + index + \", val: \" + x + \"]\").iterator&#125; 123rdd2.aggregate(\"\")(_ + _, _ + _)rdd2.aggregate(\"=\")(_ + _, _ + _)rdd2.aggregate(\"|\")(_ + _, _ + _) 12val rdd3 = sc.parallelize(List(\"qazqqw7\",\"jishhrwe9\",\"sdfwezsddf12\",\"12esdww8\"),2)rdd3.aggregate(\"\")((x,y) =&gt; math.max(x.length, y.length).toString, (x,y) =&gt; x + y) 12val rdd4 = sc.parallelize(List(\"qazqqw7\",\"jishhrwe9\",\"sdfwezsddf12\",\"\"),2)rdd4.aggregate(\"\")((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y) aggregateByKeyå¯¹æ¯ä¸ªåˆ†åŒºè¿›è¡Œè®¡ç®— 12345val pairRDD = sc.parallelize(List( (\"a\",1), (\"a\", 12), (\"b\", 4),(\"c\", 17), (\"c\", 12), (\"b\", 2)), 2)def func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = &#123; iter.toList.map(x =&gt; \"[partID:\" + index + \", val: \" + x + \"]\").iterator&#125;pairRDD.mapPartitionsWithIndex(func2).collect combineByKeyreduceByKey aggregateByKeyåº•å±‚éƒ½æ˜¯ä¾èµ–çš„combineByKeyï¼ŒcombineByKeyæ¯”è¾ƒåº•å±‚çš„ç®—å­å’ŒreduceByKeyæ˜¯ç›¸åŒçš„æ•ˆæœ combineByKeyæœ‰ä¸‰ä¸ªå‚æ•° ç¬¬ä¸€ä¸ªå‚æ•°x: åŸå°ä¸åŠ¨å–å‡ºæ¥ ç¬¬äºŒä¸ªå‚æ•°:æ˜¯å‡½æ•°, å±€éƒ¨è¿ç®—, ç¬¬ä¸‰ä¸ª:æ˜¯å‡½æ•°, å¯¹å±€éƒ¨è¿ç®—åçš„ç»“æœå†åšè¿ç®— 12345val rdd4 = sc.parallelize(List(\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\"),2)val rdd5 = sc.parallelize(List(1,1,2,2,2,1,2,2,2),2)val rdd6 = rdd5.zip(rdd4)val rdd7 = rdd6.combineByKey(List(_),(x:List[String],y:String)=&gt;x:+y,(m:List[String],n:List[String])=&gt;m ++ n)rdd7.collect reduceByKeyreduceByKey ç”¨äºå¯¹æ¯ä¸ª key å¯¹åº”çš„å¤šä¸ª value è¿›è¡Œ merge æ“ä½œï¼Œæœ€é‡è¦çš„æ˜¯å®ƒèƒ½å¤Ÿåœ¨æœ¬åœ°å…ˆè¿›è¡Œ merge æ“ä½œï¼Œå¹¶ä¸” merge æ“ä½œå¯ä»¥é€šè¿‡å‡½æ•°è‡ªå®šä¹‰ã€‚ groupByKeygroupByKey ä¹Ÿæ˜¯å¯¹æ¯ä¸ª key è¿›è¡Œæ“ä½œï¼Œä½†åªç”Ÿæˆä¸€ä¸ª sequenceã€‚ä¸ä¼šå†è¿›è¡Œ éœ€è¦ç‰¹åˆ«æ³¨æ„ â€œNoteâ€ ä¸­çš„è¯ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬ï¼šå¦‚æœéœ€è¦å¯¹ sequence è¿›è¡Œ aggregation æ“ä½œï¼ˆæ³¨æ„ï¼ŒgroupByKey æœ¬èº«ä¸èƒ½è‡ªå®šä¹‰æ“ä½œå‡½æ•°ï¼‰ï¼Œé‚£ä¹ˆï¼Œé€‰æ‹© reduceByKey/aggregateByKey æ›´å¥½ã€‚è¿™æ˜¯å› ä¸º groupByKey ä¸èƒ½è‡ªå®šä¹‰å‡½æ•°ï¼Œæˆ‘ä»¬éœ€è¦å…ˆç”¨ groupByKey ç”Ÿæˆ RDDï¼Œç„¶åæ‰èƒ½å¯¹æ­¤ RDD é€šè¿‡ map è¿›è¡Œè‡ªå®šä¹‰å‡½æ•°æ“ä½œã€‚ checkpointå°†rddå†…å®¹æŒä¹…åŒ– 1234567sc.setCheckpointDir(\"hdfs://master:9000/ck\")val rdd = sc.textFile(\"hdfs://master:9000/wc\").flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_+_)rdd.checkpointrdd.isCheckpointedrdd.countrdd.isCheckpointedrdd.getCheckpointFile coalesce, repartitionæœ‰æ—¶å€™éœ€è¦é‡æ–°è®¾ç½® Rdd çš„åˆ†åŒºæ•°é‡ï¼Œæ¯”å¦‚ Rdd çš„åˆ†åŒºä¸­ï¼ŒRdd åˆ†åŒºæ¯”è¾ƒå¤šï¼Œä½†æ˜¯æ¯ä¸ª Rdd çš„æ•°æ®é‡æ¯”è¾ƒå°ï¼Œéœ€è¦è®¾ç½®ä¸€ä¸ªæ¯”è¾ƒåˆç†çš„åˆ†åŒºã€‚æˆ–è€…éœ€è¦æŠŠ Rdd çš„åˆ†åŒºæ•°é‡è°ƒå¤§ã€‚è¿˜æœ‰å°±æ˜¯é€šè¿‡è®¾ç½®ä¸€ä¸ª Rdd çš„åˆ†åŒºæ¥è¾¾åˆ°è®¾ç½®ç”Ÿæˆçš„æ–‡ä»¶çš„æ•°é‡ã€‚ å¦‚æœåˆ†åŒºçš„æ•°é‡å‘ç”Ÿæ¿€çƒˆçš„å˜åŒ–ï¼Œå¦‚è®¾ç½® numPartitions = 1ï¼Œè¿™å¯èƒ½ä¼šé€ æˆè¿è¡Œè®¡ç®—çš„èŠ‚ç‚¹æ¯”ä½ æƒ³è±¡çš„è¦å°‘ï¼Œä¸ºäº†é¿å…è¿™ä¸ªæƒ…å†µï¼Œå¯ä»¥è®¾ç½® shuffle=trueï¼Œ é‚£ä¹ˆè¿™ä¼šå¢åŠ  shuffle æ“ä½œã€‚ å…³äºè¿™ä¸ªåˆ†åŒºçš„æ¿€çƒˆçš„å˜åŒ–æƒ…å†µï¼Œæ¯”å¦‚åˆ†åŒºæ•°é‡ä»çˆ¶ Rdd çš„å‡ åƒä¸ªåˆ†åŒºè®¾ç½®æˆå‡ ä¸ªï¼Œæœ‰å¯èƒ½ä¼šé‡åˆ°è¿™ä¹ˆä¸€ä¸ªé”™è¯¯ã€‚ 1java.io.IOException: Unable to acquire 16777216 bytes of memory è¿™ä¸ªé”™è¯¯åªè¦æŠŠ shuffle è®¾ç½®æˆ true å³å¯è§£å†³ã€‚ å½“æŠŠçˆ¶ Rdd çš„åˆ†åŒºæ•°é‡å¢å¤§æ—¶ï¼Œæ¯”å¦‚ Rdd çš„åˆ†åŒºæ˜¯ 100ï¼Œè®¾ç½®æˆ 1000ï¼Œå¦‚æœ shuffle ä¸º falseï¼Œå¹¶ä¸ä¼šèµ·ä½œç”¨ã€‚ è¿™æ—¶å€™å°±éœ€è¦è®¾ç½® shuffle ä¸º true äº†ï¼Œé‚£ä¹ˆ Rdd å°†åœ¨ shuffle ä¹‹åè¿”å›ä¸€ä¸ª 1000 ä¸ªåˆ†åŒºçš„ Rddï¼Œæ•°æ®åˆ†åŒºæ–¹å¼é»˜è®¤æ˜¯é‡‡ç”¨ hash partitionerã€‚ æœ€åæ¥çœ‹çœ‹ repartition() æ–¹æ³•çš„æºç ï¼š coalesce() æ–¹æ³•çš„ä½œç”¨æ˜¯è¿”å›æŒ‡å®šä¸€ä¸ªæ–°çš„æŒ‡å®šåˆ†åŒºçš„ Rddã€‚ 123val rdd1 = sc.parallelize(1 to 10, 10)val rdd2 = rdd1.coalesce(2, false)rdd2.partitions.length collectAsMapå°†å…¶ä»–é›†åˆä¿å­˜ä¸ºmapç»“æ„ 1234val rdd = sc.parallelize(List((\"a\", 1), (\"b\", 2)))rdd.collectAsMapå¾—åˆ°ç»“æœMap(b -&gt; 2, a -&gt; 1) countByKey1234val rdd1 = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"b\", 2), (\"c\", 2), (\"c\", 1)))rdd1.countByKey ç»Ÿè®¡Keyå‡ºç°çš„æ¬¡æ•°ç»“æœ Map(b -&gt; 2, a -&gt; 1, c -&gt; 2) countByValue1234val rdd1 = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"b\", 2), (\"c\", 2), (\"c\", 1)))rdd1.countByValueç»“æœ (å°†æ•´ä¸ªå…ƒç»„ä½œä¸ºkey)Map((b,2) -&gt; 2, (c,2) -&gt; 1, (a,1) -&gt; 1, (c,1) -&gt; 1) filterByRange12345val rdd1 = sc.parallelize(List((\"a\", 5), (\"b\", 3), (\"c\", 4), (\"d\", 2), (\"e\", 1)))val rdd2 = rdd1.filterByRange(\"b\", \"d\")rdd2.collectArray[(String, Int)] = Array((b,3), (c,4), (d,2)) flatMapValueså‹å¹³ 12345val rdd3 = sc.parallelize(List((\"a\", \"1 2\"), (\"b\", \"3 4\")))val rdd4 = rdd3.flatMapValues(_.split(\" \"))rdd4.collectArray[(String, String)] = Array((a,1), (a,2), (b,3), (b,4)) foldByKey12345678910111213val rdd1 = sc.parallelize(List(\"a22\", \"b232\", \"c\", \"d\"), 2)val rdd2 = rdd1.map(x =&gt; (x.length, x))rdd2.collectç»“æœï¼š Array[(Int, String)] = Array((3,a22), (4,b232), (1,c), (1,d))val rdd3 = rdd2.foldByKey(\"\")(_+_)rdd3.collectç»“æœï¼šå°†ç›¸åŒkeyçš„å…ƒç»„åˆå¹¶åœ¨ä¸€èµ·ï¼ŒArray[(Int, String)] = Array((4,b232), (1,cd), (3,a22)) foreachforeachæ˜¯é’ˆå¯¹äºæ¯ä¸€ä¸ªå…ƒç´ ï¼ŒforeachPartitionæ˜¯é’ˆå¯¹æ¯ä¸€ä¸ªåˆ†åŒºï¼ŒforeachPartitionæ˜¯å†™å…¥æ•°æ®åº“æ—¶ï¼Œå¯ä»¥å°†åœ¨foreachPartitionæ—¶è·å¾—ä¸€ä¸ªæ•°æ®åº“è¿æ¥ï¼Œé€šè¿‡mapæ–¹æ³•æ¥å°†æ¯ä¸ªåˆ†åŒºçš„å…¨éƒ¨å…ƒç´ å†™å…¥åˆ°æ•°æ®åº“ foreachPartition3ä¸ªåˆ†åŒº 12val rdd1 = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)rdd1.foreachPartition(x =&gt; println(x.reduce(_ + _))) keyByä»¥ä¼ å…¥çš„å‚æ•°åškey 1234val rdd1 = sc.parallelize(List(\"dog\", \"salmon\", \"salmon\", \"rat\", \"elephant\"), 3)val rdd2 = rdd1.keyBy(_.length)rdd2.collectç»“æœ Array((3,dog), (6,salmon), (6,salmon), (3,rat), (8,elephant)) keys values1234val rdd1 = sc.parallelize(List(\"dog\", \"tiger\", \"lion\", \"cat\", \"panther\", \"eagle\"), 2)val rdd2 = rdd1.map(x =&gt; (x.length, x))rdd2.keys.collectrdd2.values.collect","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"}]},{"title":"ScalaåŸºæœ¬ä½¿ç”¨-æ‚è®°","slug":"ScalaåŸºæœ¬ä½¿ç”¨","date":"2018-08-15T15:52:20.469Z","updated":"2019-06-17T04:40:09.395Z","comments":true,"path":"2018/08/15/ScalaåŸºæœ¬ä½¿ç”¨/","link":"","permalink":"http://gangtieguo.cn/2018/08/15/ScalaåŸºæœ¬ä½¿ç”¨/","excerpt":"[TOC] Varå’Œval var ä¿®é¥°çš„å˜é‡å¯æ”¹å˜ï¼Œval ä¿®é¥°çš„å˜é‡ä¸å¯æ”¹å˜ï¼›ä½†çœŸçš„å¦‚æ­¤å—ï¼Ÿäº‹å®ä¸Šï¼Œvar ä¿®é¥°çš„å¯¹è±¡å¼•ç”¨å¯ä»¥æ”¹å˜ï¼Œval ä¿®é¥°çš„åˆ™ä¸å¯æ”¹å˜ï¼Œä½†å¯¹è±¡çš„çŠ¶æ€å´æ˜¯å¯ä»¥æ”¹å˜çš„ã€‚ å®šä¹‰æ–¹æ³•1def m1(x:Int,y:Int):Int=x*y","text":"[TOC] Varå’Œval var ä¿®é¥°çš„å˜é‡å¯æ”¹å˜ï¼Œval ä¿®é¥°çš„å˜é‡ä¸å¯æ”¹å˜ï¼›ä½†çœŸçš„å¦‚æ­¤å—ï¼Ÿäº‹å®ä¸Šï¼Œvar ä¿®é¥°çš„å¯¹è±¡å¼•ç”¨å¯ä»¥æ”¹å˜ï¼Œval ä¿®é¥°çš„åˆ™ä¸å¯æ”¹å˜ï¼Œä½†å¯¹è±¡çš„çŠ¶æ€å´æ˜¯å¯ä»¥æ”¹å˜çš„ã€‚ å®šä¹‰æ–¹æ³•1def m1(x:Int,y:Int):Int=x*y å‡½æ•°çš„å®šä¹‰=&gt;123val func:Int =&gt;String=&#123;x=x.toString&#125;ä¹Ÿå¯ä»¥å†™æˆval func1=(x:Int)=&gt;x.toString å‡½æ•°å’Œæ–¹æ³•çš„åŒºåˆ«123456å®šä¹‰ä¸€ä¸ªæ–¹æ³•def m2(f:(Int,Int)=&gt;Int) =f(2,6)å®šä¹‰ä¸€ä¸ªå‡½æ•°val f2 =(x:Int,y:Int) =&gt;x-yè°ƒç”¨f2(m2) ç¥å¥‡çš„ä¸‹åˆ’çº¿å°†æ–¹æ³•è½¬æ¢æˆå‡½æ•° 123def m2(x:Int,y:Int):Int = x+yval f2(a:Int,y:Int)=&gt;x+yval f2 = m2 _ æ•°ç»„ã€æ˜ å°„ã€å…ƒç»„ã€é›†åˆæ•°ç»„123val arr = new Array[Int](10) //åˆ›å»ºæ•°ç»„ï¼Œæ— å€¼val arr1 = Array(1,2,3,4,5) //ç›´æ¥å®ä¾‹åŒ–for(e&lt;- arr) **yield** e*2; ç”¨yieldå…³é”®å­—å¯ä»¥ç”Ÿæˆä¸€ä¸ªæ–°çš„æ•°ç»„ ç”Ÿæˆçš„ç±»å‹å’Œå¾ªç¯çš„ç±»å‹æ˜¯ä¸€æ ·çš„ mapæ–¹æ³•mapæ–¹æ³•æ˜¯å°†æ¯ä¸€ä¸ªå…ƒç´ æ‹¿æ¥æ“ä½œarr.map(_*2) mapæ–¹æ³•æ›´å¥½ç”¨ mapæ˜¯æ’åºï¼Ÿ 12val a = Array(1,2,3,4,5)a.map((x:Int)=&gt;x*10)//åŒ¿åå‡½æ•° ç”±äºçŸ¥é“aä¸­çš„æ•°æ®ç±»å‹ å¯ä»¥å°†Intçœç•¥a.map(x=&gt;x10)è¿˜å¯ä»¥ç”¨å ä½ç¬¦ï¼Œè¿›è¡Œè¿›ä¸€æ­¥çš„çœç•¥a.map( 10) æ‰€æœ‰å¶æ•°å–å‡ºæ¥ç„¶åå†ä¹˜ä»¥1012345arr.filter((x:Int)=&gt;x%2==0)arr.filter(x=&gt;x%2 == 0)arr.filter(_%2==0)arr.filter(_%2==0).map(_*10)val p = println _ å¯ä»¥å°†æ–¹æ³•è½¬æ¢æˆå‡½æ•°ç”¨åˆ©ç”¨ _ trait å…³é”®å­—æ•°ç»„çš„å¸¸ç”¨å‡½æ•°åœ¨è¶…ç±»TraversableLikeä¸­å®šä¹‰äº†è¿™äº›å‡½æ•°12345678val arr = Array(1,2,3,4,5,6)arr.sum æ€»æ•°arr.sorted æ’åºarr.sorted.reverse é€†åºarr.sortBy(x=&gt;x)æŒ‰ç…§æœ¬èº«æ¥æ’åºarr.sortWith(_&gt;_)ä»å¤§åˆ°å°æ’åº &lt; ä»å°åˆ°å¤§æ’åºarr.sortWith((x,y))## æ˜ å°„ ç±»æ¯”javaä¸­çš„map12val m = Map(\"a\"-&gt;1,\"b\"-&gt;2)m(\"a\") å–å€¼ é‡Œè¾¹çš„å€¼ä¸èƒ½æ›´æ”¹å¦‚æœæ˜¯mulitbleçš„åŒ…å°±å¯ä»¥æ›´æ”¹äº†1m.getOrElse(\"c\",0) æ‰¾c å¦‚æœæ²¡æœ‰cå°±åˆ›å»º0 applyæ–¹æ³•å…ƒç»„val t = (1,â€sparkâ€,2.0); å€¼ç±»å‹éƒ½ä¸ä¸€å®š m.+=((â€œcâ€,1)) ä¸ m+=(â€œmâ€-&gt;1) å†™æ³•ç›¸åŒval t,(x,y,z) = (â€œaâ€,1,2.0)tæ˜¯å˜é‡ï¼Œx y z æ˜¯é”® å¯¹åº”ä¸‰ä¸ªå€¼ï¼Œå–å€¼çš„æ—¶å€™ ç›´æ¥ç”¨xå°±å¯ä»¥å–å€¼ å°†å¯¹å¶çš„è½¬æˆæ˜ å°„12val arr = Array((\"a\",1),(\"b\",2))arr.toMap å°±å¯ä»¥è½¬æˆæ˜ å°„ æ‹‰é“¾æ“ä½œ123val a = Array(\"a\",\"b\",\"c\")val b = Array(1,2,3);a.zip(b) å°†å…¶æ‹‰åœ¨ä¸€èµ·å˜æˆæ•°ç»„ï¼Œå…ƒç´ ä¸ºå…ƒç»„ é›†åˆåºåˆ—Seq,é›†Set,æ˜ å°„Mapé›†åˆåˆ†ä¸ºå¯å˜å’Œä¸å¯å˜çš„ mutable å’Œ immutableæ³¨æ„å’Œvalçš„å¯¹æ¯”å°†0æ’å…¥æ¥lstå‰é¢ç”Ÿæˆä¸€ä¸ªæ–°çš„é›†åˆ 1234val lst2 = 1 :: lst1; val lst3 = lst1.::(0)val lst4 = 0 +: lst1val lst5 = lst1.+:(0) å°†ä¸€ä¸ªå…ƒç´ æ·»åŠ åˆ°lståé¢äº§ç”Ÿä¸€ä¸ªæ–°çš„é›†åˆ 12val lst6 = lst1:+3val lst0 = List(4,5,6) å°†2ä¸ªliståˆå¹¶æˆä¸€ä¸ªæ–°çš„List 1val lst7 = lst1 ++ lst0 å°†lst0æ’å…¥åˆ°lstå‰é¢ç”Ÿæˆä¸€ä¸ªæ–°çš„é›†åˆ 1val lst8 = lst1 ++: lst0 foreachforeachæ˜¯å°†å…¶å€¼å–å‡ºæ¥ï¼Œä¸ä¼šæ–°ç”Ÿæˆä¸€ä¸ªé›†åˆmapå°†å€¼å–å‡ºæ¥ä¼šæ–°ç”Ÿæˆä¸€ä¸ªé›†åˆ MapMapæœ¬èº«ä¸æ”¯æŒæ’åºä½†æ˜¯æœ‰toListæ–¹æ³• æ— æ‹¬å·ï¼Œå¯ä»¥æ”¯æŒæ’åº List1val list = List(1,2,3,4,5,6); list.parè½¬æˆå¹¶è¡ŒåŒ–é›†åˆlist.par.reduce(_+_) å°†å…¶æ”¾åœ¨å¤šä¸ªreduceä¸­æ‰§è¡Œï¼Œæ•°æ®é‡å¤§çš„æ—¶å€™å°†ä¼šå˜å¾—å¾ˆå¿«reduceä¸æ˜¯å¹¶è¡Œé›†åˆçš„è¯ï¼Œå°±æ˜¯è°ƒç”¨çš„åº•å±‚reduceLeft(ä¸èƒ½å¹¶è¡Œäº†) foldå¯ä»¥æŒ‡å®šåˆå§‹å€¼1list.par.fold(0)(_+_) aggregate èšåˆéœ€è¦ä¼ ä¸¤ä¸ªå‡½æ•°ï¼Œç¬¬ä¸€ä¸ªå‡½æ•°æ˜¯å¯¹å…ƒç´ è¿›è¡Œæ“ä½œï¼Œç¬¬äºŒå‡½æ•°æ˜¯å¯¹å±€éƒ¨æ“ä½œçš„ç»“æœè¿›è¡Œæ“ä½œ 12Listï¼ˆList(1,7,9,8),List(0,1,2,3)ï¼‰list.aggregate(0)(_+_.sum,_+_) union å¹¶é›†ï¼Œintersectäº¤é›†ï¼Œdiffå·®é›† flatten å°†æ•°æ®å‹ç¼©123Listï¼ˆList(1,7,9,8),List(0,1,2,3)ï¼‰listAll.flattenList(1,7,9,8,0,1,2,3) äº§ç”Ÿæ–°çš„é›†åˆ å¯¹è±¡ä¸»æ„é€ å™¨é‡Œé¢çš„æ‰€æœ‰æ–¹æ³•éƒ½ä¼šè¢«æ‰§è¡Œ å•ä¾‹å¯¹è±¡æ‰€æœ‰çš„objectéƒ½æ˜¯ä¸€ä¸ªå•ä¾‹ï¼ˆæŠŠclassæ›¿æ¢æˆobjectï¼‰ä¸è¦newï¼Œç›´æ¥ç­‰äºç±»åå°±æ˜¯è°ƒç”¨çš„ä¸€ä¸ªå•ä¾‹å¯¹è±¡ 12345object Dog&#123; def main()&#123; val d = Dog &#125;&#125; ä¼´ç”Ÿå¯¹è±¡å°±æ˜¯å¯¹è±¡åå’Œç±»åä¸€æ ·ï¼Œå¹¶ä¸”åœ¨ä¸€ä¸ªscalaæ–‡ä»¶ä¸­å¯ä»¥å’Œç±»äº’ç›¸è®¿é—®ç§æœ‰å±æ€§scalaä¸­è¿”å›çš„å°±æ˜¯unitå°±æ˜¯è¿”å›çš„ä¸€ä¸ªæ‹¬å· applyæ–¹æ³•1234567891011121314Object Dog&#123; def apply():Unit = &#123; print (); &#125; def apply(name:String):Unit = &#123; print(name) &#125; def main()&#123; //ä¼šè°ƒç”¨ç¬¬ä¸€ä¸ªæ— å‚æ•°çš„applyæ–¹æ³• val d1 = Dog() //ä¼šè°ƒç”¨ç¬¬æœ‰å‚æ•°çš„applyæ–¹æ³• val d2 = Dog(\"haha\"); &#125;&#125; åº”ç”¨ç¨‹åºå¯¹è±¡æ²¡æœ‰ä»€ä¹ˆå®é™…ä½œç”¨ æ„é€ å‡½æ•°ç”¨thiså…³é”®å­—å®šä¹‰è¾…åŠ©æ„é€ å™¨1234def this(name:String,age:Int,gender:String)&#123;//æ¯ä¸ªæœåŠ¡æ„é€ å™¨å¿…é¡»ä»¥ä¸»æ„é€ å™¨æˆ–è€…å…¶ä»–çš„è¾…åŠ©æ„é€ å™¨çš„è°ƒç”¨å¼€å§‹//ä¸»æ„é€ å™¨å°±æ˜¯ç±»åä¸Šç›´æ¥å¡«å†™å‚æ•°&#125; å‡½æ•°ä¸æ–¹æ³•çš„äº’æ¢ï¼Œ ç¥å¥‡çš„ä¸‹åˆ’çº¿è¦æƒ³ä¼ åˆ°mapé‡Œé¢ï¼Œå¿…é¡»å¾—æ˜¯å‡½æ•° 123456789def fangfaæ–¹æ³•val func å‡½æ•°val arr = Array(1,2,3,4,5,6)arr.map(func(5))arr.map(func())val m = fangfa _ æ–¹æ³•funcè½¬æˆå‡½æ•°fangfa() ä¹Ÿå¯ä»¥è½¬å‡½æ•°def m(x:Int) = (y:int)=&gt;x*y æŸ¯é‡ŒåŒ–çš„ä¸¤ç§è¡¨è¾¾æ–¹å¼æŸ¯é‡ŒåŒ–æ˜¯ä¸»è¦æ˜¯é€šè¿‡ç±»å‹ç±»åŒ¹é…çš„ 12def m1(x:Int) = (y:Int)=&gt;x*ydef m2(x:Int)(y:Int)=&gt;x*y æŸ¯é‡ŒåŒ–ä¼šå…ˆæ‰§è¡Œä¸€éƒ¨åˆ†ï¼Œè¿”å›ä¸€ä¸ªå‡½æ•°1234567891011 def multi= (x:Int) =&gt;&#123; x*x &#125; def main(args: Array[String]): Unit = &#123; val arr = Array(1,2,3,5,4) val a1 = multi(10) println(s\"å¹³æ–¹å¼ï¼š$&#123;a1&#125;\") //æŒ‰ç…§è§„åˆ™ mapåªèƒ½å‚æ•°åªèƒ½æ˜¯å‡½æ•°ï¼Œmultiæ˜¯ä¸€ä¸ªæ–¹æ³•ï¼Œä½†æ˜¯åœ¨æŸ¯é‡ŒåŒ–çš„æ—¶å€™ï¼Œä¼šå…ˆè¿”å›ä¸€ä¸ªä¸­é—´ç»“æœæ˜¯å‡½æ•° val a2 = arr.map(multi) &#125; ç»§æ‰¿ ä»£ç† è£…é¥° ä¹‹é—´çš„åŒºåˆ«ç»§æ‰¿æ˜¯ç±»çš„å¢å¼º ä»£ç†æ˜¯å¯¹å®ä¾‹ï¼Œæ–¹æ³•çš„å¢å¼º è£…é¥°ä¹Ÿæ˜¯å¯¹æ–¹æ³•çš„å¢å¼º implicit def éšå¼çš„ï¼Œéšå¼è½¬åŒ–çš„åŒ…åœ¨predefä¸­ æ³›å‹12&lt;? extends clazz&gt; ä¼ å…¥çš„æ•°æ®æ˜¯clazzçš„å­ç±» &lt;? super clazz&gt; ä¼ å…¥çš„æ•°æ®æ˜¯clazzçš„çˆ¶ç±» &gt; &lt; &gt;= &lt;=ä»¥ä¸Šæ“ä½œç¬¦ï¼Œåœ¨scalaä¸­éƒ½æ˜¯æ–¹æ³• è§†å›¾å®šç•Œ view bound &lt;%scalaæ³›å‹ 12345class Person[T] &#123; def chooser[T &lt;: Comparable[T]](firit: T, second: T): T = &#123; first &#125; &#125; éšå¼è½¬æ¢ï¼šæˆ‘è‡ªå·±çš„éšå¼ä¸Šä¸‹æ–‡ 1234object MyPredef&#123; implicit å‡½æ•° implicit å€¼ &#125; viewboundè¦æ±‚ä¼ å…¥ä¸€ä¸ªéšå¼è½¬æ¢å‡½æ•° 123456789101112class Chooser[T &lt;% Ordered[T]] &#123; def bigger(first: T, second: T) : T = &#123; if(first &gt; second) first else second &#125; &#125; class Chooser[T] &#123; def bigger(first: T, second: T)(implicit ord: T =&gt; Ordered[T]) : T = &#123; if(first &gt; second) first else second &#125; &#125; contextboundè¦æ±‚ä¼ å…¥ä¸€ä¸ªéšå¼è½¬æ¢å€¼ 1234567891011class Chooser[T: Ordering] &#123; def bigger(first: T, second: T) : T = &#123; val ord = implicitly[Ordering[T]] if(ord.gt(first, second)) first else second &#125; &#125; class Chooser[T] &#123; def bigger(first: T, second: T)(implicit ord : Ordering[T]) : T = &#123; if(ord.gt(first, second)) first else second &#125; &#125; [+T][-T] ç›¸å½“äºä¼ å…¥äº†ä¸€ä¸ªéšå¼è½¬æ¢çš„å‡½æ•°ä¸€å®šè¦ä¼ å…¥ä¸€ä¸ªéšå¼è½¬æ¢å‡½æ•° 1234567class Chooser [t &lt;% Order[T]]&#123; def choose(first T,second T) :T =&#123; //val ord = implicitly[Ordering[T]] // if(ord.gt(first,second))) first else second; if(first.compare(second) &gt; 0) first else second; &#125;&#125; 12345678910implict object girlOrdering extends Ordering[girl]&#123;override def compare(x:girl,y:girl):Int = &#123;&#125;&#125;== å’Œè¿™ä¸ªå®ç°çš„æ•ˆæœæ˜¯ä¸€æ ·çš„,åªæ˜¯å–äº†ä¸€ä¸ªåå­—implicit val girlOrder = new Ordering[Girl]&#123;&#125; ä¸Šä¸‹æ–‡å®šç•Œ : content boundç›¸å½“äºä¼ å…¥äº†ä¸€ä¸ªéšå¼è½¬æ¢çš„å€¼ å…³äºå®ä½“ç±»Predefçš„å…³ç³»","categories":[{"name":"è¯­è¨€","slug":"è¯­è¨€","permalink":"http://gangtieguo.cn/categories/è¯­è¨€/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://gangtieguo.cn/tags/Scala/"}]},{"title":"Hadoop-HA-Federationæœºåˆ¶","slug":"Hadoop-æ„æˆåŠHA-","date":"2018-08-15T15:21:34.817Z","updated":"2018-12-25T03:53:59.560Z","comments":true,"path":"2018/08/15/Hadoop-æ„æˆåŠHA-/","link":"","permalink":"http://gangtieguo.cn/2018/08/15/Hadoop-æ„æˆåŠHA-/","excerpt":"[TOC] ï¼ˆ1ï¼‰hadoop-HAé›†ç¾¤è¿ä½œæœºåˆ¶ä»‹ç» æ‰€è°“HAï¼Œå³é«˜å¯ç”¨ï¼ˆ7*24å°æ—¶ä¸ä¸­æ–­æœåŠ¡ï¼‰å®ç°é«˜å¯ç”¨æœ€å…³é”®çš„æ˜¯æ¶ˆé™¤å•ç‚¹æ•…éšœï¼Œhadoop-haä¸¥æ ¼æ¥è¯´åº”è¯¥åˆ†æˆå„ä¸ªç»„ä»¶çš„HAæœºåˆ¶â€”â€”HDFSçš„HAã€YARNçš„HA","text":"[TOC] ï¼ˆ1ï¼‰hadoop-HAé›†ç¾¤è¿ä½œæœºåˆ¶ä»‹ç» æ‰€è°“HAï¼Œå³é«˜å¯ç”¨ï¼ˆ7*24å°æ—¶ä¸ä¸­æ–­æœåŠ¡ï¼‰å®ç°é«˜å¯ç”¨æœ€å…³é”®çš„æ˜¯æ¶ˆé™¤å•ç‚¹æ•…éšœï¼Œhadoop-haä¸¥æ ¼æ¥è¯´åº”è¯¥åˆ†æˆå„ä¸ªç»„ä»¶çš„HAæœºåˆ¶â€”â€”HDFSçš„HAã€YARNçš„HA ï¼ˆ2ï¼‰HDFSçš„HAæœºåˆ¶è¯¦è§£é€šè¿‡åŒnamenodeæ¶ˆé™¤å•ç‚¹æ•…éšœåŒnamenodeåè°ƒå·¥ä½œçš„è¦ç‚¹ï¼šâ€‹ Aã€å…ƒæ•°æ®ç®¡ç†æ–¹å¼éœ€è¦æ”¹å˜ï¼šâ€‹ å†…å­˜ä¸­å„è‡ªä¿å­˜ä¸€ä»½å…ƒæ•°æ®â€‹ Editsæ—¥å¿—åªèƒ½æœ‰ä¸€ä»½ï¼Œåªæœ‰ActiveçŠ¶æ€çš„namenodeèŠ‚ç‚¹å¯ä»¥åšå†™æ“ä½œâ€‹ï¼Œä¸¤ä¸ªnamenodeéƒ½å¯ä»¥è¯»å–editsï¼Œå…±äº«çš„editsæ”¾åœ¨ä¸€ä¸ªå…±äº«å­˜å‚¨ä¸­ç®¡ç†ï¼ˆqjournalå’ŒNFSä¸¤ä¸ªä¸»æµå®ç°ï¼‰â€‹ Bã€éœ€è¦ä¸€ä¸ªçŠ¶æ€ç®¡ç†åŠŸèƒ½æ¨¡å—â€‹ å®ç°äº†ä¸€ä¸ªzkfailoverï¼Œå¸¸é©»åœ¨æ¯ä¸€ä¸ªnamenodeæ‰€åœ¨çš„èŠ‚ç‚¹â€‹ æ¯ä¸€ä¸ªzkfailoverè´Ÿè´£ç›‘æ§è‡ªå·±æ‰€åœ¨namenodeèŠ‚ç‚¹ï¼Œåˆ©ç”¨zkè¿›è¡ŒçŠ¶æ€æ ‡è¯†â€‹ å½“éœ€è¦è¿›è¡ŒçŠ¶æ€åˆ‡æ¢æ—¶ï¼Œç”±zkfailoveræ¥è´Ÿè´£åˆ‡æ¢â€‹ åˆ‡æ¢æ—¶éœ€è¦é˜²æ­¢brain splitç°è±¡çš„å‘ç”Ÿ Hadoop-HAçš„ä¸»è¦æ€æƒ³æ˜¯æœ‰ä¸¤ä¸ªNameNodeï¼Œä¸€ä¸ªä½œä¸ºä¸»NameNodeï¼Œä¸€ä¸ªä½œä¸ºstandbyï¼Œä¸¤ä¸ªNameNodeä½¿ç”¨åŒä¸€ä¸ªå‘½åç©ºé—´ã€‚é€šè¿‡zookeeprï¼ˆJournalNodeï¼‰æ¥è¿›è¡Œåè°ƒï¼Œå®ç°NameNodeçš„ä¸»å¤‡åˆ‡æ¢ã€‚ çœŸæ­£çš„æ¶æ„å’Œæµç¨‹å¦‚ä¸Šå›¾æ‰€ç¤º Hadoopä¸­federationæœºåˆ¶å…±åŒè¿è¡Œå¤šä¸ªactiveçš„namenodeï¼ˆå¤šå¥—ä¸»å¤‡çš„namenodeé›†ç¾¤ï¼‰ï¼Œä¸”å…¬ç”¨ç”¨ä¸€å¥—datanode","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gangtieguo.cn/tags/Hadoop/"}]},{"title":"","slug":"HBaseæ€§èƒ½åˆ†æ","date":"2018-08-15T07:24:37.471Z","updated":"2019-06-17T04:40:09.381Z","comments":true,"path":"2018/08/15/HBaseæ€§èƒ½åˆ†æ/","link":"","permalink":"http://gangtieguo.cn/2018/08/15/HBaseæ€§èƒ½åˆ†æ/","excerpt":"[TOC] HBaseä»‹ç»HBaseè¡¨å¾ˆå¤§ï¼šä¸€ä¸ªè¡¨å¯ä»¥æœ‰æ•°åäº¿è¡Œï¼Œä¸Šç™¾ä¸‡åˆ—ï¼› HBaseçš„è¡¨å°†ä¼šåˆ†æˆå¾ˆå¤šä¸ªåˆ†åŒºï¼Œæ¯ä¸ªåˆ†åŒºéƒ¨åˆ†ä¼šå­˜åœ¨ä¸åŒçš„æœºå™¨ä¸Šåˆ†åŒºæ˜¯ä¸ºäº†ä¾¿äºæŸ¥è¯¢ï¼Œæ”¾åœ¨ä¸åŒæœºå™¨ä¸Šï¼Œioä¹Ÿå¢å¤§ï¼Œå‡å¦‚ä¸€ä¸ªæœºå™¨çš„ioçš„æ˜¯100mï¼Œä¸¤ä¸ªå°±ä¸º200mï¼Œè¯»å–é€Ÿåº¦å°±å˜å¿«äº†==&gt;å¤šå°æœºå™¨çš„ioèƒ½å¾—åˆ°å……åˆ†åˆ©ç”¨","text":"[TOC] HBaseä»‹ç»HBaseè¡¨å¾ˆå¤§ï¼šä¸€ä¸ªè¡¨å¯ä»¥æœ‰æ•°åäº¿è¡Œï¼Œä¸Šç™¾ä¸‡åˆ—ï¼› HBaseçš„è¡¨å°†ä¼šåˆ†æˆå¾ˆå¤šä¸ªåˆ†åŒºï¼Œæ¯ä¸ªåˆ†åŒºéƒ¨åˆ†ä¼šå­˜åœ¨ä¸åŒçš„æœºå™¨ä¸Šåˆ†åŒºæ˜¯ä¸ºäº†ä¾¿äºæŸ¥è¯¢ï¼Œæ”¾åœ¨ä¸åŒæœºå™¨ä¸Šï¼Œioä¹Ÿå¢å¤§ï¼Œå‡å¦‚ä¸€ä¸ªæœºå™¨çš„ioçš„æ˜¯100mï¼Œä¸¤ä¸ªå°±ä¸º200mï¼Œè¯»å–é€Ÿåº¦å°±å˜å¿«äº†==&gt;å¤šå°æœºå™¨çš„ioèƒ½å¾—åˆ°å……åˆ†åˆ©ç”¨ HBaseè¡¨æ— æ¨¡å¼ï¼šæ¯è¡Œéƒ½æœ‰ä¸€ä¸ªå¯æ’åºçš„ä¸»é”®å¥½ä»»æ„å¤šçš„åˆ—ï¼Œåˆ—å¯ä»¥æ ¹æ®éœ€è¦åŠ¨æ€çš„å¢åŠ ï¼ŒåŒä¸€å¼ è¡¨ä¸­ä¸åŒçš„è¡Œå¯ä»¥æœ‰ä¸åŒçš„åˆ—ï¼› é¢å‘åˆ—ï¼šåˆ—ç‹¬ç«‹æ£€ç´¢ï¼› ç¨€ç–ï¼šç©ºåˆ—å¹¶ä¸å ç”¨å­˜å‚¨ç©ºé—´ï¼Œè¡¨å¯ä»¥è®¾è®¡çš„éå¸¸ç¨€ç–ï¼› æ•°æ®ç±»å‹å•ä¸€ï¼šHBaseä¸­çš„æ•°æ®éƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œæ²¡æœ‰ç±»å‹ HBaseé‡‡ç”¨ç±»LSMçš„æ¶æ„ä½“ç³»ï¼Œæ•°æ®å†™å…¥å¹¶æ²¡æœ‰ç›´æ¥å†™å…¥æ•°æ®æ–‡ä»¶ï¼Œè€Œæ˜¯ä¼šå…ˆå†™å…¥ç¼“å­˜ï¼ˆMemstoreï¼‰ï¼Œåœ¨æ»¡è¶³ä¸€å®šæ¡ä»¶ä¸‹ç¼“å­˜æ•°æ®å†ä¼šå¼‚æ­¥åˆ·æ–°åˆ°ç¡¬ç›˜ã€‚ä¸ºäº†é˜²æ­¢æ•°æ®å†™å…¥ç¼“å­˜ä¹‹åä¸ä¼šå› ä¸ºRegionServerè¿›ç¨‹å‘ç”Ÿå¼‚å¸¸å¯¼è‡´æ•°æ®ä¸¢å¤±ï¼Œåœ¨å†™å…¥ç¼“å­˜ä¹‹å‰ä¼šé¦–å…ˆå°†æ•°æ®é¡ºåºå†™å…¥HLogä¸­ã€‚å¦‚æœä¸å¹¸ä¸€æ—¦å‘ç”ŸRegionServerå®•æœºæˆ–è€…å…¶ä»–å¼‚å¸¸ï¼Œè¿™ç§è®¾è®¡å¯ä»¥ä»HLogä¸­è¿›è¡Œæ—¥å¿—å›æ”¾è¿›è¡Œæ•°æ®è¡¥æ•‘ï¼Œä¿è¯æ•°æ®ä¸ä¸¢å¤±ã€‚HBaseæ•…éšœæ¢å¤çš„æœ€å¤§çœ‹ç‚¹å°±åœ¨äºå¦‚ä½•é€šè¿‡HLogå›æ”¾è¡¥æ•‘ä¸¢å¤±æ•°æ®ã€‚ HBaseç»“æ„HBaseè¿›è¡Œå­˜å‚¨çš„æœåŠ¡å™¨ HRegionæ˜¯HBaseå½“ä¸­çš„ä¸€ä¸ªç±»ï¼Œä¸€ä¸ªè¡¨åˆ†åŒºçš„ç±»ï¼ŒæŒ‰ç…§è¡Œåˆ†åŒºä¸€ä¸ªHRegionåªä¼šåœ¨ä¸€ä¸ªHBaseä¸Šï¼Œä¸€ä¸ªHBaseä¸Šå¯ä»¥æœ‰å¤šä¸ªHRegionHBaseè¡¨æ¯ä¸ªåˆ†åŒºï¼ˆæŒ‰ç…§è¡Œæ¥åˆ†åŒºï¼‰çš„æ•°æ®è¢«å°è£…åˆ°ä¸€ä¸ªç±»HRegionå†…å¦‚ï¼šHBaseå­˜åœ¨userè¡¨ï¼Œroleè¡¨ï¼Œå…±4ä¸ªregionServerï¼ŒHRegion1å­˜å‚¨ç®¡ç†userè¡¨çš„ä¸€éƒ¨åˆ†ï¼ŒHRegion2å­˜å‚¨ç®¡ç†userè¡¨çš„ä¸€éƒ¨åˆ†ï¼ŒHRegion3å­˜å‚¨ç®¡ç†roleè¡¨çš„ä¸€éƒ¨åˆ†ï¼ŒHRegion4å­˜å‚¨ç®¡ç†roleè¡¨çš„ä¸€éƒ¨åˆ† HRegionserverç®¡ç†ç”¨æˆ·å¯¹Tableçš„å¢ã€åˆ ã€æ”¹ã€æŸ¥æ“ä½œï¼›è®°å½•regionåœ¨å“ªå°Hregion serverä¸Šåœ¨Region Splitåï¼Œè´Ÿè´£æ–°Regionçš„åˆ†é…ï¼›æ–°æœºå™¨åŠ å…¥æ—¶ï¼Œç®¡ç†HRegion Serverçš„è´Ÿè½½å‡è¡¡ï¼Œè°ƒæ•´Regionåˆ†å¸ƒåœ¨HRegion Serverå®•æœºåï¼Œè´Ÿè´£å¤±æ•ˆHRegion Server ä¸Šçš„Regionsè¿ç§»ã€‚ HBaseHRegion Serverä¸»è¦è´Ÿè´£å“åº”ç”¨æˆ·I/Oè¯·æ±‚ï¼Œå‘HDFSæ–‡ä»¶ç³»ç»Ÿä¸­è¯»å†™æ•°æ®ï¼Œæ˜¯HBaseä¸­æœ€æ ¸å¿ƒçš„æ¨¡å—ã€‚ HRegion Serverç®¡ç†äº†å¾ˆå¤štableçš„åˆ†åŒºï¼Œä¹Ÿå°±æ˜¯regionã€‚ HRegionæ„æˆHRegionç±»ä¸­æœ‰HLogï¼Œstoreæˆå‘˜ï¼Œåˆ†åˆ«ä»£è¡¨ç¡¬ç›˜å’Œå†…å­˜ Storeæ¯ä¸ªRegionåŒ…å«ç€å¤šä¸ªStoreå¯¹è±¡ï¼Œä¸€ä¸ªåˆ—ç°‡å¯¹åº”ä¸€ä¸ªstore ã€‚æ¯ä¸ªStoreåŒ…å«ä¸€ä¸ªMemStoreå’Œè‹¥å¹²StoreFileï¼ŒStoreFileåŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªHFileï¼ŒStoreFileæ˜¯å¯¹HFileçš„ä¸€ç§å°è£…ã€‚MemStoreå­˜æ”¾åœ¨å†…å­˜ä¸­ï¼ŒStoreFileå­˜å‚¨åœ¨HDFSä¸Šã€‚ HLogHLogæœ€ç»ˆæ˜¯æ”¾åœ¨hdfsä¸Šã€‚ å½“æˆ‘ä»¬å®¢æˆ·ç«¯ä¸Šä¼ ä¸€ä¸ªè¡¨åï¼Œä¸€ä¸ªåˆ—ç°‡ï¼Œä¸€ä¸ªå€¼ï¼Œè¿™æ¡å‘½ä»¤çš„å€¼ä¼šåŸå°ä¸åŠ¨çš„å°†å…¶å†™å…¥åˆ°HLogé‡Œé¢ï¼Œ è¿™ä¸ªæ˜¯ä¸€ä¸ªappendLog,åªå¯ä»¥ä»åº•éƒ¨è¿½åŠ ï¼Œä¸å…è®¸ä¿®æ”¹ï¼Œå†™åˆ°HLogä¹‹åï¼Œå†å°†æ•°æ®å†™å…¥åˆ°å†…å­˜ï¼ˆmemstoreï¼‰å½“ä¸­ï¼ŒHLogé‡Œé¢æ˜¯å­˜å‚¨çš„æ“ä½œä¿¡æ¯çš„æ•°æ® å†™åœ¨HLogä¸­æ˜¯å› ä¸ºï¼Œé˜²æ­¢åœ¨å†™å…¥åˆ°å†…å­˜ä¸­çš„æ—¶å€™ï¼Œå®•æœº Regionçš„åˆ’åˆ†RegionæŒ‰å¤§å°åˆ†å‰²çš„ï¼Œéšç€æ•°æ®å¢å¤šï¼ŒRegionä¸æ–­å¢å¤§ï¼Œå½“å¢å¤§åˆ°ä¸€ä¸ªé˜€å€¼ï¼ˆé»˜è®¤256mï¼‰çš„æ—¶å€™ï¼ŒRegionå°±ä¼šåˆ†æˆä¸¤ä¸ªæ–°çš„Region HRegionçš„å­˜å‚¨ROOTè¡¨å’ŒMETAè¡¨HBaseçš„æ‰€æœ‰Regionå…ƒæ•°æ®è¢«å­˜å‚¨åœ¨.META.è¡¨ä¸­ï¼Œéšç€Regionçš„å¢å¤šï¼Œ.META.è¡¨ä¸­çš„æ•°æ®ä¹Ÿä¼šå¢å¤§ï¼Œå¹¶åˆ†è£‚æˆå¤šä¸ªæ–°çš„Regionã€‚ä¸ºäº†å®šä½.META.è¡¨ä¸­å„ä¸ªRegionçš„ä½ç½®ï¼ŒæŠŠ.META.è¡¨ä¸­æ‰€æœ‰Regionçš„å…ƒæ•°æ®ä¿å­˜åœ¨-ROOT-è¡¨ä¸­ï¼Œæœ€åç”±Zookeeperè®°å½•-ROOT-è¡¨çš„ä½ç½®ä¿¡æ¯ã€‚æ‰€æœ‰å®¢æˆ·ç«¯è®¿é—®ç”¨æˆ·æ•°æ®å‰ï¼Œéœ€è¦é¦–å…ˆè®¿é—®Zookeeperè·å¾—-ROOT-çš„ä½ç½®ï¼Œç„¶åè®¿é—®-ROOT-è¡¨è·å¾—.META.è¡¨çš„ä½ç½®ï¼Œæœ€åæ ¹æ®.META.è¡¨ä¸­çš„ä¿¡æ¯ç¡®å®šç”¨æˆ·æ•°æ®å­˜æ”¾çš„ä½ç½®ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ -ROOT-è¡¨æ°¸è¿œä¸ä¼šè¢«åˆ†å‰²ï¼Œå®ƒåªæœ‰ä¸€ä¸ªRegionï¼Œè¿™æ ·å¯ä»¥ä¿è¯æœ€å¤šåªéœ€è¦ä¸‰æ¬¡è·³è½¬å°±å¯ä»¥å®šä½ä»»æ„ä¸€ä¸ªRegionã€‚ä¸ºäº†åŠ å¿«è®¿é—®é€Ÿåº¦ï¼Œ.META.è¡¨çš„æ‰€æœ‰Regionå…¨éƒ¨ä¿å­˜åœ¨å†…å­˜ä¸­ã€‚å®¢æˆ·ç«¯ä¼šå°†æŸ¥è¯¢è¿‡çš„ä½ç½®ä¿¡æ¯ç¼“å­˜èµ·æ¥ï¼Œä¸”ç¼“å­˜ä¸ä¼šä¸»åŠ¨å¤±æ•ˆã€‚å¦‚æœå®¢æˆ·ç«¯æ ¹æ®ç¼“å­˜ä¿¡æ¯è¿˜è®¿é—®ä¸åˆ°æ•°æ®ï¼Œåˆ™è¯¢é—®ç›¸å…³.META.è¡¨çš„RegionæœåŠ¡å™¨ï¼Œè¯•å›¾è·å–æ•°æ®çš„ä½ç½®ï¼Œå¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œåˆ™è¯¢é—®-ROOT-è¡¨ç›¸å…³çš„.META.è¡¨åœ¨å“ªé‡Œã€‚æœ€åï¼Œå¦‚æœå‰é¢çš„ä¿¡æ¯å…¨éƒ¨å¤±æ•ˆï¼Œåˆ™é€šè¿‡ZooKeeperé‡æ–°å®šä½Regionçš„ä¿¡æ¯ã€‚æ‰€ä»¥å¦‚æœå®¢æˆ·ç«¯ä¸Šçš„ç¼“å­˜å…¨éƒ¨æ˜¯å¤±æ•ˆï¼Œåˆ™éœ€è¦è¿›è¡Œ6æ¬¡ç½‘ç»œæ¥å›ï¼Œæ‰èƒ½å®šä½åˆ°æ­£ç¡®çš„Regionã€‚ rootè¡¨,mateè¡¨éƒ½ä¸ä¼šå¾ˆå¤§å› ä¸ºrootè¡¨ï¼Œåªæ˜¯è®°å½•ä½ç½®ï¼Œæœ¬èº«å°±ä¸ä¼šå¤ªå¤§ï¼Œmetaè¡¨ï¼Œå’Œrootè¡¨åœ¨è¿‡ç¨‹ä¸­éƒ½ä¼šè¢«åŠ è½½åˆ°å†…å­˜ä¸­ç»è¿‡3æ¬¡æ¥å›ï¼Œæ€»å…±å…­æ¬¡ï¼Œä¼šå¾—åˆ°æ•°æ®è¡¨çš„ä½ç½®1ï¼Œclientå‘zookeeperè·å–rootè¡¨çš„ä½ç½® 2ï¼Œzookeeperè¿”å›rootè¡¨åœ°å€ä¿¡æ¯3ï¼Œclientè¯»å–rootè¡¨ï¼Œè·å¾—table1çš„metaè¡¨çš„åœ°å€ï¼Œ4 rootè¡¨æ‰€åœ¨æœºå™¨è¿”å›metaè¡¨åœ°å€5ï¼Œclientå‘meteè¡¨è¯»å–tableåœ°å€ï¼Œ6 clientå‘tableæ’å…¥æ•°æ®è¯»å–å’Œå†™å…¥éƒ½ä¼šç»å†ä¸Šé¢çš„è¿‡ç¨‹ HBaseå†™æ•°æ®æµç¨‹clientå‘HRegionserverå‘é€å†™è¯·æ±‚ã€‚HRegionserverå°†æ“ä½œä¿¡æ¯æ•°æ®å†™åˆ°hlogï¼ˆwrite ahead logï¼‰ã€‚ä¸ºäº†æ•°æ®çš„æŒä¹…åŒ–å’Œæ¢å¤ã€‚ HLogè®°å½•çš„æ˜¯æ“ä½œæ•°æ® HRegionserverå°†å®é™…æ•°æ®å†™åˆ°å†…å­˜ï¼ˆmemstoreï¼‰åé¦ˆclientå†™æˆåŠŸã€‚ æ•°æ®flushå½“memstoreæ•°æ®è¾¾åˆ°é˜ˆå€¼64ï¼ˆæ–°ç‰ˆæœ¬é»˜è®¤æ˜¯128Mï¼‰ï¼Œå°†å†…å­˜é›†åˆä¸­çš„æ•°æ®åˆ·åˆ°ç¡¬ç›˜ï¼Œå°†å†…å­˜ä¸­çš„æ•°æ®åˆ é™¤ï¼ŒåŒæ—¶åˆ é™¤Hlogä¸­çš„å†å²æ•°æ®ã€‚å¹¶å°†æ•°æ®å­˜å‚¨åˆ°hdfsä¸­ã€‚ä»¥æ•°æ®å—çš„å½¢å¼å­˜å‚¨ã€‚åœ¨hlogä¸­åšæ ‡è®°ç‚¹ã€‚ flushçš„è¯´æ˜å½“å†…å­˜æ–‡ä»¶memstoreæ–‡ä»¶è¾¾åˆ°64mçš„æ—¶å€™ï¼Œä¼šå°†æ•°æ®åˆå¹¶åˆ·æ–°å†™å…¥åˆ°StroeFileæ–‡ä»¶é‡Œé¢ï¼Œå†å°†æ•°æ®å†™å…¥åˆ°HFileé‡Œé¢ï¼ŒHFileæ–‡ä»¶æ˜¯ä¸€ä¸ªhdfsæ–‡ä»¶ï¼Œåºåˆ—åŒ–åˆ°hdfsé‡Œé¢ï¼Œå†é€šè¿‡hdfsçš„apiå†™å…¥åˆ°hdfsé›†ç¾¤é‡Œé¢æäº¤åˆ°hdfsé›†ç¾¤åï¼ŒHLogï¼Œmemstoreçš„æ•°æ®å°†ä¼šè¢«æ¸…é™¤ æ•°æ®åˆå¹¶1ã€å½“ï¼ˆhdfsä¸­ï¼‰æ•°æ®å—è¾¾åˆ°4å—ï¼Œhmasterå°†æ•°æ®å—åŠ è½½åˆ°æœ¬åœ°(HRegionserver)ï¼Œè¿›è¡Œåˆå¹¶æ³¨ï¼šè¿™ä¸ªæ•°æ®å—å•å—æ²¡æœ‰å¤§å°é™åˆ¶2ã€å½“åˆå¹¶çš„æ•°æ®è¶…è¿‡256Mï¼Œè¿›è¡Œæ‹†åˆ†ï¼Œå°†æ‹†åˆ†åçš„regionåˆ†é…ç»™ä¸åŒçš„hregionserverç®¡ç†æ³¨ï¼šå¦‚æœä¸å¤§äº256Mï¼Œå°†æ•°æ®åŸå°ä¸åŠ¨å†™å›hdfs3ã€å½“hregionseverå®•æœºåï¼Œå°†è¯¥hregionserverä¸Šçš„hlogæ‹†åˆ†ï¼ˆæŒ‰è¡¨æ‹†åˆ†)ï¼Œç„¶ååˆ†é…ç»™ä¸åŒçš„hregionserveråŠ è½½ï¼Œä¿®æ”¹.META.æ³¨ï¼šç”±Hmasteræ¥æ›´æ”¹.META.æ–‡ä»¶ï¼Œä¸ä¼šå¯¹HRegionserverçš„æ€§èƒ½é€ æˆå½±å“4ã€æ³¨æ„ï¼šhlogä¼šåŒæ­¥åˆ°hdfsæ³¨ï¼šåˆå¹¶çš„æ•°æ®æ˜¯å¯¹ç›¸åŒrowkeyçš„åˆ†ç»„åˆå¹¶ï¼Œå¦‚userè¡¨ä¸­ï¼Œå¯¹id=â€™1â€™çš„æ“ä½œå†…å®¹åˆå¹¶ï¼Œå¯¹id=â€™2â€™çš„åˆå¹¶ åˆå¹¶çš„å¥½å¤„åˆå¹¶æ“ä½œæ˜¯ç”±hmasteræ¥å·¥ä½œåˆå¹¶æ“ä½œæ˜¯é’ˆå¯¹ä¸€ä¸ªè¡¨æ¥è¯´ï¼Œuserè¡¨åˆå¹¶userè¡¨ï¼Œroleè¡¨åˆå¹¶roleè¡¨1ã€æ¸…ç†äº†åƒåœ¾æ•°æ®2ã€å°†å¤§çš„æ•°æ®å—æ‹†åˆ†åï¼Œç»™å¤šä¸ªæœºå™¨ç®¡ç†ï¼Œä¼˜åŒ–è¯»å–æ“ä½œç­‰é€Ÿç‡ HBaseçš„è¯»æµç¨‹é€šè¿‡zookeeperå’Œ-ROOT- .META.è¡¨å®šä½HBaseã€‚æ•°æ®ä»å†…å­˜å’Œç¡¬ç›˜åˆå¹¶åè¿”å›ç»™clientæ•°æ®å—ä¼šç¼“å­˜ ClientHBase Clientä½¿ç”¨HBaseçš„RPCæœºåˆ¶ä¸HRegionserverå’ŒRegionServerè¿›è¡Œé€šä¿¡ç®¡ç†ç±»æ“ä½œï¼šClientä¸HRegionserverè¿›è¡ŒRPCï¼›æ•°æ®è¯»å†™ç±»æ“ä½œï¼šClientä¸HBaseè¿›è¡ŒRPCã€‚ è¯»å–æ•°æ®ä¸å†™å…¥æ•°æ®æµç¨‹çš„ä¸åŒé™¤äº†åœ¨è¡¨ä¸­è¯»å–æ•°æ®ï¼Œè¿˜è¦åœ¨å†…å­˜ç£ç›˜ä¸Šå»æœå¯»è¿˜æœªå­˜åˆ°hdfsé‡Œé¢çš„æ•°æ®ã€‚æ‰€ä»¥æ•°æ®å—å¤ªå¤§çš„åº”è¯¥æ‹†åˆ†ï¼Œå¯ä»¥åŠ å¿«æŸ¥è¯¢é€Ÿåº¦ç»å¸¸æŸ¥è¯¢çš„æ•°æ®å—è¿˜åº”è¯¥æ”¾åœ¨å†…å­˜ä¸­ï¼ˆHRegionserverçš„å†…å­˜ä¸­ï¼‰ HBaseçš„å‡ºç°hdfsæ˜¯åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼Œåªèƒ½ä¿å­˜æ•´ä¸ªæ–‡ä»¶ï¼Œå¦‚æœä¸€è¡Œä¸€è¡Œçš„ä¿å­˜æ•°æ®ï¼Œnamenodeçš„å‹åŠ›ä¼šå¾ˆå¤§å‡å¦‚æœ‰ä¸€ä¸ªæ–‡ä»¶ä¸‹æœ‰100wå°ä¸ªæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶éƒ½æ˜¯1kï¼Œåœ¨datanodeä¸­ä¸ä¼šå ç”¨128mçš„åˆ†å—å¤§ï¼Œä½†æ˜¯æ¯ä¸ªæ–‡ä»¶å…ƒæ•°æ®æ‰€å çš„å¤§å°æ˜¯ä¸€æ ·çš„ï¼Œè¿™æ ·çš„è¯ï¼Œnamenodeç©ºé—´å æ»¡æ—¶ï¼Œdatanodeä¸­çš„æ•°æ®å®é™…ä¸Šå¾ˆå°‘ã€‚HBaseå°±ä¼šå¾ˆå¥½çš„è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¸€ä¸ªæ–‡ä»¶ä¸€ä¸ªæ–‡ä»¶å†™çš„æ—¶å€™ï¼Œæ˜¯å…ˆå†™å…¥åˆ°HBaseä¸­ï¼Œå…ˆå†™å…¥åˆ°HBaseé›†ç¾¤ä¸­ï¼ˆHBaseä¹Ÿåˆ†ä¸ºä¸»ä»ï¼Œä¸»ä¸ºHMasterï¼Œä»ä¸ºHRegionserverï¼‰ï¼ŒHRegionserverçš„å†…å­˜ä¸­ï¼Œå½“æ•°æ®é‡è¾¾åˆ°128mçš„æ—¶å€™ï¼Œå°†æ•°æ®å†™å…¥åˆ°hdfsä¸­ï¼Œè¿™æ ·128mæ•°æ®çš„å…ƒæ•°æ®åªæœ‰ä¸€æ¡ HBaseç»†èŠ‚HBaseå®é™…ä¸Šæ˜¯ä¸€ä¸ªç¼“å­˜å±‚ï¼Œå­˜å‚¨çš„æ•°æ®é‡å¾ˆå°‘ï¼Œå­˜çš„ä¸€éƒ¨åˆ†ç¼“å­˜çš„æ•°æ®ï¼ŒHBaseéœ€è¦zookeeperæ¥å®šä½HBaseæŸ¥æ‰¾æ•°æ®çš„åç§»é‡ HBase ä¸»ä»ä¹‹é—´çš„å…³ç³»hmasteråªæ˜¯ä¸€ä¸ªç®¡ç†è€…ï¼Œè€Œä¸”åªç®¡ç†ï¼Œå½“HBaseé›†ç¾¤æŒ‚æ‰ä¹‹åï¼Œæ•°æ®åç§»ä¿¡æ¯å’Œè¡¨çš„ä¿¡æ¯ï¼Œè€Œä¸ç®¡ç†æ•°æ®ä¿¡æ¯ï¼Œæ‰€ä»¥æœ‰ä¸€ç§æç«¯æƒ…å†µï¼Œå½“HBaseé›†ç¾¤å¯åŠ¨ä¹‹åï¼Œè¡¨åˆ›å»ºå®Œæˆï¼Œæ­£å¸¸è¿è¡Œä¹‹åï¼Œå°†hmasterå…³é—­ä¹Ÿä¸ä¼šå½±å“æ•´ä¸ªé›†ç¾¤çš„è¿è¡Œã€‚ä¸åƒnamenodeæŒ‚äº†ä¹‹åä¸èƒ½å“åº”äº† noteï¼šHBaseå†™å¿«è¯»æ…¢ï¼Œè¯»æ…¢æ˜¯ç›¸å¯¹äºå†™æ¥è¯´çš„ï¼Œä½†æ˜¯è·Ÿmysqlç›¸æ¯”ï¼Œä¹Ÿä¸æ˜¯ä¸€ä¸ªé‡çº§çš„","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://gangtieguo.cn/tags/HBase/"},{"name":"åŸç†","slug":"åŸç†","permalink":"http://gangtieguo.cn/tags/åŸç†/"}]},{"title":"Kafkaè¯»å†™æ•°æ®","slug":"Kafkaè¯»å–æ•°æ®æ€§èƒ½","date":"2018-08-14T11:47:22.892Z","updated":"2018-08-15T07:23:24.298Z","comments":true,"path":"2018/08/14/Kafkaè¯»å–æ•°æ®æ€§èƒ½/","link":"","permalink":"http://gangtieguo.cn/2018/08/14/Kafkaè¯»å–æ•°æ®æ€§èƒ½/","excerpt":"[TOC] é¦–å…ˆkafkaä¾èµ–äºæ“ä½œç³»ç»Ÿçš„pageCacheæœºåˆ¶ï¼Œå°½å¯èƒ½çš„æŠŠç©ºé—²çš„å†…å­˜ä½œä¸ºä¸€ä¸ªç£ç›˜ï¼Œåªæœ‰å‘ç”Ÿç¼ºé¡µçš„æ‰ä¼šæ”¾åœ¨ç£ç›˜ä¸­","text":"[TOC] é¦–å…ˆkafkaä¾èµ–äºæ“ä½œç³»ç»Ÿçš„pageCacheæœºåˆ¶ï¼Œå°½å¯èƒ½çš„æŠŠç©ºé—²çš„å†…å­˜ä½œä¸ºä¸€ä¸ªç£ç›˜ï¼Œåªæœ‰å‘ç”Ÿç¼ºé¡µçš„æ‰ä¼šæ”¾åœ¨ç£ç›˜ä¸­ é‚£ä¹ˆè½åœ¨ç£ç›˜ä¸Šçš„è¯ï¼Œè¿˜æœ‰å°±æ˜¯é‡‡ç”¨çš„æ˜¯sendfileæœºåˆ¶ï¼šä¸»è¦æ€æƒ³å°±æ˜¯åœ¨å†…æ ¸ä¸­è¿›è¡Œæ‹·è´ï¼Œå†å†™socketæµï¼Œä¼ ç»Ÿioæ˜¯å…ˆå°†æ•°æ®è¯»åˆ°å†…æ ¸åœ¨å†™åˆ°ç”¨æˆ·åŒºï¼Œåœ¨å†™åˆ°å†…æ ¸ï¼Œå†æ˜¯socketæµï¼Œè¿™æ ·å°±ç®—ç£ç›˜çš„è¯ï¼Œä¹Ÿæ˜¯ç›¸å½“å¿«çš„","categories":[{"name":"ç»„ä»¶","slug":"ç»„ä»¶","permalink":"http://gangtieguo.cn/categories/ç»„ä»¶/"}],"tags":[{"name":"åŸç†","slug":"åŸç†","permalink":"http://gangtieguo.cn/tags/åŸç†/"},{"name":"Kafka","slug":"Kafka","permalink":"http://gangtieguo.cn/tags/Kafka/"}]},{"title":"Kafkaæ·±å…¥è§£æ","slug":"Kafkaæ·±å…¥è§£æ","date":"2018-08-14T07:43:27.968Z","updated":"2019-06-17T04:40:09.390Z","comments":true,"path":"2018/08/14/Kafkaæ·±å…¥è§£æ/","link":"","permalink":"http://gangtieguo.cn/2018/08/14/Kafkaæ·±å…¥è§£æ/","excerpt":"[TOC] Kafkaç»“æ„ Producer ï¼šæ¶ˆæ¯ç”Ÿäº§è€…ï¼Œå°±æ˜¯å‘kafka brokerå‘æ¶ˆæ¯çš„å®¢æˆ·ç«¯ã€‚ Consumer ï¼šæ¶ˆæ¯æ¶ˆè´¹è€…ï¼Œå‘kafka brokerå–æ¶ˆæ¯çš„å®¢æˆ·ç«¯ Topic ï¼šå¯ä»¥ç†è§£ä¸ºä¸€ä¸ªé˜Ÿåˆ—","text":"[TOC] Kafkaç»“æ„ Producer ï¼šæ¶ˆæ¯ç”Ÿäº§è€…ï¼Œå°±æ˜¯å‘kafka brokerå‘æ¶ˆæ¯çš„å®¢æˆ·ç«¯ã€‚ Consumer ï¼šæ¶ˆæ¯æ¶ˆè´¹è€…ï¼Œå‘kafka brokerå–æ¶ˆæ¯çš„å®¢æˆ·ç«¯ Topic ï¼šå¯ä»¥ç†è§£ä¸ºä¸€ä¸ªé˜Ÿåˆ— Consumer Group ï¼ˆCGï¼‰ï¼š ï¼ˆæ¶ˆè´¹ç»„ï¼‰è¿™æ˜¯kafkaç”¨æ¥å®ç°ä¸€ä¸ªtopicæ¶ˆæ¯çš„å¹¿æ’­ï¼ˆå‘ç»™æ‰€æœ‰çš„consumerï¼‰å’Œå•æ’­ï¼ˆå‘ç»™ä»»æ„ä¸€ä¸ªconsumerï¼‰çš„æ‰‹æ®µã€‚ ä¸€ä¸ªtopicå¯ä»¥æœ‰å¤šä¸ªCGã€‚topicçš„æ¶ˆæ¯ä¼šå¤åˆ¶ï¼ˆä¸æ˜¯çœŸçš„å¤åˆ¶ï¼Œæ˜¯æ¦‚å¿µä¸Šçš„ï¼‰åˆ°æ‰€æœ‰çš„CGï¼Œä½†æ¯ä¸ªpartionåªä¼šæŠŠæ¶ˆæ¯å‘ç»™è¯¥CGä¸­çš„ä¸€ä¸ªconsumerã€‚å¦‚æœéœ€è¦å®ç°å¹¿æ’­ï¼Œåªè¦æ¯ä¸ªconsumeræœ‰ä¸€ä¸ªç‹¬ç«‹çš„CGå°±å¯ä»¥äº†ï¼ˆå³æ˜¯éœ€è¦æ¶ˆè´¹æ¶ˆæ¯çš„æ¶ˆè´¹è€…ï¼Œå±äºä¸åŒçš„æ¶ˆè´¹ç»„ï¼‰ã€‚è¦å®ç°å•æ’­åªè¦æ‰€æœ‰çš„consumeråœ¨åŒä¸€ä¸ªCGï¼ˆå³æ˜¯éœ€è¦æ¶ˆè´¹æ¶ˆæ¯çš„æ¶ˆè´¹è€…ï¼Œå±äºåŒä¸€ä¸ªæ¶ˆè´¹ç»„ï¼‰ã€‚ç”¨CGè¿˜å¯ä»¥å°†consumerè¿›è¡Œè‡ªç”±çš„åˆ†ç»„è€Œä¸éœ€è¦å¤šæ¬¡å‘é€æ¶ˆæ¯åˆ°ä¸åŒçš„topicã€‚ Broker ï¼šä¸€å°kafkaæœåŠ¡å™¨å°±æ˜¯ä¸€ä¸ªbrokerã€‚ä¸€ä¸ªé›†ç¾¤ç”±å¤šä¸ªbrokerç»„æˆã€‚ä¸€ä¸ªbrokerå¯ä»¥å®¹çº³å¤šä¸ªtopicã€‚ brokerçš„ä½œç”¨å°±æ˜¯å¸®ä½ æŠŠæ¶ˆæ¯ä»å‘é€ç«¯ä¼ é€åˆ°æ¥æ”¶ç«¯ Partitionï¼šä¸ºäº†å®ç°æ‰©å±•æ€§ï¼Œä¸€ä¸ªéå¸¸å¤§çš„topicå¯ä»¥åˆ†å¸ƒåˆ°å¤šä¸ªbrokerï¼ˆå³æœåŠ¡å™¨ï¼‰ä¸Šï¼Œä¸€ä¸ªtopicå¯ä»¥åˆ†ä¸ºå¤šä¸ªpartitionï¼Œæ¯ä¸ªpartitionæ˜¯ä¸€ä¸ªæœ‰åºçš„é˜Ÿåˆ—ã€‚partitionä¸­çš„æ¯æ¡æ¶ˆæ¯éƒ½ä¼šè¢«åˆ†é…ä¸€ä¸ªæœ‰åºçš„idï¼ˆoffsetï¼‰ã€‚kafkaåªä¿è¯æŒ‰ä¸€ä¸ªpartitionä¸­çš„é¡ºåºå°†æ¶ˆæ¯å‘ç»™consumerï¼Œä¸ä¿è¯ä¸€ä¸ªtopicçš„æ•´ä½“ï¼ˆå¤šä¸ªpartitioné—´ï¼‰çš„é¡ºåºã€‚ Offsetï¼škafkaçš„å­˜å‚¨æ–‡ä»¶éƒ½æ˜¯æŒ‰ç…§offset.kafkaæ¥å‘½åï¼Œç”¨offsetåšåå­—çš„å¥½å¤„æ˜¯æ–¹ä¾¿æŸ¥æ‰¾ã€‚ä¾‹å¦‚ä½ æƒ³æ‰¾ä½äº2049çš„ä½ç½®ï¼Œåªè¦æ‰¾åˆ°2048.kafkaçš„æ–‡ä»¶å³å¯ã€‚å½“ç„¶the first offsetå°±æ˜¯00000000000.kafka Consumerä¸topicå…³ç³»æœ¬è´¨ä¸Škafkaåªæ”¯æŒTopicï¼› æ¯ä¸ªgroupä¸­å¯ä»¥æœ‰å¤šä¸ªconsumerï¼Œæ¯ä¸ªconsumerå±äºä¸€ä¸ªconsumer groupï¼› é€šå¸¸æƒ…å†µä¸‹ï¼Œä¸€ä¸ªgroupä¸­ä¼šåŒ…å«å¤šä¸ªconsumerï¼Œè¿™æ ·ä¸ä»…å¯ä»¥æé«˜topicä¸­æ¶ˆæ¯çš„å¹¶å‘æ¶ˆè´¹èƒ½åŠ›ï¼Œè€Œä¸”è¿˜èƒ½æé«˜â€æ•…éšœå®¹é”™â€æ€§ï¼Œå¦‚æœgroupä¸­çš„æŸä¸ªconsumerå¤±æ•ˆé‚£ä¹ˆå…¶æ¶ˆè´¹çš„partitionså°†ä¼šæœ‰å…¶ä»–consumerè‡ªåŠ¨æ¥ç®¡ã€‚ å¯¹äºTopicä¸­çš„ä¸€æ¡ç‰¹å®šçš„æ¶ˆæ¯ï¼Œåªä¼šè¢«è®¢é˜…æ­¤Topicçš„æ¯ä¸ªgroupä¸­çš„å…¶ä¸­ä¸€ä¸ªconsumeræ¶ˆè´¹ï¼Œæ­¤æ¶ˆæ¯ä¸ä¼šå‘é€ç»™ä¸€ä¸ªgroupçš„å¤šä¸ªconsumerï¼› é‚£ä¹ˆä¸€ä¸ªgroupä¸­æ‰€æœ‰çš„consumerå°†ä¼šäº¤é”™çš„æ¶ˆè´¹æ•´ä¸ªTopicï¼Œæ¯ä¸ªgroupä¸­consumeræ¶ˆæ¯æ¶ˆè´¹äº’ç›¸ç‹¬ç«‹ï¼Œæˆ‘ä»¬å¯ä»¥è®¤ä¸ºä¸€ä¸ªgroupæ˜¯ä¸€ä¸ªâ€è®¢é˜…â€è€…ã€‚ åœ¨kafkaä¸­,ä¸€ä¸ªpartitionä¸­çš„æ¶ˆæ¯åªä¼šè¢«groupä¸­çš„ä¸€ä¸ªconsumeræ¶ˆè´¹(åŒä¸€æ—¶åˆ»)ï¼› ä¸€ä¸ªTopicä¸­çš„æ¯ä¸ªpartionsï¼Œåªä¼šè¢«ä¸€ä¸ªâ€è®¢é˜…è€…â€ä¸­çš„ä¸€ä¸ªconsumeræ¶ˆè´¹ï¼Œä¸è¿‡ä¸€ä¸ªconsumerå¯ä»¥åŒæ—¶æ¶ˆè´¹å¤šä¸ªpartitionsä¸­çš„æ¶ˆæ¯ã€‚ kafkaçš„è®¾è®¡åŸç†å†³å®š,å¯¹äºä¸€ä¸ªtopicï¼ŒåŒä¸€ä¸ªgroupä¸­ä¸èƒ½æœ‰å¤šäºpartitionsä¸ªæ•°çš„consumeråŒæ—¶æ¶ˆè´¹ï¼Œå¦åˆ™å°†æ„å‘³ç€æŸäº›consumerå°†æ— æ³•å¾—åˆ°æ¶ˆæ¯ã€‚ kafkaåªèƒ½ä¿è¯ä¸€ä¸ªpartitionä¸­çš„æ¶ˆæ¯è¢«æŸä¸ªconsumeræ¶ˆè´¹æ—¶æ˜¯é¡ºåºçš„ï¼›äº‹å®ä¸Šï¼Œä»Topicè§’åº¦æ¥è¯´,å½“æœ‰å¤šä¸ªpartitionsæ—¶,æ¶ˆæ¯ä»ä¸æ˜¯å…¨å±€æœ‰åºçš„ã€‚ Kafkaæ¶ˆæ¯çš„åˆ†å‘Producerå®¢æˆ·ç«¯è´Ÿè´£æ¶ˆæ¯çš„åˆ†å‘ kafkaé›†ç¾¤ä¸­çš„ä»»ä½•ä¸€ä¸ªbrokeréƒ½å¯ä»¥å‘produceræä¾›metadataä¿¡æ¯,è¿™äº›metadataä¸­åŒ…å«â€é›†ç¾¤ä¸­å­˜æ´»çš„serversåˆ—è¡¨â€/â€œpartitions leaderåˆ—è¡¨â€ç­‰ä¿¡æ¯ï¼› å½“producerè·å–åˆ°metadataä¿¡æ¯ä¹‹å, producerå°†ä¼šå’ŒTopicä¸‹æ‰€æœ‰partition leaderä¿æŒsocketè¿æ¥ï¼› æ¶ˆæ¯ç”±producerç›´æ¥é€šè¿‡socketå‘é€åˆ°brokerï¼Œä¸­é—´ä¸ä¼šç»è¿‡ä»»ä½•â€è·¯ç”±å±‚â€ï¼Œäº‹å®ä¸Šï¼Œæ¶ˆæ¯è¢«è·¯ç”±åˆ°å“ªä¸ªpartitionä¸Šç”±producerå®¢æˆ·ç«¯å†³å®šï¼› æ¯”å¦‚å¯ä»¥é‡‡ç”¨â€randomâ€â€key-hashâ€â€è½®è¯¢â€ç­‰,å¦‚æœä¸€ä¸ªtopicä¸­æœ‰å¤šä¸ªpartitions,é‚£ä¹ˆåœ¨producerç«¯å®ç°â€æ¶ˆæ¯å‡è¡¡åˆ†å‘â€æ˜¯å¿…è¦çš„ã€‚ åœ¨producerç«¯çš„é…ç½®æ–‡ä»¶ä¸­,å¼€å‘è€…å¯ä»¥æŒ‡å®špartitionè·¯ç”±çš„æ–¹å¼ã€‚ Produceræ¶ˆæ¯å‘é€çš„åº”ç­”æœºåˆ¶è®¾ç½®å‘é€æ•°æ®æ˜¯å¦éœ€è¦æœåŠ¡ç«¯çš„åé¦ˆ,æœ‰ä¸‰ä¸ªå€¼0,1,-1 0: producerä¸ä¼šç­‰å¾…brokerå‘é€ack 1: å½“leaderæ¥æ”¶åˆ°æ¶ˆæ¯ä¹‹åå‘é€ack -1: å½“æ‰€æœ‰çš„followeréƒ½åŒæ­¥æ¶ˆæ¯æˆåŠŸåå‘é€ack request.required.acks=0 Consumerçš„è´Ÿè½½å‡è¡¡å½“ä¸€ä¸ªgroupä¸­,æœ‰consumeråŠ å…¥æˆ–è€…ç¦»å¼€æ—¶,ä¼šè§¦å‘partitionså‡è¡¡.å‡è¡¡çš„æœ€ç»ˆç›®çš„,æ˜¯æå‡topicçš„å¹¶å‘æ¶ˆè´¹èƒ½åŠ›ï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š å‡å¦‚topic1,å…·æœ‰å¦‚ä¸‹partitions: P0,P1,P2,P3 åŠ å…¥groupä¸­,æœ‰å¦‚ä¸‹consumer: C1,C2 é¦–å…ˆæ ¹æ®partitionç´¢å¼•å·å¯¹partitionsæ’åº: P0,P1,P2,P3 æ ¹æ®consumer.idæ’åº: C0,C1 è®¡ç®—å€æ•°: M = [P0,P1,P2,P3].size / [C0,C1].size,æœ¬ä¾‹å€¼M=2(å‘ä¸Šå–æ•´) ç„¶åä¾æ¬¡åˆ†é…partitions: C0 = [P0,P1],C1=[P2,P3],å³Ci = [P(i M),P((i + 1) M -1)] Kafkaæ–‡ä»¶å­˜å‚¨æœºåˆ¶Kafkaæ–‡ä»¶å­˜å‚¨åŸºæœ¬ç»“æ„åœ¨Kafkaæ–‡ä»¶å­˜å‚¨ä¸­ï¼ŒåŒä¸€ä¸ªtopicä¸‹æœ‰å¤šä¸ªä¸åŒpartitionï¼Œæ¯ä¸ªpartitionä¸ºä¸€ä¸ªç›®å½•ï¼Œpartitonå‘½åè§„åˆ™ä¸ºtopicåç§°+æœ‰åºåºå·ï¼Œç¬¬ä¸€ä¸ªpartitonåºå·ä»0å¼€å§‹ï¼Œåºå·æœ€å¤§å€¼ä¸ºpartitionsæ•°é‡å‡1ã€‚ æ¯ä¸ªpartion(ç›®å½•)ç›¸å½“äºä¸€ä¸ªå·¨å‹æ–‡ä»¶è¢«å¹³å‡åˆ†é…åˆ°å¤šä¸ªå¤§å°ç›¸ç­‰segment(æ®µ)æ•°æ®æ–‡ä»¶ä¸­ã€‚ä½†æ¯ä¸ªæ®µ**segmentfile**æ¶ˆæ¯æ•°é‡ä¸ä¸€å®šç›¸ç­‰ï¼Œè¿™ç§ç‰¹æ€§æ–¹ä¾¿oldsegment fileå¿«é€Ÿè¢«åˆ é™¤ã€‚é»˜è®¤ä¿ç•™7å¤©çš„æ•°æ®ã€‚ æ¯ä¸ªpartitonåªéœ€è¦æ”¯æŒé¡ºåºè¯»å†™å°±è¡Œäº†ï¼Œsegmentæ–‡ä»¶ç”Ÿå‘½å‘¨æœŸç”±æœåŠ¡ç«¯é…ç½®å‚æ•°å†³å®šã€‚ï¼ˆä»€ä¹ˆæ—¶å€™åˆ›å»ºï¼Œä»€ä¹ˆæ—¶å€™åˆ é™¤ï¼‰ Kafka Partition SegmentSegment fileç»„æˆï¼šç”±2å¤§éƒ¨åˆ†ç»„æˆï¼Œåˆ†åˆ«ä¸ºindex fileå’Œdata fileï¼Œæ­¤2ä¸ªæ–‡ä»¶ä¸€ä¸€å¯¹åº”ï¼Œæˆå¯¹å‡ºç°ï¼Œåç¼€â€.indexâ€å’Œâ€œ.logâ€åˆ†åˆ«è¡¨ç¤ºä¸ºsegmentç´¢å¼•æ–‡ä»¶ã€æ•°æ®æ–‡ä»¶ã€‚ l Segmentæ–‡ä»¶å‘½åè§„åˆ™ï¼špartionå…¨å±€çš„ç¬¬ä¸€ä¸ªsegmentä»0å¼€å§‹ï¼Œåç»­æ¯ä¸ªsegmentæ–‡ä»¶åä¸ºä¸Šä¸€ä¸ªsegmentæ–‡ä»¶æœ€åä¸€æ¡æ¶ˆæ¯çš„offsetå€¼ã€‚æ•°å€¼æœ€å¤§ä¸º64ä½longå¤§å°ï¼Œ19ä½æ•°å­—å­—ç¬¦é•¿åº¦ï¼Œæ²¡æœ‰æ•°å­—ç”¨0å¡«å……ã€‚ l ç´¢å¼•æ–‡ä»¶å­˜å‚¨å¤§é‡å…ƒæ•°æ®ï¼Œæ•°æ®æ–‡ä»¶å­˜å‚¨å¤§é‡æ¶ˆæ¯ï¼Œç´¢å¼•æ–‡ä»¶ä¸­å…ƒæ•°æ®æŒ‡å‘å¯¹åº”æ•°æ®æ–‡ä»¶ä¸­messageçš„ç‰©ç†åç§»åœ°å€ã€‚ 3ï¼Œ497ï¼šå½“å‰logæ–‡ä»¶ä¸­çš„ç¬¬å‡ æ¡ä¿¡æ¯ï¼Œå­˜æ”¾åœ¨ç£ç›˜ä¸Šçš„é‚£ä¸ªåœ°æ–¹ ä¸Šè¿°å›¾ä¸­ç´¢å¼•æ–‡ä»¶å­˜å‚¨å¤§é‡å…ƒæ•°æ®ï¼Œæ•°æ®æ–‡ä»¶å­˜å‚¨å¤§é‡æ¶ˆæ¯ï¼Œç´¢å¼•æ–‡ä»¶ä¸­å…ƒæ•°æ®æŒ‡å‘å¯¹åº”æ•°æ®æ–‡ä»¶ä¸­messageçš„ç‰©ç†åç§»åœ°å€ã€‚ å…¶ä¸­ä»¥ç´¢å¼•æ–‡ä»¶ä¸­å…ƒæ•°æ®3,497ä¸ºä¾‹ï¼Œä¾æ¬¡åœ¨æ•°æ®æ–‡ä»¶ä¸­è¡¨ç¤ºç¬¬3ä¸ªmessage(åœ¨å…¨å±€partitonè¡¨ç¤ºç¬¬368772ä¸ªmessage)ã€ä»¥åŠè¯¥æ¶ˆæ¯çš„ç‰©ç†åç§»åœ°å€ä¸º497ã€‚ l segment data fileç”±è®¸å¤šmessageç»„æˆï¼Œ qqç‰©ç†ç»“æ„å¦‚ä¸‹ï¼š å…³é”®å­—** è§£é‡Šè¯´æ˜** 8 byte offset åœ¨parition(åˆ†åŒº)å†…çš„æ¯æ¡æ¶ˆæ¯éƒ½æœ‰ä¸€ä¸ªæœ‰åºçš„idå·ï¼Œè¿™ä¸ªidå·è¢«ç§°ä¸ºåç§»(offset),å®ƒå¯ä»¥å”¯ä¸€ç¡®å®šæ¯æ¡æ¶ˆæ¯åœ¨parition(åˆ†åŒº)å†…çš„ä½ç½®ã€‚å³offsetè¡¨ç¤ºpartiionçš„ç¬¬å¤šå°‘message 4 byte message size messageå¤§å° 4 byte CRC32 ç”¨crc32æ ¡éªŒmessage 1 byte â€œmagicâ€ è¡¨ç¤ºæœ¬æ¬¡å‘å¸ƒKafkaæœåŠ¡ç¨‹åºåè®®ç‰ˆæœ¬å· 1 byte â€œattributesâ€ è¡¨ç¤ºä¸ºç‹¬ç«‹ç‰ˆæœ¬ã€æˆ–æ ‡è¯†å‹ç¼©ç±»å‹ã€æˆ–ç¼–ç ç±»å‹ã€‚ 4 byte key length è¡¨ç¤ºkeyçš„é•¿åº¦,å½“keyä¸º-1æ—¶ï¼ŒK byte keyå­—æ®µä¸å¡« K byte key å¯é€‰ value bytes payload è¡¨ç¤ºå®é™…æ¶ˆæ¯æ•°æ®ã€‚ kafkaæŸ¥æ‰¾messageï¼Œå…ˆæŸ¥æ‰¾segment file 00000000000000000000.indexè¡¨ç¤ºæœ€å¼€å§‹çš„æ–‡ä»¶ï¼Œèµ·å§‹åç§»é‡(offset)ä¸º0 00000000000000368769.indexçš„æ¶ˆæ¯é‡èµ·å§‹åç§»é‡ä¸º368770= 368769 + 1 00000000000000737337.indexçš„èµ·å§‹åç§»é‡ä¸º737338=737337+ 1 å…¶ä»–åç»­æ–‡ä»¶ä¾æ¬¡ç±»æ¨ã€‚ ä»¥èµ·å§‹åç§»é‡å‘½åå¹¶æ’åºè¿™äº›æ–‡ä»¶ï¼Œåªè¦æ ¹æ®offsetäºŒåˆ†æŸ¥æ‰¾æ–‡ä»¶åˆ—è¡¨ï¼Œå°±å¯ä»¥å¿«é€Ÿå®šä½åˆ°å…·ä½“æ–‡ä»¶ã€‚å½“offset=368776æ—¶å®šä½åˆ°00000000000000368769.indexå’Œå¯¹åº”logæ–‡ä»¶ã€‚ å†é€šè¿‡segment fileæŸ¥æ‰¾message å½“offset=368776æ—¶ï¼Œä¾æ¬¡å®šä½åˆ°00000000000000368769.indexçš„å…ƒæ•°æ®ç‰©ç†ä½ç½®å’Œ00000000000000368769.logçš„ç‰©ç†åç§»åœ°å€ ç„¶åå†é€šè¿‡00000000000000368769.logé¡ºåºæŸ¥æ‰¾ç›´åˆ°offset=368776ä¸ºæ­¢ã€‚","categories":[{"name":"ç»„ä»¶","slug":"ç»„ä»¶","permalink":"http://gangtieguo.cn/categories/ç»„ä»¶/"}],"tags":[{"name":"åŸç†","slug":"åŸç†","permalink":"http://gangtieguo.cn/tags/åŸç†/"},{"name":"Kafka","slug":"Kafka","permalink":"http://gangtieguo.cn/tags/Kafka/"}]},{"title":"Kafkaé›†ç¾¤é…ç½®åŠé…ç½®æ–‡ä»¶","slug":"Kafkaé›†ç¾¤é…ç½®åŠé…ç½®æ–‡ä»¶","date":"2018-08-13T06:54:35.320Z","updated":"2018-08-13T07:21:34.526Z","comments":true,"path":"2018/08/13/Kafkaé›†ç¾¤é…ç½®åŠé…ç½®æ–‡ä»¶/","link":"","permalink":"http://gangtieguo.cn/2018/08/13/Kafkaé›†ç¾¤é…ç½®åŠé…ç½®æ–‡ä»¶/","excerpt":"[TOC] Kafkaé›†ç¾¤éƒ¨ç½²kafkaé»˜è®¤æ¨èçš„æ˜¯2.11å¼€å¤´çš„ï¼Œå¦‚æœç³»ç»Ÿä¸­æ²¡æœ‰å…¶ä»–è½¯ä»¶ä¾èµ–äºScalaçš„è¯ï¼Œå°±ä½¿ç”¨2.11ç‰ˆæœ¬çš„scalaæ˜¯ä¾èµ–äºzookeeperçš„ï¼Œæ‰€ä»¥éœ€è¦ç»™zookeeperé…ç½®åœ°å€","text":"[TOC] Kafkaé›†ç¾¤éƒ¨ç½²kafkaé»˜è®¤æ¨èçš„æ˜¯2.11å¼€å¤´çš„ï¼Œå¦‚æœç³»ç»Ÿä¸­æ²¡æœ‰å…¶ä»–è½¯ä»¶ä¾èµ–äºScalaçš„è¯ï¼Œå°±ä½¿ç”¨2.11ç‰ˆæœ¬çš„scalaæ˜¯ä¾èµ–äºzookeeperçš„ï¼Œæ‰€ä»¥éœ€è¦ç»™zookeeperé…ç½®åœ°å€ 1ã€ä¸‹è½½å®‰è£…åŒ…http://kafka.apache.org/downloads.htmlåœ¨linuxä¸­ä½¿ç”¨wgetå‘½ä»¤ä¸‹è½½å®‰è£…åŒ…wget http://mirrors.hust.edu.cn/apache/kafka/0.8.2.2/kafka_2.11-0.8.2.2.tgz 2ã€è§£å‹å®‰è£…åŒ…123tar -zxvf kafka_2.11-0.8.2.2.tgz -C /home/bigdata/apps/kafka/cd /home/bigdata/apps/kafka/ln -s kafka_2.11-0.8.2.2 kafka 3ã€ä¿®æ”¹é…ç½®æ–‡ä»¶é…ç½®æ–‡ä»¶æœ‰4ä¸ªç‚¹ hostnameåº”è¯¥ä¿æŒä¸€è‡´ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061broker.id =0#æ¯ä¸€ä¸ªbrokeråœ¨é›†ç¾¤ä¸­çš„å”¯ä¸€è¡¨ç¤ºï¼Œè¦æ±‚æ˜¯æ­£æ•°ã€‚å½“è¯¥æœåŠ¡å™¨çš„IPåœ°å€å‘ç”Ÿæ”¹å˜æ—¶ï¼Œbroker.idæ²¡æœ‰å˜åŒ–ï¼Œåˆ™ä¸ä¼šå½±å“consumersçš„æ¶ˆæ¯æƒ…å†µ log.dirs=/home/bigdata/logs/kafka-logs#kafkaæ•°æ®çš„å­˜æ”¾åœ°å€ï¼Œå¤šä¸ªåœ°å€çš„è¯ç”¨é€—å·åˆ†å‰² /data/kafka-logs-1ï¼Œ/data/kafka-logs-2 port =9092#broker serveræœåŠ¡ç«¯å£ message.max.bytes =6525000#è¡¨ç¤ºæ¶ˆæ¯ä½“çš„æœ€å¤§å¤§å°ï¼Œå•ä½æ˜¯å­—èŠ‚ num.network.threads =3#brokerå¤„ç†æ¶ˆæ¯çš„æœ€å¤§çº¿ç¨‹æ•°ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ä¸éœ€è¦å»ä¿®æ”¹ é…ç½®äº†ä¸‰å°æœåŠ¡å™¨ï¼Œæ‰€ä»¥é€‰æ‹©ä¸‰ä¸ª #num.io.threads =8#brokerå¤„ç†ç£ç›˜IOçš„çº¿ç¨‹æ•°ï¼Œæ•°å€¼åº”è¯¥å¤§äºä½ çš„ç¡¬ç›˜æ•° #background.threads =4#ä¸€äº›åå°ä»»åŠ¡å¤„ç†çš„çº¿ç¨‹æ•°ï¼Œä¾‹å¦‚è¿‡æœŸæ¶ˆæ¯æ–‡ä»¶çš„åˆ é™¤ç­‰ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ä¸éœ€è¦å»åšä¿®æ”¹ queued.max.requests =500#ç­‰å¾…IOçº¿ç¨‹å¤„ç†çš„è¯·æ±‚é˜Ÿåˆ—æœ€å¤§æ•°ï¼Œè‹¥æ˜¯ç­‰å¾…IOçš„è¯·æ±‚è¶…è¿‡è¿™ä¸ªæ•°å€¼ï¼Œé‚£ä¹ˆä¼šåœæ­¢æ¥å—å¤–éƒ¨æ¶ˆæ¯ï¼Œåº”è¯¥æ˜¯ä¸€ç§è‡ªæˆ‘ä¿æŠ¤æœºåˆ¶ã€‚#brokerçš„ä¸»æœºåœ°å€ï¼Œè‹¥æ˜¯è®¾ç½®äº†ï¼Œé‚£ä¹ˆä¼šç»‘å®šåˆ°è¿™ä¸ªåœ°å€ä¸Šï¼Œè‹¥æ˜¯æ²¡æœ‰ï¼Œä¼šç»‘å®šåˆ°æ‰€æœ‰çš„æ¥å£ä¸Šï¼Œå¹¶å°†å…¶ä¸­ä¹‹ä¸€å‘é€åˆ°ZKï¼Œä¸€èˆ¬ä¸è®¾ç½® socket.send.buffer.bytes=102400#socketçš„å‘é€ç¼“å†²åŒºï¼Œsocketçš„è°ƒä¼˜å‚æ•°SO_SNDBUFF socket.receive.buffer.bytes =102400#socketçš„æ¥å—ç¼“å†²åŒºï¼Œsocketçš„è°ƒä¼˜å‚æ•°SO_RCVBUFF socket.request.max.bytes =104857600#socketè¯·æ±‚çš„æœ€å¤§æ•°å€¼ï¼Œé˜²æ­¢serverOOMï¼Œmessage.max.byteså¿…ç„¶è¦å°äºsocket.request.max.bytesï¼Œä¼šè¢«topicåˆ›å»ºæ—¶çš„æŒ‡å®šå‚æ•°è¦†ç›– #log.segment.bytes =1024*1024*1024#topicçš„åˆ†åŒºæ˜¯ä»¥ä¸€å †segmentæ–‡ä»¶å­˜å‚¨çš„ï¼Œè¿™ä¸ªæ§åˆ¶æ¯ä¸ªsegmentçš„å¤§å°ï¼Œä¼šè¢«topicåˆ›å»ºæ—¶çš„æŒ‡å®šå‚æ•°è¦†ç›– log.roll.hours =168 zookeeper.connect = bigdata1:2181,bigdata2:2181,bigdata3:2181#zookeeperé›†ç¾¤çš„åœ°å€ï¼Œå¯ä»¥æ˜¯å¤šä¸ªï¼Œå¤šä¸ªä¹‹é—´ç”¨é€—å·åˆ†å‰² hostname1:port1,hostname2:port2,hostname3:port3 zookeeper.session.timeout.ms=6000#ZooKeeperçš„æœ€å¤§è¶…æ—¶æ—¶é—´ï¼Œå°±æ˜¯å¿ƒè·³çš„é—´éš”ï¼Œè‹¥æ˜¯æ²¡æœ‰åæ˜ ï¼Œé‚£ä¹ˆè®¤ä¸ºå·²ç»æ­»äº†ï¼Œä¸æ˜“è¿‡å¤§ zookeeper.connection.timeout.ms =6000#ZooKeeperçš„è¿æ¥è¶…æ—¶æ—¶é—´ zookeeper.sync.time.ms =2000#host.name=bigdata1 #brokerçš„ä¸»æœºåœ°å€ï¼Œè‹¥æ˜¯è®¾ç½®äº†ï¼Œé‚£ä¹ˆä¼šç»‘å®šåˆ°è¿™ä¸ªåœ°å€ä¸Šï¼Œè‹¥æ˜¯æ²¡æœ‰ï¼Œä¼šç»‘å®šåˆ°æ‰€æœ‰çš„æ¥å£ä¸Šï¼Œå¹¶å°†å…¶ä¸­ä¹‹ä¸€å‘é€åˆ°ZKï¼Œä¸€èˆ¬ä¸è®¾ç½®,hostnameä¸ºä¸»æœº # è¿™ä¸ªæ˜¯è½»é‡çš„é…ç½®æ–‡ä»¶#brokerçš„å…¨å±€å”¯ä¸€ç¼–å·ï¼Œä¸èƒ½é‡å¤broker.id=0#ç”¨æ¥ç›‘å¬è¿æ¥çš„ç«¯å£ï¼Œproduceræˆ–consumerå°†åœ¨æ­¤ç«¯å£å»ºç«‹è¿æ¥port=9092#å¤„ç†ç½‘ç»œè¯·æ±‚çš„çº¿ç¨‹æ•°é‡ï¼Œé›†ç¾¤ä¸­æœ‰å‡ ä¸ªèŠ‚ç‚¹å°±è®¾ç½®å‡ ä¸ªnum.network.threads=3#ç”¨æ¥å¤„ç†ç£ç›˜ioçš„çº¿ç¨‹æ•°é‡num.io.threads=8#å‘é€å¥—æ¥å­—çš„ç¼“å†²åŒºå¤§å°socket.send.buffer.bytes=102400#æ¥å—å¥—æ¥å­—çš„ç¼“å†²åŒºå¤§å°socket.receive.buffer.bytes=102400#è¯·æ±‚å¥—æ¥å­—çš„ç¼“å†²åŒºå¤§å°socket.request.max.bytes=104857600#kafkaè¿è¡Œæ—¥å¿—å­˜æ”¾çš„è·¯å¾„log.dirs=/home/hadoop/logs/kafka#topicåœ¨å½“å‰brokerä¸Šçš„åˆ†ç‰‡ä¸ªæ•°num.partitions=2#ç”¨æ¥æ¢å¤å’Œæ¸…ç†dataä¸‹æ•°æ®çš„çº¿ç¨‹æ•°é‡num.recovery.threads.per.data.dir=1#segmentæ–‡ä»¶ä¿ç•™çš„æœ€é•¿æ—¶é—´ã€‚è¶…æ—¶å°†ä¼šè¢«åˆ é™¤log.retention.hours=168#æ»šåŠ¨åˆ é™¤ç”Ÿæˆå¿ƒå¾—segmentæ–‡ä»¶çš„æœ€å¤§æ—¶é—´log.roll.hour=168 ------------------ 12ipï¼ˆé‡è¦ï¼‰ï¼Œå¦‚æœä¸æ”¹ï¼Œåˆ™å®¢æˆ·ç«¯ä¼šæŠ›å‡ºï¼šproducer connection to localhost:9092 unsuccessfulé”™è¯¯ï¼Œadvertised.host.name=192.168.11.11 \\12 cp /home/bigdata/apps/kafka/config/server.properties /home/bigdata/apps/kafka/config/server.properties.bakvi /home/bigdata/apps/kafka/config/server.properties 12# 4ã€åˆ†å‘å®‰è£…åŒ… scp -r /home/bigdata/apps/kafka/ bigdata2:/home/bigdata/apps/ 12ç„¶ååˆ†åˆ«åœ¨å„æœºå™¨ä¸Šåˆ›å»ºè½¯è¿ cd /home/bigdata/apps/kafkaln -s kafka_2.11-0.8.2.2 kafka 12# 5ã€å†æ¬¡ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼ˆé‡è¦ï¼‰ ä¾æ¬¡ä¿®æ”¹å„æœåŠ¡å™¨ä¸Šé…ç½®æ–‡ä»¶çš„çš„broker.idï¼Œåˆ†åˆ«æ˜¯0,1,2ä¸å¾—é‡å¤ã€‚12345678910# éœ€è¦é…ç½®kafkaçš„ç¯å¢ƒå˜é‡# 6ã€å¯åŠ¨é›†ç¾¤**ä¾æ¬¡åœ¨å„èŠ‚ç‚¹ä¸Šå¯åŠ¨kafka** åå°å¯åŠ¨ `nohupæœ€ååŠ ä¸€ä¸ª&amp;` ```bashKAFKA_HOME/bin/kafka-server-start.sh KAFKA_HOME/config/server.properties &amp; é…ç½®æ–‡ä»¶åŠæ³¨é‡Š123456789101112131415161718192021222324252627282930313233343536373839broker.id=0#å½“å‰æœºå™¨åœ¨é›†ç¾¤ä¸­çš„å”¯ä¸€æ ‡è¯†ï¼Œå’Œzookeeperçš„myidæ€§è´¨ä¸€æ ·port=9092#å½“å‰kafkaå¯¹å¤–æä¾›æœåŠ¡çš„ç«¯å£é»˜è®¤æ˜¯9092host.name=192.168.11.11advertised.host.name=192.168.11.11#è¿™ä¸ªå‚æ•°é»˜è®¤æ˜¯å…³é—­çš„ï¼Œåœ¨0.8.1æœ‰ä¸ªbugï¼ŒDNSè§£æé—®é¢˜ï¼Œå¤±è´¥ç‡çš„é—®é¢˜ã€‚num.network.threads=3#è¿™ä¸ªæ˜¯borkerè¿›è¡Œç½‘ç»œå¤„ç†çš„çº¿ç¨‹æ•°num.io.threads=8#è¿™ä¸ªæ˜¯borkerè¿›è¡ŒI/Oå¤„ç†çš„çº¿ç¨‹æ•°log.dirs=/home/bigdata/apps/kafka/kafkalogs/#æ¶ˆæ¯å­˜æ”¾çš„ç›®å½•ï¼Œè¿™ä¸ªç›®å½•å¯ä»¥é…ç½®ä¸ºâ€œï¼Œâ€é€—å·åˆ†å‰²çš„è¡¨è¾¾å¼ï¼Œä¸Šé¢çš„num.io.threadsè¦å¤§äºè¿™ä¸ªç›®å½•çš„ä¸ªæ•°è¿™ä¸ªç›®å½•ï¼Œå¦‚æœé…ç½®å¤šä¸ªç›®å½•ï¼Œæ–°åˆ›å»ºçš„topicä»–æŠŠæ¶ˆæ¯æŒä¹…åŒ–çš„åœ°æ–¹æ˜¯ï¼Œå½“å‰ä»¥é€—å·åˆ†å‰²çš„ç›®å½•ä¸­ï¼Œé‚£ä¸ªåˆ†åŒºæ•°æœ€å°‘å°±æ”¾é‚£ä¸€ä¸ªsocket.send.buffer.bytes=102400#å‘é€ç¼“å†²åŒºbufferå¤§å°ï¼Œæ•°æ®ä¸æ˜¯ä¸€ä¸‹å­å°±å‘é€çš„ï¼Œå…ˆå›å­˜å‚¨åˆ°ç¼“å†²åŒºäº†åˆ°è¾¾ä¸€å®šçš„å¤§å°ååœ¨å‘é€ï¼Œèƒ½æé«˜æ€§èƒ½socket.receive.buffer.bytes=102400#kafkaæ¥æ”¶ç¼“å†²åŒºå¤§å°ï¼Œå½“æ•°æ®åˆ°è¾¾ä¸€å®šå¤§å°ååœ¨åºåˆ—åŒ–åˆ°ç£ç›˜socket.request.max.bytes=104857600#è¿™ä¸ªå‚æ•°æ˜¯å‘kafkaè¯·æ±‚æ¶ˆæ¯æˆ–è€…å‘kafkaå‘é€æ¶ˆæ¯çš„è¯·è¯·æ±‚çš„æœ€å¤§æ•°ï¼Œè¿™ä¸ªå€¼ä¸èƒ½è¶…è¿‡javaçš„å †æ ˆå¤§å°num.partitions=1#é»˜è®¤çš„åˆ†åŒºæ•°ï¼Œä¸€ä¸ªtopicé»˜è®¤1ä¸ªåˆ†åŒºæ•°log.retention.hours=168#é»˜è®¤æ¶ˆæ¯çš„æœ€å¤§æŒä¹…åŒ–æ—¶é—´ï¼Œ168å°æ—¶ï¼Œ7å¤©message.max.bytes=5242880#æ¶ˆæ¯ä¿å­˜çš„æœ€å¤§å€¼5Mdefault.replication.factor=2#kafkaä¿å­˜æ¶ˆæ¯çš„å‰¯æœ¬æ•°ï¼Œå¦‚æœä¸€ä¸ªå‰¯æœ¬å¤±æ•ˆäº†ï¼Œå¦ä¸€ä¸ªè¿˜å¯ä»¥ç»§ç»­æä¾›æœåŠ¡replica.fetch.max.bytes=5242880#å–æ¶ˆæ¯çš„æœ€å¤§ç›´æ¥æ•°log.segment.bytes=1073741824#è¿™ä¸ªå‚æ•°æ˜¯ï¼šå› ä¸ºkafkaçš„æ¶ˆæ¯æ˜¯ä»¥è¿½åŠ çš„å½¢å¼è½åœ°åˆ°æ–‡ä»¶ï¼Œå½“è¶…è¿‡è¿™ä¸ªå€¼çš„æ—¶å€™ï¼Œkafkaä¼šæ–°èµ·ä¸€ä¸ªæ–‡ä»¶log.retention.check.interval.ms=300000#æ¯éš”300000æ¯«ç§’å»æ£€æŸ¥ä¸Šé¢é…ç½®çš„logå¤±æ•ˆæ—¶é—´ï¼ˆlog.retention.hours=168 ï¼‰ï¼Œåˆ°ç›®å½•æŸ¥çœ‹æ˜¯å¦æœ‰è¿‡æœŸçš„æ¶ˆæ¯å¦‚æœæœ‰ï¼Œåˆ é™¤log.cleaner.enable=false#æ˜¯å¦å¯ç”¨logå‹ç¼©ï¼Œä¸€èˆ¬ä¸ç”¨å¯ç”¨ï¼Œå¯ç”¨çš„è¯å¯ä»¥æé«˜æ€§èƒ½zookeeper.connect=192.168.11.11:2181,192.168.11.12:2181,192.168.11.13:2181#è®¾ç½®zookeeperçš„è¿æ¥ç«¯å£ å¸¸ç”¨12345678910111213141516171819202122232425broker.id =0#æ¯ä¸€ä¸ªbrokeråœ¨é›†ç¾¤ä¸­çš„å”¯ä¸€è¡¨ç¤ºï¼Œè¦æ±‚æ˜¯æ­£æ•°ã€‚å½“è¯¥æœåŠ¡å™¨çš„IPåœ°å€å‘ç”Ÿæ”¹å˜æ—¶ï¼Œbroker.idæ²¡æœ‰å˜åŒ–ï¼Œåˆ™ä¸ä¼šå½±å“consumersçš„æ¶ˆæ¯æƒ…å†µlog.dirs=/data/kafka-logs#kafkaæ•°æ®çš„å­˜æ”¾åœ°å€ï¼Œå¤šä¸ªåœ°å€çš„è¯ç”¨é€—å·åˆ†å‰² /data/kafka-logs-1ï¼Œ/data/kafka-logs-2port =9092#broker serveræœåŠ¡ç«¯å£message.max.bytes =6525000#è¡¨ç¤ºæ¶ˆæ¯ä½“çš„æœ€å¤§å¤§å°ï¼Œå•ä½æ˜¯å­—èŠ‚num.network.threads =3#brokerå¤„ç†æ¶ˆæ¯çš„æœ€å¤§çº¿ç¨‹æ•°ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ä¸éœ€è¦å»ä¿®æ”¹ é…ç½®äº†ä¸‰å°æœåŠ¡å™¨ï¼Œæ‰€ä»¥é€‰æ‹©ä¸‰ä¸ªnum.io.threads =8#brokerå¤„ç†ç£ç›˜IOçš„çº¿ç¨‹æ•°ï¼Œæ•°å€¼åº”è¯¥å¤§äºä½ çš„ç¡¬ç›˜æ•°background.threads =4#ä¸€äº›åå°ä»»åŠ¡å¤„ç†çš„çº¿ç¨‹æ•°ï¼Œä¾‹å¦‚è¿‡æœŸæ¶ˆæ¯æ–‡ä»¶çš„åˆ é™¤ç­‰ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ä¸éœ€è¦å»åšä¿®æ”¹queued.max.requests =500#ç­‰å¾…IOçº¿ç¨‹å¤„ç†çš„è¯·æ±‚é˜Ÿåˆ—æœ€å¤§æ•°ï¼Œè‹¥æ˜¯ç­‰å¾…IOçš„è¯·æ±‚è¶…è¿‡è¿™ä¸ªæ•°å€¼ï¼Œé‚£ä¹ˆä¼šåœæ­¢æ¥å—å¤–éƒ¨æ¶ˆæ¯ï¼Œåº”è¯¥æ˜¯ä¸€ç§è‡ªæˆ‘ä¿æŠ¤æœºåˆ¶ã€‚socket.send.buffer.bytes=100*1024#socketçš„å‘é€ç¼“å†²åŒºï¼Œsocketçš„è°ƒä¼˜å‚æ•°SO_SNDBUFFsocket.receive.buffer.bytes =100*1024#socketçš„æ¥å—ç¼“å†²åŒºï¼Œsocketçš„è°ƒä¼˜å‚æ•°SO_RCVBUFFsocket.request.max.bytes =100*1024*1024#socketè¯·æ±‚çš„æœ€å¤§æ•°å€¼ï¼Œé˜²æ­¢serverOOMï¼Œmessage.max.byteså¿…ç„¶è¦å°äºsocket.request.max.bytesï¼Œä¼šè¢«topicåˆ›å»ºæ—¶çš„æŒ‡å®šå‚æ•°è¦†ç›–log.segment.bytes =1024*1024*1024#topicçš„åˆ†åŒºæ˜¯ä»¥ä¸€å †segmentæ–‡ä»¶å­˜å‚¨çš„ï¼Œè¿™ä¸ªæ§åˆ¶æ¯ä¸ªsegmentçš„å¤§å°ï¼Œä¼šè¢«topicåˆ›å»ºæ—¶çš„æŒ‡å®šå‚æ•°è¦†ç›–log.roll.hours =24*7 è¯¦è§£123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114broker.id =0#æ¯ä¸€ä¸ªbrokeråœ¨é›†ç¾¤ä¸­çš„å”¯ä¸€è¡¨ç¤ºï¼Œè¦æ±‚æ˜¯æ­£æ•°ã€‚å½“è¯¥æœåŠ¡å™¨çš„IPåœ°å€å‘ç”Ÿæ”¹å˜æ—¶ï¼Œbroker.idæ²¡æœ‰å˜åŒ–ï¼Œåˆ™ä¸ä¼šå½±å“consumersçš„æ¶ˆæ¯æƒ…å†µlog.dirs=/data/kafka-logs#kafkaæ•°æ®çš„å­˜æ”¾åœ°å€ï¼Œå¤šä¸ªåœ°å€çš„è¯ç”¨é€—å·åˆ†å‰² /data/kafka-logs-1ï¼Œ/data/kafka-logs-2port =9092#broker serveræœåŠ¡ç«¯å£message.max.bytes =6525000#è¡¨ç¤ºæ¶ˆæ¯ä½“çš„æœ€å¤§å¤§å°ï¼Œå•ä½æ˜¯å­—èŠ‚num.network.threads =4#brokerå¤„ç†æ¶ˆæ¯çš„æœ€å¤§çº¿ç¨‹æ•°ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ä¸éœ€è¦å»ä¿®æ”¹num.io.threads =8#brokerå¤„ç†ç£ç›˜IOçš„çº¿ç¨‹æ•°ï¼Œæ•°å€¼åº”è¯¥å¤§äºä½ çš„ç¡¬ç›˜æ•°background.threads =4#ä¸€äº›åå°ä»»åŠ¡å¤„ç†çš„çº¿ç¨‹æ•°ï¼Œä¾‹å¦‚è¿‡æœŸæ¶ˆæ¯æ–‡ä»¶çš„åˆ é™¤ç­‰ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ä¸éœ€è¦å»åšä¿®æ”¹queued.max.requests =500#ç­‰å¾…IOçº¿ç¨‹å¤„ç†çš„è¯·æ±‚é˜Ÿåˆ—æœ€å¤§æ•°ï¼Œè‹¥æ˜¯ç­‰å¾…IOçš„è¯·æ±‚è¶…è¿‡è¿™ä¸ªæ•°å€¼ï¼Œé‚£ä¹ˆä¼šåœæ­¢æ¥å—å¤–éƒ¨æ¶ˆæ¯ï¼Œåº”è¯¥æ˜¯ä¸€ç§è‡ªæˆ‘ä¿æŠ¤æœºåˆ¶ã€‚host.name#brokerçš„ä¸»æœºåœ°å€ï¼Œè‹¥æ˜¯è®¾ç½®äº†ï¼Œé‚£ä¹ˆä¼šç»‘å®šåˆ°è¿™ä¸ªåœ°å€ä¸Šï¼Œè‹¥æ˜¯æ²¡æœ‰ï¼Œä¼šç»‘å®šåˆ°æ‰€æœ‰çš„æ¥å£ä¸Šï¼Œå¹¶å°†å…¶ä¸­ä¹‹ä¸€å‘é€åˆ°ZKï¼Œä¸€èˆ¬ä¸è®¾ç½®socket.send.buffer.bytes=100*1024#socketçš„å‘é€ç¼“å†²åŒºï¼Œsocketçš„è°ƒä¼˜å‚æ•°SO_SNDBUFFsocket.receive.buffer.bytes =100*1024#socketçš„æ¥å—ç¼“å†²åŒºï¼Œsocketçš„è°ƒä¼˜å‚æ•°SO_RCVBUFFsocket.request.max.bytes =100*1024*1024#socketè¯·æ±‚çš„æœ€å¤§æ•°å€¼ï¼Œé˜²æ­¢serverOOMï¼Œmessage.max.byteså¿…ç„¶è¦å°äºsocket.request.max.bytesï¼Œä¼šè¢«topicåˆ›å»ºæ—¶çš„æŒ‡å®šå‚æ•°è¦†ç›–log.segment.bytes =1024*1024*1024#topicçš„åˆ†åŒºæ˜¯ä»¥ä¸€å †segmentæ–‡ä»¶å­˜å‚¨çš„ï¼Œè¿™ä¸ªæ§åˆ¶æ¯ä¸ªsegmentçš„å¤§å°ï¼Œä¼šè¢«topicåˆ›å»ºæ—¶çš„æŒ‡å®šå‚æ•°è¦†ç›–log.roll.hours =24*7#è¿™ä¸ªå‚æ•°ä¼šåœ¨æ—¥å¿—segmentæ²¡æœ‰è¾¾åˆ°log.segment.bytesè®¾ç½®çš„å¤§å°ï¼Œä¹Ÿä¼šå¼ºåˆ¶æ–°å»ºä¸€ä¸ªsegmentä¼šè¢« topicåˆ›å»ºæ—¶çš„æŒ‡å®šå‚æ•°è¦†ç›–log.cleanup.policy = delete#æ—¥å¿—æ¸…ç†ç­–ç•¥é€‰æ‹©æœ‰ï¼šdeleteå’Œcompactä¸»è¦é’ˆå¯¹è¿‡æœŸæ•°æ®çš„å¤„ç†ï¼Œæˆ–æ˜¯æ—¥å¿—æ–‡ä»¶è¾¾åˆ°é™åˆ¶çš„é¢åº¦ï¼Œä¼šè¢« topicåˆ›å»ºæ—¶çš„æŒ‡å®šå‚æ•°è¦†ç›–log.retention.minutes=60*24 # ä¸€å¤©ååˆ é™¤#æ•°æ®å­˜å‚¨çš„æœ€å¤§æ—¶é—´è¶…è¿‡è¿™ä¸ªæ—¶é—´ä¼šæ ¹æ®log.cleanup.policyè®¾ç½®çš„ç­–ç•¥å¤„ç†æ•°æ®ï¼Œä¹Ÿå°±æ˜¯æ¶ˆè´¹ç«¯èƒ½å¤Ÿå¤šä¹…å»æ¶ˆè´¹æ•°æ®#log.retention.byteså’Œlog.retention.minutesä»»æ„ä¸€ä¸ªè¾¾åˆ°è¦æ±‚ï¼Œéƒ½ä¼šæ‰§è¡Œåˆ é™¤ï¼Œä¼šè¢«topicåˆ›å»ºæ—¶çš„æŒ‡å®šå‚æ•°è¦†ç›–log.retention.bytes=-1#topicæ¯ä¸ªåˆ†åŒºçš„æœ€å¤§æ–‡ä»¶å¤§å°ï¼Œä¸€ä¸ªtopicçš„å¤§å°é™åˆ¶ = åˆ†åŒºæ•°*log.retention.bytesã€‚-1æ²¡æœ‰å¤§å°é™log.retention.byteså’Œlog.retention.minutesä»»æ„ä¸€ä¸ªè¾¾åˆ°è¦æ±‚ï¼Œéƒ½ä¼šæ‰§è¡Œåˆ é™¤ï¼Œä¼šè¢«topicåˆ›å»ºæ—¶çš„æŒ‡å®šå‚æ•°è¦†ç›–log.retention.check.interval.ms=5minutes#æ–‡ä»¶å¤§å°æ£€æŸ¥çš„å‘¨æœŸæ—¶é—´ï¼Œæ˜¯å¦å¤„ç½š log.cleanup.policyä¸­è®¾ç½®çš„ç­–ç•¥log.cleaner.enable=false#æ˜¯å¦å¼€å¯æ—¥å¿—å‹ç¼©log.cleaner.threads = 2#æ—¥å¿—å‹ç¼©è¿è¡Œçš„çº¿ç¨‹æ•°log.cleaner.io.max.bytes.per.second=None#æ—¥å¿—å‹ç¼©æ—¶å€™å¤„ç†çš„æœ€å¤§å¤§å°log.cleaner.dedupe.buffer.size=500*1024*1024#æ—¥å¿—å‹ç¼©å»é‡æ—¶å€™çš„ç¼“å­˜ç©ºé—´ï¼Œåœ¨ç©ºé—´å…è®¸çš„æƒ…å†µä¸‹ï¼Œè¶Šå¤§è¶Šå¥½log.cleaner.io.buffer.size=512*1024#æ—¥å¿—æ¸…ç†æ—¶å€™ç”¨åˆ°çš„IOå—å¤§å°ä¸€èˆ¬ä¸éœ€è¦ä¿®æ”¹log.cleaner.io.buffer.load.factor =0.9#æ—¥å¿—æ¸…ç†ä¸­hashè¡¨çš„æ‰©å¤§å› å­ä¸€èˆ¬ä¸éœ€è¦ä¿®æ”¹log.cleaner.backoff.ms =15000#æ£€æŸ¥æ˜¯å¦å¤„ç½šæ—¥å¿—æ¸…ç†çš„é—´éš”log.cleaner.min.cleanable.ratio=0.5#æ—¥å¿—æ¸…ç†çš„é¢‘ç‡æ§åˆ¶ï¼Œè¶Šå¤§æ„å‘³ç€æ›´é«˜æ•ˆçš„æ¸…ç†ï¼ŒåŒæ—¶ä¼šå­˜åœ¨ä¸€äº›ç©ºé—´ä¸Šçš„æµªè´¹ï¼Œä¼šè¢«topicåˆ›å»ºæ—¶çš„æŒ‡å®šå‚æ•°è¦†ç›–log.cleaner.delete.retention.ms =1day#å¯¹äºå‹ç¼©çš„æ—¥å¿—ä¿ç•™çš„æœ€é•¿æ—¶é—´ï¼Œä¹Ÿæ˜¯å®¢æˆ·ç«¯æ¶ˆè´¹æ¶ˆæ¯çš„æœ€é•¿æ—¶é—´ï¼ŒåŒlog.retention.minutesçš„åŒºåˆ«åœ¨äºä¸€ä¸ªæ§åˆ¶æœªå‹ç¼©æ•°æ®ï¼Œä¸€ä¸ªæ§åˆ¶å‹ç¼©åçš„æ•°æ®ã€‚ä¼šè¢«topicåˆ›å»ºæ—¶çš„æŒ‡å®šå‚æ•°è¦†ç›–log.index.size.max.bytes =10*1024*1024#å¯¹äºsegmentæ—¥å¿—çš„ç´¢å¼•æ–‡ä»¶å¤§å°é™åˆ¶ï¼Œä¼šè¢«topicåˆ›å»ºæ—¶çš„æŒ‡å®šå‚æ•°è¦†ç›–log.index.interval.bytes =4096#å½“æ‰§è¡Œä¸€ä¸ªfetchæ“ä½œåï¼Œéœ€è¦ä¸€å®šçš„ç©ºé—´æ¥æ‰«ææœ€è¿‘çš„offsetå¤§å°ï¼Œè®¾ç½®è¶Šå¤§ï¼Œä»£è¡¨æ‰«æé€Ÿåº¦è¶Šå¿«ï¼Œä½†æ˜¯ä¹Ÿæ›´å¥½å†…å­˜ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ä¸éœ€è¦æ­ç†è¿™ä¸ªå‚æ•°log.flush.interval.messages=None#logæ–‡ä»¶â€syncâ€åˆ°ç£ç›˜ä¹‹å‰ç´¯ç§¯çš„æ¶ˆæ¯æ¡æ•°,å› ä¸ºç£ç›˜IOæ“ä½œæ˜¯ä¸€ä¸ªæ…¢æ“ä½œ,ä½†åˆæ˜¯ä¸€ä¸ªâ€æ•°æ®å¯é æ€§&quot;çš„å¿…è¦æ‰‹æ®µ,æ‰€ä»¥æ­¤å‚æ•°çš„è®¾ç½®,éœ€è¦åœ¨&quot;æ•°æ®å¯é æ€§&quot;ä¸&quot;æ€§èƒ½&quot;ä¹‹é—´åšå¿…è¦çš„æƒè¡¡.å¦‚æœæ­¤å€¼è¿‡å¤§,å°†ä¼šå¯¼è‡´æ¯æ¬¡&quot;fsync&quot;çš„æ—¶é—´è¾ƒé•¿(IOé˜»å¡),å¦‚æœæ­¤å€¼è¿‡å°,å°†ä¼šå¯¼è‡´&quot;fsync&quot;çš„æ¬¡æ•°è¾ƒå¤š,è¿™ä¹Ÿæ„å‘³ç€æ•´ä½“çš„clientè¯·æ±‚æœ‰ä¸€å®šçš„å»¶è¿Ÿ.ç‰©ç†serveræ•…éšœ,å°†ä¼šå¯¼è‡´æ²¡æœ‰fsyncçš„æ¶ˆæ¯ä¸¢å¤±.log.flush.scheduler.interval.ms =3000#æ£€æŸ¥æ˜¯å¦éœ€è¦å›ºåŒ–åˆ°ç¡¬ç›˜çš„æ—¶é—´é—´éš”log.flush.interval.ms = None#ä»…ä»…é€šè¿‡intervalæ¥æ§åˆ¶æ¶ˆæ¯çš„ç£ç›˜å†™å…¥æ—¶æœº,æ˜¯ä¸è¶³çš„.æ­¤å‚æ•°ç”¨äºæ§åˆ¶&quot;fsync&quot;çš„æ—¶é—´é—´éš”,å¦‚æœæ¶ˆæ¯é‡å§‹ç»ˆæ²¡æœ‰è¾¾åˆ°é˜€å€¼,ä½†æ˜¯ç¦»ä¸Šä¸€æ¬¡ç£ç›˜åŒæ­¥çš„æ—¶é—´é—´éš”è¾¾åˆ°é˜€å€¼,ä¹Ÿå°†è§¦å‘.log.delete.delay.ms =60000#æ–‡ä»¶åœ¨ç´¢å¼•ä¸­æ¸…é™¤åä¿ç•™çš„æ—¶é—´ä¸€èˆ¬ä¸éœ€è¦å»ä¿®æ”¹log.flush.offset.checkpoint.interval.ms =60000#æ§åˆ¶ä¸Šæ¬¡å›ºåŒ–ç¡¬ç›˜çš„æ—¶é—´ç‚¹ï¼Œä»¥ä¾¿äºæ•°æ®æ¢å¤ä¸€èˆ¬ä¸éœ€è¦å»ä¿®æ”¹auto.create.topics.enable =true#æ˜¯å¦å…è®¸è‡ªåŠ¨åˆ›å»ºtopicï¼Œè‹¥æ˜¯falseï¼Œå°±éœ€è¦é€šè¿‡å‘½ä»¤åˆ›å»ºtopicdefault.replication.factor =1#æ˜¯å¦å…è®¸è‡ªåŠ¨åˆ›å»ºtopicï¼Œè‹¥æ˜¯falseï¼Œå°±éœ€è¦é€šè¿‡å‘½ä»¤åˆ›å»ºtopicnum.partitions =1#æ¯ä¸ªtopicçš„åˆ†åŒºä¸ªæ•°ï¼Œè‹¥æ˜¯åœ¨topicåˆ›å»ºæ—¶å€™æ²¡æœ‰æŒ‡å®šçš„è¯ä¼šè¢«topicåˆ›å»ºæ—¶çš„æŒ‡å®šå‚æ•°è¦†ç›–##è¿™æ˜¯è½»é‡çº§çš„é…ç½®æ–‡ä»¶broker.id=0#å½“å‰æœºå™¨åœ¨é›†ç¾¤ä¸­çš„å”¯ä¸€æ ‡è¯†ï¼Œå’Œzookeeperçš„myidæ€§è´¨ä¸€æ ·port=9092#å½“å‰kafkaå¯¹å¤–æä¾›æœåŠ¡çš„ç«¯å£é»˜è®¤æ˜¯9092host.name=192.168.11.11#è¿™ä¸ªå‚æ•°é»˜è®¤æ˜¯å…³é—­çš„ï¼Œåœ¨0.8.1æœ‰ä¸ªbugï¼ŒDNSè§£æé—®é¢˜ï¼Œå¤±è´¥ç‡çš„é—®é¢˜ã€‚num.network.threads=3#è¿™ä¸ªæ˜¯borkerè¿›è¡Œç½‘ç»œå¤„ç†çš„çº¿ç¨‹æ•°num.io.threads=8#è¿™ä¸ªæ˜¯borkerè¿›è¡ŒI/Oå¤„ç†çš„çº¿ç¨‹æ•°log.dirs=/home/hadoop/logs/kafka-logs #æ¶ˆæ¯å­˜æ”¾çš„ç›®å½•ï¼Œè¿™ä¸ªç›®å½•å¯ä»¥é…ç½®ä¸ºâ€œï¼Œâ€é€—å·åˆ†å‰²çš„è¡¨è¾¾å¼ï¼Œä¸Šé¢çš„num.io.threadsè¦å¤§äºè¿™ä¸ªç›®å½•çš„ä¸ªæ•°è¿™ä¸ªç›®å½•ï¼Œå¦‚æœé…ç½®å¤šä¸ªç›®å½•ï¼Œæ–°åˆ›å»ºçš„topicä»–æŠŠæ¶ˆæ¯æŒä¹…åŒ–çš„åœ°æ–¹æ˜¯ï¼Œå½“å‰ä»¥é€—å·åˆ†å‰²çš„ç›®å½•ä¸­ï¼Œé‚£ä¸ªåˆ†åŒºæ•°æœ€å°‘å°±æ”¾é‚£ä¸€ä¸ªsocket.send.buffer.bytes=102400#å‘é€ç¼“å†²åŒºbufferå¤§å°ï¼Œæ•°æ®ä¸æ˜¯ä¸€ä¸‹å­å°±å‘é€çš„ï¼Œå…ˆå›å­˜å‚¨åˆ°ç¼“å†²åŒºäº†åˆ°è¾¾ä¸€å®šçš„å¤§å°ååœ¨å‘é€ï¼Œèƒ½æé«˜æ€§èƒ½socket.receive.buffer.bytes=102400#kafkaæ¥æ”¶ç¼“å†²åŒºå¤§å°ï¼Œå½“æ•°æ®åˆ°è¾¾ä¸€å®šå¤§å°ååœ¨åºåˆ—åŒ–åˆ°ç£ç›˜socket.request.max.bytes=104857600#è¿™ä¸ªå‚æ•°æ˜¯å‘kafkaè¯·æ±‚æ¶ˆæ¯æˆ–è€…å‘kafkaå‘é€æ¶ˆæ¯çš„è¯·è¯·æ±‚çš„æœ€å¤§æ•°ï¼Œè¿™ä¸ªå€¼ä¸èƒ½è¶…è¿‡javaçš„å †æ ˆå¤§å°num.partitions=1#é»˜è®¤çš„åˆ†åŒºæ•°ï¼Œä¸€ä¸ªtopicé»˜è®¤1ä¸ªåˆ†åŒºæ•°log.retention.hours=168#é»˜è®¤æ¶ˆæ¯çš„æœ€å¤§æŒä¹…åŒ–æ—¶é—´ï¼Œ168å°æ—¶ï¼Œ7å¤©message.max.byte=5242880#æ¶ˆæ¯ä¿å­˜çš„æœ€å¤§å€¼5Mdefault.replication.factor=2#kafkaä¿å­˜æ¶ˆæ¯çš„å‰¯æœ¬æ•°ï¼Œå¦‚æœä¸€ä¸ªå‰¯æœ¬å¤±æ•ˆäº†ï¼Œå¦ä¸€ä¸ªè¿˜å¯ä»¥ç»§ç»­æä¾›æœåŠ¡replica.fetch.max.bytes=5242880#å–æ¶ˆæ¯çš„æœ€å¤§ç›´æ¥æ•°log.segment.bytes=1073741824#è¿™ä¸ªå‚æ•°æ˜¯ï¼šå› ä¸ºkafkaçš„æ¶ˆæ¯æ˜¯ä»¥è¿½åŠ çš„å½¢å¼è½åœ°åˆ°æ–‡ä»¶ï¼Œå½“è¶…è¿‡è¿™ä¸ªå€¼çš„æ—¶å€™ï¼Œkafkaä¼šæ–°èµ·ä¸€ä¸ªæ–‡ä»¶log.retention.check.interval.ms=300000#æ¯éš”300000æ¯«ç§’å»æ£€æŸ¥ä¸Šé¢é…ç½®çš„logå¤±æ•ˆæ—¶é—´ï¼ˆlog.retention.hours=168 ï¼‰ï¼Œåˆ°ç›®å½•æŸ¥çœ‹æ˜¯å¦æœ‰è¿‡æœŸçš„æ¶ˆæ¯å¦‚æœæœ‰ï¼Œåˆ é™¤log.cleaner.enable=false#æ˜¯å¦å¯ç”¨logå‹ç¼©ï¼Œä¸€èˆ¬ä¸ç”¨å¯ç”¨ï¼Œå¯ç”¨çš„è¯å¯ä»¥æé«˜æ€§èƒ½zookeeper.connect=192.168.11.11:2181,192.168.11.12:2181,192.168.11.13:2181#è®¾ç½®zookeeperçš„è¿æ¥ç«¯å£ ä»¥ä¸‹æ˜¯kafkaä¸­Leader,replicasé…ç½®å‚æ•°1234567891011121314151617181920212223242526272829303132333435363738controller.socket.timeout.ms =30000#partition leaderä¸replicasä¹‹é—´é€šè®¯æ—¶,socketçš„è¶…æ—¶æ—¶é—´controller.message.queue.size=10#partition leaderä¸replicasæ•°æ®åŒæ­¥æ—¶,æ¶ˆæ¯çš„é˜Ÿåˆ—å°ºå¯¸replica.lag.time.max.ms =10000#replicaså“åº”partition leaderçš„æœ€é•¿ç­‰å¾…æ—¶é—´ï¼Œè‹¥æ˜¯è¶…è¿‡è¿™ä¸ªæ—¶é—´ï¼Œå°±å°†replicasåˆ—å…¥ISR(in-sync replicas)ï¼Œå¹¶è®¤ä¸ºå®ƒæ˜¯æ­»çš„ï¼Œä¸ä¼šå†åŠ å…¥ç®¡ç†ä¸­replica.lag.max.messages =4000#å¦‚æœfollowerè½åä¸leaderå¤ªå¤š,å°†ä¼šè®¤ä¸ºæ­¤follower[æˆ–è€…è¯´partition relicas]å·²ç»å¤±æ•ˆ###é€šå¸¸,åœ¨followerä¸leaderé€šè®¯æ—¶,å› ä¸ºç½‘ç»œå»¶è¿Ÿæˆ–è€…é“¾æ¥æ–­å¼€,æ€»ä¼šå¯¼è‡´replicasä¸­æ¶ˆæ¯åŒæ­¥æ»å##å¦‚æœæ¶ˆæ¯ä¹‹åå¤ªå¤š,leaderå°†è®¤ä¸ºæ­¤followerç½‘ç»œå»¶è¿Ÿè¾ƒå¤§æˆ–è€…æ¶ˆæ¯ååèƒ½åŠ›æœ‰é™,å°†ä¼šæŠŠæ­¤replicasè¿ç§»##åˆ°å…¶ä»–followerä¸­.##åœ¨brokeræ•°é‡è¾ƒå°‘,æˆ–è€…ç½‘ç»œä¸è¶³çš„ç¯å¢ƒä¸­,å»ºè®®æé«˜æ­¤å€¼.replica.socket.timeout.ms=30*1000#followerä¸leaderä¹‹é—´çš„socketè¶…æ—¶æ—¶é—´replica.socket.receive.buffer.bytes=64*1024#leaderå¤åˆ¶æ—¶å€™çš„socketç¼“å­˜å¤§å°replica.fetch.max.bytes =1024*1024#replicasæ¯æ¬¡è·å–æ•°æ®çš„æœ€å¤§å¤§å°replica.fetch.wait.max.ms =500#replicasåŒleaderä¹‹é—´é€šä¿¡çš„æœ€å¤§ç­‰å¾…æ—¶é—´ï¼Œå¤±è´¥äº†ä¼šé‡è¯•replica.fetch.min.bytes =1#fetchçš„æœ€å°æ•°æ®å°ºå¯¸,å¦‚æœleaderä¸­å°šæœªåŒæ­¥çš„æ•°æ®ä¸è¶³æ­¤å€¼,å°†ä¼šé˜»å¡,ç›´åˆ°æ»¡è¶³æ¡ä»¶num.replica.fetchers=1#leaderè¿›è¡Œå¤åˆ¶çš„çº¿ç¨‹æ•°ï¼Œå¢å¤§è¿™ä¸ªæ•°å€¼ä¼šå¢åŠ followerçš„IOreplica.high.watermark.checkpoint.interval.ms =5000#æ¯ä¸ªreplicaæ£€æŸ¥æ˜¯å¦å°†æœ€é«˜æ°´ä½è¿›è¡Œå›ºåŒ–çš„é¢‘ç‡controlled.shutdown.enable =false#æ˜¯å¦å…è®¸æ§åˆ¶å™¨å…³é—­broker ,è‹¥æ˜¯è®¾ç½®ä¸ºtrue,ä¼šå…³é—­æ‰€æœ‰åœ¨è¿™ä¸ªbrokerä¸Šçš„leaderï¼Œå¹¶è½¬ç§»åˆ°å…¶ä»–brokercontrolled.shutdown.max.retries =3#æ§åˆ¶å™¨å…³é—­çš„å°è¯•æ¬¡æ•°controlled.shutdown.retry.backoff.ms =5000#æ¯æ¬¡å…³é—­å°è¯•çš„æ—¶é—´é—´éš”leader.imbalance.per.broker.percentage =10#leaderçš„ä¸å¹³è¡¡æ¯”ä¾‹ï¼Œè‹¥æ˜¯è¶…è¿‡è¿™ä¸ªæ•°å€¼ï¼Œä¼šå¯¹åˆ†åŒºè¿›è¡Œé‡æ–°çš„å¹³è¡¡leader.imbalance.check.interval.seconds =300#æ£€æŸ¥leaderæ˜¯å¦ä¸å¹³è¡¡çš„æ—¶é—´é—´éš”offset.metadata.max.bytes#å®¢æˆ·ç«¯ä¿ç•™offsetä¿¡æ¯çš„æœ€å¤§ç©ºé—´å¤§å° kafkaä¸­zookeeperå‚æ•°é…ç½®1234567zookeeper.connect = bigdata1:2181,bigdata2:2181,bigdata3:2181#zookeeperé›†ç¾¤çš„åœ°å€ï¼Œå¯ä»¥æ˜¯å¤šä¸ªï¼Œå¤šä¸ªä¹‹é—´ç”¨é€—å·åˆ†å‰² hostname1:port1,hostname2:port2,hostname3:port3zookeeper.session.timeout.ms=6000#ZooKeeperçš„æœ€å¤§è¶…æ—¶æ—¶é—´ï¼Œå°±æ˜¯å¿ƒè·³çš„é—´éš”ï¼Œè‹¥æ˜¯æ²¡æœ‰åæ˜ ï¼Œé‚£ä¹ˆè®¤ä¸ºå·²ç»æ­»äº†ï¼Œä¸æ˜“è¿‡å¤§zookeeper.connection.timeout.ms =6000#ZooKeeperçš„è¿æ¥è¶…æ—¶æ—¶é—´zookeeper.sync.time.ms =2000","categories":[{"name":"å®‰è£…éƒ¨ç½²","slug":"å®‰è£…éƒ¨ç½²","permalink":"http://gangtieguo.cn/categories/å®‰è£…éƒ¨ç½²/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://gangtieguo.cn/tags/Kafka/"},{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/tags/å¤§æ•°æ®/"}]},{"title":"Kafkaåˆæ­¥æ€»ç»“","slug":"Kafkaå°çŸ¥è¯†ç‚¹","date":"2018-08-13T06:50:40.913Z","updated":"2019-07-03T08:13:07.174Z","comments":true,"path":"2018/08/13/Kafkaå°çŸ¥è¯†ç‚¹/","link":"","permalink":"http://gangtieguo.cn/2018/08/13/Kafkaå°çŸ¥è¯†ç‚¹/","excerpt":"[TOC] Kafkaæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼æ¶ˆæ¯é˜Ÿåˆ—ï¼šç”Ÿäº§è€…ã€æ¶ˆè´¹è€…çš„åŠŸèƒ½ã€‚å®ƒæä¾›äº†ç±»ä¼¼äºJMSçš„ç‰¹æ€§ï¼Œä½†æ˜¯åœ¨è®¾è®¡å®ç°ä¸Šå®Œå…¨ä¸åŒï¼Œæ­¤å¤–å®ƒå¹¶ä¸æ˜¯JMSè§„èŒƒçš„å®ç°ã€‚ Kafkaå¯¹æ¶ˆæ¯ä¿å­˜æ—¶æ ¹æ®Topicè¿›è¡Œå½’ç±»ï¼Œå‘é€æ¶ˆæ¯è€…ç§°ä¸ºProducer,æ¶ˆæ¯æ¥å—è€…ç§°ä¸ºConsumer,æ­¤å¤–kafkaé›†ç¾¤æœ‰å¤šä¸ªkafkaå®ä¾‹ç»„æˆï¼Œæ¯ä¸ªå®ä¾‹(server)æˆä¸ºbrokerã€‚ æ— è®ºæ˜¯kafkaé›†ç¾¤ï¼Œè¿˜æ˜¯producerå’Œconsumeréƒ½ä¾èµ–äºzookeeperé›†ç¾¤ä¿å­˜ä¸€äº›metaä¿¡æ¯ï¼Œæ¥ä¿è¯ç³»ç»Ÿå¯ç”¨æ€§ JMSçš„åŸºç¡€JMSæ˜¯ä»€ä¹ˆï¼Ÿï¼šJMSæ˜¯Javaæä¾›çš„ä¸€å¥—æŠ€æœ¯è§„èŒƒ JMSåšä»€ä¹ˆï¼Ÿï¼šç”¨æ¥å¼‚æ„ç³»ç»Ÿ é›†æˆé€šä¿¡ï¼Œç¼“è§£ç³»ç»Ÿç“¶é¢ˆï¼Œæé«˜ç³»ç»Ÿçš„ä¼¸ç¼©æ€§å¢å¼ºç³»ç»Ÿç”¨æˆ·ä½“éªŒï¼Œä½¿å¾—ç³»ç»Ÿæ¨¡å—åŒ–å’Œç»„ä»¶åŒ–å˜å¾—å¯è¡Œå¹¶æ›´åŠ çµæ´» JMSé€šè¿‡ä»€ä¹ˆæ–¹å¼ï¼šç”Ÿäº§æ¶ˆè´¹è€…æ¨¡å¼ï¼ˆç”Ÿäº§è€…ã€æœåŠ¡å™¨ã€æ¶ˆè´¹è€…ï¼‰ jdkï¼Œkafkaï¼Œactivemqâ€¦â€¦","text":"[TOC] Kafkaæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼æ¶ˆæ¯é˜Ÿåˆ—ï¼šç”Ÿäº§è€…ã€æ¶ˆè´¹è€…çš„åŠŸèƒ½ã€‚å®ƒæä¾›äº†ç±»ä¼¼äºJMSçš„ç‰¹æ€§ï¼Œä½†æ˜¯åœ¨è®¾è®¡å®ç°ä¸Šå®Œå…¨ä¸åŒï¼Œæ­¤å¤–å®ƒå¹¶ä¸æ˜¯JMSè§„èŒƒçš„å®ç°ã€‚ Kafkaå¯¹æ¶ˆæ¯ä¿å­˜æ—¶æ ¹æ®Topicè¿›è¡Œå½’ç±»ï¼Œå‘é€æ¶ˆæ¯è€…ç§°ä¸ºProducer,æ¶ˆæ¯æ¥å—è€…ç§°ä¸ºConsumer,æ­¤å¤–kafkaé›†ç¾¤æœ‰å¤šä¸ªkafkaå®ä¾‹ç»„æˆï¼Œæ¯ä¸ªå®ä¾‹(server)æˆä¸ºbrokerã€‚ æ— è®ºæ˜¯kafkaé›†ç¾¤ï¼Œè¿˜æ˜¯producerå’Œconsumeréƒ½ä¾èµ–äºzookeeperé›†ç¾¤ä¿å­˜ä¸€äº›metaä¿¡æ¯ï¼Œæ¥ä¿è¯ç³»ç»Ÿå¯ç”¨æ€§ JMSçš„åŸºç¡€JMSæ˜¯ä»€ä¹ˆï¼Ÿï¼šJMSæ˜¯Javaæä¾›çš„ä¸€å¥—æŠ€æœ¯è§„èŒƒ JMSåšä»€ä¹ˆï¼Ÿï¼šç”¨æ¥å¼‚æ„ç³»ç»Ÿ é›†æˆé€šä¿¡ï¼Œç¼“è§£ç³»ç»Ÿç“¶é¢ˆï¼Œæé«˜ç³»ç»Ÿçš„ä¼¸ç¼©æ€§å¢å¼ºç³»ç»Ÿç”¨æˆ·ä½“éªŒï¼Œä½¿å¾—ç³»ç»Ÿæ¨¡å—åŒ–å’Œç»„ä»¶åŒ–å˜å¾—å¯è¡Œå¹¶æ›´åŠ çµæ´» JMSé€šè¿‡ä»€ä¹ˆæ–¹å¼ï¼šç”Ÿäº§æ¶ˆè´¹è€…æ¨¡å¼ï¼ˆç”Ÿäº§è€…ã€æœåŠ¡å™¨ã€æ¶ˆè´¹è€…ï¼‰ jdkï¼Œkafkaï¼Œactivemqâ€¦â€¦ JMSæ¶ˆæ¯ä¼ è¾“æ¨¡å‹ç‚¹å¯¹ç‚¹æ¨¡å¼ï¼ˆä¸€å¯¹ä¸€ï¼Œæ¶ˆè´¹è€…ä¸»åŠ¨æ‹‰å–æ•°æ®ï¼Œæ¶ˆæ¯æ”¶åˆ°åæ¶ˆæ¯æ¸…é™¤ï¼‰ç‚¹å¯¹ç‚¹æ¨¡å‹é€šå¸¸æ˜¯ä¸€ä¸ªåŸºäºæ‹‰å–æˆ–è€…è½®è¯¢çš„æ¶ˆæ¯ä¼ é€æ¨¡å‹ï¼Œè¿™ç§æ¨¡å‹ä»é˜Ÿåˆ—ä¸­è¯·æ±‚ä¿¡æ¯ï¼Œè€Œä¸æ˜¯å°†æ¶ˆæ¯æ¨é€åˆ°å®¢æˆ·ç«¯ã€‚è¿™ä¸ªæ¨¡å‹çš„ç‰¹ç‚¹æ˜¯å‘é€åˆ°é˜Ÿåˆ—çš„æ¶ˆæ¯è¢«ä¸€ä¸ªä¸”åªæœ‰ä¸€ä¸ªæ¥æ”¶è€…æ¥æ”¶å¤„ç†ï¼Œå³ä½¿æœ‰å¤šä¸ªæ¶ˆæ¯ç›‘å¬è€…ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ å‘å¸ƒ/è®¢é˜…æ¨¡å¼ï¼ˆä¸€å¯¹å¤šï¼Œæ•°æ®ç”Ÿäº§åï¼Œæ¨é€ç»™æ‰€æœ‰è®¢é˜…è€…ï¼‰å‘å¸ƒè®¢é˜…æ¨¡å‹åˆ™æ˜¯ä¸€ä¸ªåŸºäºæ¨é€çš„æ¶ˆæ¯ä¼ é€æ¨¡å‹ã€‚å‘å¸ƒè®¢é˜…æ¨¡å‹å¯ä»¥æœ‰å¤šç§ä¸åŒçš„è®¢é˜…è€…ï¼Œä¸´æ—¶è®¢é˜…è€…åªåœ¨ä¸»åŠ¨ç›‘å¬ä¸»é¢˜æ—¶æ‰æ¥æ”¶æ¶ˆæ¯ï¼Œè€ŒæŒä¹…è®¢é˜…è€…åˆ™ç›‘å¬ä¸»é¢˜çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå³æ—¶å½“å‰è®¢é˜…è€…ä¸å¯ç”¨ï¼Œå¤„äºç¦»çº¿çŠ¶æ€ã€‚ queue.putï¼ˆobjectï¼‰ æ•°æ®ç”Ÿäº§ queue.take(object) æ•°æ®æ¶ˆè´¹ kafkaæ˜¯é‡‡ç”¨çš„ç±»jmsæ¨¡å¼ï¼Œä¸ç±»jmsæ¨¡å¼åŒºåˆ«æ˜¯ï¼š jmsä¸¤ç§æ¨¡å¼ï¼š æ¨é€çš„è¯å¯ä»¥å¤šä¸ª æ‹‰å–çš„è¯åªèƒ½ä¸€ä¸ªæ¶ˆè´¹è€…ï¼Œå› ä¸ºæ¶ˆè´¹å®Œï¼Œæ¶ˆæ¯æ•°æ®å°±ä¼šä¸å­˜åœ¨äº†ã€‚kafkaè§£å†³äº†è¿™ç§å¼Šç«¯ï¼Œæ‹‰å–æ¨¡å¼ä¸‹ä¹Ÿå¯ä»¥å¤šä¸ªæ¶ˆè´¹è€…ï¼Œå› ä¸ºæ¶ˆæ¯å¯ä»¥æŒä¹…åŒ–åˆ°ç¡¬ç›˜ï¼Œå°±ç®—æ¶ˆè´¹äº†ä¹Ÿæ˜¯å­˜åœ¨çš„ã€‚Kafkaä¸­ackæœºåˆ¶ä¹Ÿå¯ä»¥ä¿è¯æ¶ˆæ¯å®Œæ•´è¢«å¤„ç†ã€‚ kafkaæ˜¯ä»€ä¹ˆç±»JMSæ¶ˆæ¯é˜Ÿåˆ—ï¼Œç»“åˆJMSä¸­çš„ä¸¤ç§æ¨¡å¼ï¼Œå¯ä»¥æœ‰å¤šä¸ªæ¶ˆè´¹è€…ä¸»åŠ¨æ‹‰å–æ•°æ®ï¼Œåœ¨JMSä¸­åªæœ‰ç‚¹å¯¹ç‚¹æ¨¡å¼æ‰æœ‰æ¶ˆè´¹è€…ä¸»åŠ¨æ‹‰å–æ•°æ®ã€‚kafkaæ˜¯ä¸€ä¸ªç”Ÿäº§-æ¶ˆè´¹æ¨¡å‹ã€‚ Producerï¼šç”Ÿäº§è€…ï¼Œåªè´Ÿè´£æ•°æ®ç”Ÿäº§ï¼Œç”Ÿäº§è€…çš„ä»£ç å¯ä»¥é›†æˆåˆ°ä»»åŠ¡ç³»ç»Ÿä¸­ã€‚ æ•°æ®çš„åˆ†å‘ç­–ç•¥ç”±producerå†³å®šï¼Œé»˜è®¤æ˜¯defaultPartitionï¼šç­–ç•¥ä¸ºUtils.abs(key.hashCode) % numPartitions Brokerï¼šå½“å‰æœåŠ¡å™¨ä¸Šçš„Kafkaè¿›ç¨‹ã€‚åªç®¡æ•°æ®å­˜å‚¨ï¼Œä¸ç®¡æ˜¯è°ç”Ÿäº§ï¼Œä¸ç®¡æ˜¯è°æ¶ˆè´¹ã€‚ åœ¨é›†ç¾¤ä¸­æ¯ä¸ªbrokeréƒ½æœ‰ä¸€ä¸ªå”¯ä¸€brokeridï¼Œä¸å¾—é‡å¤ã€‚ Topic: 1. ç›®æ ‡å‘é€çš„ç›®çš„åœ°ï¼Œè¿™æ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„æ¦‚å¿µï¼Œè½åˆ°ç£ç›˜ä¸Šæ˜¯ä¸€ä¸ªpartitionçš„ç›®å½•ã€‚partitionçš„ç›®å½•ä¸­æœ‰å¤šä¸ªsegmentç»„åˆ(index,log) 2. ä¸€ä¸ªTopicå¯¹åº”å¤šä¸ªpartition[0,1,2,3]ï¼Œä¸€ä¸ªpartitionå¯¹åº”å¤šä¸ªsegmentç»„åˆã€‚ä¸€ä¸ªsegmentæœ‰é»˜è®¤çš„å¤§å°æ˜¯1Gã€‚ 3. æ¯ä¸ªpartitionå¯ä»¥è®¾ç½®å¤šä¸ªå‰¯æœ¬(replication-factor 1),ä¼šä»æ‰€æœ‰çš„å‰¯æœ¬ä¸­é€‰å–ä¸€ä¸ªleaderå‡ºæ¥ã€‚æ‰€æœ‰è¯»å†™æ“ä½œéƒ½æ˜¯é€šè¿‡leaderæ¥è¿›è¡Œçš„ã€‚ ConsumerGroupï¼šæ•°æ®æ¶ˆè´¹è€…ç»„ï¼ŒConsumerGroupå¯ä»¥æœ‰å¤šä¸ªï¼Œæ¯ä¸ªConsumerGroupæ¶ˆè´¹çš„æ•°æ®éƒ½æ˜¯ä¸€æ ·çš„ã€‚å¯ä»¥æŠŠå¤šä¸ªconsumerçº¿ç¨‹åˆ’åˆ†ä¸ºä¸€ä¸ªç»„ï¼Œç»„é‡Œé¢æ‰€æœ‰æˆå‘˜å…±åŒæ¶ˆè´¹ä¸€ä¸ªtopicçš„æ•°æ®ï¼Œç»„å‘˜ä¹‹é—´ä¸èƒ½é‡å¤æ¶ˆè´¹ã€‚ ç‰¹åˆ«å¼ºè°ƒï¼Œå’Œmysqlä¸­ä¸»ä»æœ‰åŒºåˆ«ï¼Œmysqlåšä¸»ä»æ˜¯ä¸ºäº†è¯»å†™åˆ†ç¦»ï¼Œåœ¨kafkaä¸­è¯»å†™æ“ä½œéƒ½æ˜¯leaderã€‚ Kafkaæ ¸å¿ƒç»„ä»¶Topic ï¼šæ¶ˆæ¯æ ¹æ®Topicè¿›è¡Œå½’ç±»Producerï¼šå‘é€æ¶ˆæ¯è€…Consumerï¼šæ¶ˆæ¯æ¥å—è€…brokerï¼šæ¯ä¸ªkafkaå®ä¾‹(server)Zookeeperï¼šä¾èµ–é›†ç¾¤ä¿å­˜metaä¿¡æ¯ã€‚ ä¸ºä»€ä¹ˆéœ€è¦æ¶ˆæ¯é˜Ÿåˆ—æ¶ˆæ¯ç³»ç»Ÿçš„æ ¸å¿ƒä½œç”¨å°±æ˜¯ä¸‰ç‚¹ï¼šè§£è€¦ï¼Œå¼‚æ­¥å’Œå¹¶è¡Œ æ¶ˆæ¯é˜Ÿåˆ—å’Œrpcè°ƒç”¨åŒºåˆ«æ¶ˆæ¯é˜Ÿåˆ—å¹¶ä¸å…³å¿ƒæ˜¯å“ªä¸ªæ¶ˆè´¹è€…æ¶ˆè´¹äº†æ•°æ®ï¼Œå‘å¸ƒæˆåŠŸåå°±ä¸å¿…ç®¡æ¶ˆæ¯é˜Ÿåˆ—çš„å†…å®¹æ˜¯å¦è¢«æ¶ˆè´¹ï¼Œä½†æ˜¯rpcè°ƒç”¨çš„è¯ï¼Œå¿…é¡»è¦ç»™è°ƒç”¨çš„ç³»ç»Ÿè¿”å›ä¸€ä¸ªçŠ¶æ€ç  ä»¥ä¸‹ä¸ºé’ˆå¯¹Kafkaçš„ä¸€äº›æ€»ç»“ kafkaç”Ÿäº§æ•°æ®æ—¶çš„åˆ†ç»„ç­–ç•¥é»˜è®¤æ˜¯defaultPartition Utils.abs(key.hashCode) % numPartitions ä¸Šæ–‡ä¸­çš„keyæ˜¯produceråœ¨å‘é€æ•°æ®æ—¶ä¼ å…¥çš„ï¼Œproduer.send(KeyedMessage(topic,myPartitionKey,messageContent)) kafkaå¦‚ä½•ä¿è¯æ•°æ®çš„å®Œå…¨ç”Ÿäº§ ackæœºåˆ¶ï¼šbrokerè¡¨ç¤ºå‘æ¥çš„æ•°æ®å·²ç¡®è®¤æ¥æ”¶æ— è¯¯ï¼Œè¡¨ç¤ºæ•°æ®å·²ç»ä¿å­˜åˆ°ç£ç›˜ã€‚æœ‰ä¸‰ä¸ªé€‰é¡¹ 0ï¼šä¸ç­‰å¾…brokerè¿”å›ç¡®è®¤æ¶ˆæ¯ 1ï¼šç­‰å¾…topicä¸­æŸä¸ªpartition leaderä¿å­˜æˆåŠŸçš„çŠ¶æ€åé¦ˆ -1ï¼šç­‰å¾…topicä¸­æŸä¸ªpartition æ‰€æœ‰å‰¯æœ¬éƒ½ä¿å­˜æˆåŠŸçš„çŠ¶æ€åé¦ˆ brokerå¦‚ä½•ä¿å­˜æ•°æ®åœ¨ç†è®ºç¯å¢ƒä¸‹ï¼ŒbrokeræŒ‰ç…§é¡ºåºè¯»å†™çš„æœºåˆ¶ï¼Œå¯ä»¥æ¯ç§’ä¿å­˜600Mçš„æ•°æ®ã€‚ä¸»è¦é€šè¿‡pagecacheæœºåˆ¶ï¼Œå°½å¯èƒ½çš„åˆ©ç”¨å½“å‰ç‰©ç†æœºå™¨ä¸Šçš„ç©ºé—²å†…å­˜æ¥åšç¼“å­˜ã€‚ å½“å‰topicæ‰€å±çš„brokerï¼Œå¿…å®šæœ‰ä¸€ä¸ªè¯¥topicçš„partitionï¼Œpartitionæ˜¯ä¸€ä¸ªç£ç›˜ç›®å½•ã€‚partitionçš„ç›®å½•ä¸­æœ‰å¤šä¸ªsegmentç»„åˆ(index,log) partitionå¦‚ä½•åˆ†å¸ƒåœ¨ä¸åŒçš„brokerä¸Š123456int i = 0list&#123;kafka01,kafka02,kafka03&#125;for(int i=0;i&lt;5;i++)&#123; brIndex = i%broker; hostName = list.get(brIndex)&#125; consumerGroupçš„ç»„å‘˜å’Œpartitionä¹‹é—´å¦‚ä½•åšè´Ÿè½½å‡è¡¡æœ€å¥½æ˜¯ä¸€ä¸€å¯¹åº”ï¼Œä¸€ä¸ªpartitionå¯¹åº”ä¸€ä¸ªconsumer,å¦‚æœconsumerçš„æ•°é‡è¿‡å¤šï¼Œå¿…ç„¶æœ‰ç©ºé—²çš„consumerã€‚ ç®—æ³•ç†è®ºå¦‚ä¸‹ï¼š 123456å‡å¦‚topic1,å…·æœ‰å¦‚ä¸‹partitions: P0,P1,P2,P3åŠ å…¥groupä¸­,æœ‰å¦‚ä¸‹consumer: C1,C2é¦–å…ˆæ ¹æ®partitionç´¢å¼•å·å¯¹partitionsæ’åº: P0,P1,P2,P3æ ¹æ®consumer.idæ’åº: C0,C1è®¡ç®—å€æ•°: M = [P0,P1,P2,P3].size / [C0,C1].size,æœ¬ä¾‹å€¼M=2(å‘ä¸Šå–æ•´)ç„¶åä¾æ¬¡åˆ†é…partitions: C0 = [P0,P1],C1=[P2,P3],å³Ci = [P(i * M),P((i + 1) * M -1)] å¦‚ä½•ä¿è¯kafkaæ¶ˆè´¹è€…æ¶ˆè´¹æ•°æ®æ˜¯å…¨å±€æœ‰åºçš„å¦‚æœè¦å…¨å±€æœ‰åºçš„ï¼Œå¿…é¡»ä¿è¯ç”Ÿäº§æœ‰åºï¼Œå­˜å‚¨æœ‰åºï¼Œæ¶ˆè´¹æœ‰åºã€‚ç”±äºç”Ÿäº§å¯ä»¥åšé›†ç¾¤ï¼Œå­˜å‚¨å¯ä»¥åˆ†ç‰‡ï¼Œæ¶ˆè´¹å¯ä»¥è®¾ç½®ä¸ºä¸€ä¸ªconsumerGroupï¼Œè¦ä¿è¯å…¨å±€æœ‰åºï¼Œå°±éœ€è¦ä¿è¯æ¯ä¸ªç¯èŠ‚éƒ½æœ‰åºã€‚ åªæœ‰ä¸€ä¸ªå¯èƒ½ï¼Œå°±æ˜¯ä¸€ä¸ªç”Ÿäº§è€…ï¼Œä¸€ä¸ªpartitionï¼Œä¸€ä¸ªæ¶ˆè´¹è€…ã€‚è¿™ç§åœºæ™¯å’Œå¤§æ•°æ®åº”ç”¨åœºæ™¯ç›¸æ‚–ã€‚ 1.ç”Ÿäº§è€…æ˜¯é›†ç¾¤æ¨¡å¼â€“ã€‹å…¨å±€åºå·ç®¡ç†å™¨ 2.brokeræ–­åªè®¾ç½®ä¸€ä¸ªpartion-ã€‹kafkaçš„é«˜å¹¶å‘ä¸‹çš„è´Ÿè½½å‡è¡¡ 3.æ¶ˆè´¹è€…å¦‚æœæ˜¯ä¸€ä¸ªç»„ï¼Œå¦‚ä½•ä¿éšœæ¶ˆæ¯æœ‰åºï¼Ÿæ¶ˆè´¹æ¥ä¸€ä¸ªçº¿ç¨‹ï¼ˆè‡ªå®šä¹‰ä¸€ä¸ªæ•°æ®ç»“æ„æ¥æ’åºï¼‰ ä¸è¢«å®Œæ•´å¤„ç†ï¼Œä¼šé€ æˆç»“æœï¼Ÿæ˜¯å¦å¼€å¯ack-failæœºåˆ¶éœ€è¦æ ¹æ®ä¸šåŠ¡åœºæ™¯æ¥ åœ¨å¤§æ•°æ®æ“ä½œç‚¹å‡»æµæ•°æ®åŸºæœ¬ä¸Šæ˜¯ä¸å¼€å¯çš„ï¼Œç‚¹å‡»æµæ—¥å¿—ä¸­ä¸€æ¡pvï¼Œuvæ•°æ®ä¸¢å¤±ä¸ä¼šé€ æˆä»€ä¹ˆå½±å“ã€‚","categories":[{"name":"æ€»ç»“","slug":"æ€»ç»“","permalink":"http://gangtieguo.cn/categories/æ€»ç»“/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://gangtieguo.cn/tags/Kafka/"}]},{"title":"Hiveç´¯è®¡æŠ¥è¡¨","slug":"Hiveç´¯è®¡æŠ¥è¡¨","date":"2018-08-13T03:49:13.857Z","updated":"2019-06-17T04:40:09.385Z","comments":true,"path":"2018/08/13/Hiveç´¯è®¡æŠ¥è¡¨/","link":"","permalink":"http://gangtieguo.cn/2018/08/13/Hiveç´¯è®¡æŠ¥è¡¨/","excerpt":"[TOC] åœ¨ hive åšç»Ÿè®¡çš„æ—¶å€™ï¼Œæ€»æ˜¯æ¶‰åŠåˆ°åšç´¯è®¡çš„æŠ¥è¡¨å¤„ç†ï¼Œä¸‹é¢æ¡ˆåˆ—å°±æ˜¯æ¥åšç›¸åº”å¤„ç† å‡†å¤‡åˆ›å»ºæ•°æ®æ–‡ä»¶ï¼ˆåœ¨hadoopæ‰€åœ¨çš„æœºå™¨ï¼‰ vim /usr/hadoop/hivedata/t_sales.dat 12345678910èˆ’è‚¤ä½³,2018-06,5èˆ’è‚¤ä½³,2018-06,15ç¾å§¿,2018-06,5èˆ’è‚¤ä½³,2018-06,8ç¾å§¿,2018-06,25èˆ’è‚¤ä½³,2018-06,5èˆ’è‚¤ä½³,2018-07,4èˆ’è‚¤ä½³,2018-07,6ç¾å§¿,2018-07,10ç¾å§¿,2018-07,5 ä¸Šä¼ åˆ°hdfs 1hadoop fs -put /usr/hadoop/hivedata/t_sales.dat /local/hivedata/t_sales.dat","text":"[TOC] åœ¨ hive åšç»Ÿè®¡çš„æ—¶å€™ï¼Œæ€»æ˜¯æ¶‰åŠåˆ°åšç´¯è®¡çš„æŠ¥è¡¨å¤„ç†ï¼Œä¸‹é¢æ¡ˆåˆ—å°±æ˜¯æ¥åšç›¸åº”å¤„ç† å‡†å¤‡åˆ›å»ºæ•°æ®æ–‡ä»¶ï¼ˆåœ¨hadoopæ‰€åœ¨çš„æœºå™¨ï¼‰ vim /usr/hadoop/hivedata/t_sales.dat 12345678910èˆ’è‚¤ä½³,2018-06,5èˆ’è‚¤ä½³,2018-06,15ç¾å§¿,2018-06,5èˆ’è‚¤ä½³,2018-06,8ç¾å§¿,2018-06,25èˆ’è‚¤ä½³,2018-06,5èˆ’è‚¤ä½³,2018-07,4èˆ’è‚¤ä½³,2018-07,6ç¾å§¿,2018-07,10ç¾å§¿,2018-07,5 ä¸Šä¼ åˆ°hdfs 1hadoop fs -put /usr/hadoop/hivedata/t_sales.dat /local/hivedata/t_sales.dat åˆ›å»ºè¡¨åŠè¯»å–æ•°æ®123create table t_sales(brandname string,month string,sales int)row format delimited fields terminated by ',';load data inpath '/local/hivedata/t_sales.dat' into table t_sales; å¦‚æœæ˜¯ä¸Šä¼ æœ¬åœ°æ–‡ä»¶ï¼ˆå¦‚æœåœ¨hiveæ‰€åœ¨ä¸»æœºä¸Šï¼‰ åˆ™åœ¨load data ååŠ  localï¼Œå¦‚ load data local inpath &#39;/usr/hadoop/hivedata/t_sales.dat&#39; into table t_sales; 1ã€å…ˆæ±‚æ¯ä¸ªå“ç‰Œçš„æœˆæ€»é‡‘é¢1select brandname,month,sum(sales) as all_sales from t_sales group by brandname,month 2ã€å°†æœˆæ€»é‡‘é¢è‡ªè¿æ¥1234567select * from (select brandname,month,sum(sales) as sal from t_sales group by brandname,month) A inner join (select brandname,month,sum(sales) as sal from t_sales group by brandname,month) B onA.brandname=B.brandnamewhere B.month &lt;= A.month; 3ã€ä»ä¸Šä¸€æ­¥çš„ç»“æœä¸­è¿›è¡Œåˆ†ç»„æŸ¥è¯¢åˆ†ç»„çš„å­—æ®µæ˜¯ a.brandname a.month æ±‚æœˆç´¯è®¡å€¼ï¼š å°† b.month &lt;= a.month çš„æ‰€æœ‰ b.salsæ±‚å’Œå³å¯ 12345678910select A.brandname,A.month,max(A.sales) as sales,sum(B.sales) as accumulatefrom (select brandname,month,sum(sales) as sales from t_sales group by brandname,month) A inner join (select brandname,month,sum(sales) as sales from t_sales group by brandname,month) BonA.brandname=B.brandnamewhere B.month &lt;= A.monthgroup by A.brandname,A.monthorder by A.brandname,A.month;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://gangtieguo.cn/tags/Hive/"},{"name":"æŠ¥è¡¨","slug":"æŠ¥è¡¨","permalink":"http://gangtieguo.cn/tags/æŠ¥è¡¨/"}]},{"title":"Hiveåˆ†æ¡¶è¡¨,åˆ†åŒºè¡¨ç®€å•åˆ†æ","slug":"Hiveåˆ†æ¡¶è¡¨ç›¸å…³","date":"2018-08-11T03:21:32.532Z","updated":"2019-06-17T04:40:09.384Z","comments":true,"path":"2018/08/11/Hiveåˆ†æ¡¶è¡¨ç›¸å…³/","link":"","permalink":"http://gangtieguo.cn/2018/08/11/Hiveåˆ†æ¡¶è¡¨ç›¸å…³/","excerpt":"[TOC] å¯¹äºæ¯ä¸€ä¸ªè¡¨æˆ–è€…æ˜¯åˆ†åŒºï¼ŒHive å¯ä»¥è¿›ä¸€æ­¥ç»„ç»‡æˆæ¡¶ï¼Œä¹Ÿå°±æ˜¯è¯´æ¡¶æ˜¯æ›´ä¸ºç»†ç²’åº¦çš„æ•°æ®èŒƒå›´åˆ’åˆ†ã€‚Hive æ˜¯é’ˆå¯¹æŸä¸€åˆ—è¿›è¡Œåˆ†æ¡¶ã€‚Hive é‡‡ç”¨å¯¹åˆ—å€¼å“ˆå¸Œï¼Œç„¶åé™¤ä»¥æ¡¶çš„ä¸ªæ•°æ±‚ä½™çš„æ–¹å¼å†³å®šè¯¥æ¡è®°å½•å­˜æ”¾åœ¨å“ªä¸ªæ¡¶ä¸­ã€‚åˆ†æ¡¶çš„å¥½å¤„æ˜¯å¯ä»¥è·å¾—æ›´é«˜çš„æŸ¥è¯¢å¤„ç†æ•ˆç‡ã€‚ä½¿å–æ ·æ›´é«˜æ•ˆã€‚ åˆ†æ¡¶ä¾èµ–äºyarnçš„æ‰€ä»¥åˆ†æ¡¶çš„æ—¶å€™éœ€è¦å¯åŠ¨yarn","text":"[TOC] å¯¹äºæ¯ä¸€ä¸ªè¡¨æˆ–è€…æ˜¯åˆ†åŒºï¼ŒHive å¯ä»¥è¿›ä¸€æ­¥ç»„ç»‡æˆæ¡¶ï¼Œä¹Ÿå°±æ˜¯è¯´æ¡¶æ˜¯æ›´ä¸ºç»†ç²’åº¦çš„æ•°æ®èŒƒå›´åˆ’åˆ†ã€‚Hive æ˜¯é’ˆå¯¹æŸä¸€åˆ—è¿›è¡Œåˆ†æ¡¶ã€‚Hive é‡‡ç”¨å¯¹åˆ—å€¼å“ˆå¸Œï¼Œç„¶åé™¤ä»¥æ¡¶çš„ä¸ªæ•°æ±‚ä½™çš„æ–¹å¼å†³å®šè¯¥æ¡è®°å½•å­˜æ”¾åœ¨å“ªä¸ªæ¡¶ä¸­ã€‚åˆ†æ¡¶çš„å¥½å¤„æ˜¯å¯ä»¥è·å¾—æ›´é«˜çš„æŸ¥è¯¢å¤„ç†æ•ˆç‡ã€‚ä½¿å–æ ·æ›´é«˜æ•ˆã€‚ åˆ†æ¡¶ä¾èµ–äºyarnçš„æ‰€ä»¥åˆ†æ¡¶çš„æ—¶å€™éœ€è¦å¯åŠ¨yarn åˆ†æ¡¶è¡¨åˆ›å»º#è®¾ç½®å˜é‡,è®¾ç½®åˆ†æ¡¶ä¸ºtrue, è®¾ç½®reduceæ•°é‡æ˜¯åˆ†æ¡¶çš„æ•°é‡ä¸ªæ•° 12set hive.enforce.bucketing = true;set mapreduce.job.reduces=4; åˆ›å»ºè¡¨ 123456create table person_buck(id int,name string,sex string,age int)clustered by(id) sorted by(id DESC)into 4 bucketsrow format delimitedfields terminated by ','; 123å¼€ä¼šå¾€åˆ›å»ºçš„åˆ†æ¡¶è¡¨æ’å…¥æ•°æ®(æ’å…¥æ•°æ®éœ€è¦æ˜¯å·²åˆ†æ¡¶, ä¸”æ’åºçš„)å¯ä»¥ä½¿ç”¨distribute by(id) sort by(id asc) æˆ–æ˜¯æ’åºå’Œåˆ†æ¡¶çš„å­—æ®µç›¸åŒçš„æ—¶å€™ä½¿ç”¨Cluster by(å­—æ®µ)æ³¨æ„ä½¿ç”¨cluster by å°±ç­‰åŒäºåˆ†æ¡¶+æ’åº(sort) 1insert into table person_buck select id,name,sex,age from student distribute by(id) sort by(id asc); åˆ†æ¡¶æ¨¡å¼çš„å‚æ•°è®¾ç½®å˜é‡,è®¾ç½®åˆ†æ¡¶ä¸ºtrue, è®¾ç½®reduceæ•°é‡æ˜¯åˆ†æ¡¶çš„æ•°é‡ä¸ªæ•° set hive.enforce.bucketing = true; ä¸è®¾ç½®reduceçš„æ•°é‡ä¼šä½¿ç”¨é»˜è®¤çš„æ•°é‡ï¼Œé»˜è®¤çš„æ•°é‡ä¼šå’Œåˆ†æ¡¶çš„æ•°é‡ä¸ä¸€è‡´ï¼Œåˆ™ä¸èƒ½åˆ†å‡ºæ­£ç¡®åˆ†æ¡¶ set mapreduce.job.reduces=4; æœ¬åœ°æ¨¡å¼ set hive.exec.mode.local.auto=true åŠ¨æ€åˆ†åŒº â€“è®¾ç½®ä¸ºtrueè¡¨ç¤ºå¼€å¯åŠ¨æ€åˆ†åŒºåŠŸèƒ½ï¼ˆé»˜è®¤ä¸ºfalseï¼‰ set hive.exec.dynamic.partition=true; â€“è®¾ç½®ä¸ºnonstrict,è¡¨ç¤ºå…è®¸æ‰€æœ‰åˆ†åŒºéƒ½æ˜¯åŠ¨æ€çš„ï¼ˆé»˜è®¤ä¸ºstrictï¼‰ set hive.exec.dynamic.partition.mode=nonstrict; Updateä¸åˆ†æ¡¶è¡¨å…³ç³»Hiveå¯¹ä½¿ç”¨UpdateåŠŸèƒ½çš„è¡¨æœ‰ç‰¹å®šçš„è¯­æ³•è¦æ±‚, è¯­æ³•è¦æ±‚å¦‚ä¸‹:(1)è¦æ‰§è¡ŒUpdateçš„è¡¨ä¸­, å»ºè¡¨æ—¶å¿…é¡»å¸¦æœ‰buckets(åˆ†æ¡¶)å±æ€§(2)è¦æ‰§è¡ŒUpdateçš„è¡¨ä¸­, éœ€è¦æŒ‡å®šæ ¼å¼,å…¶ä½™æ ¼å¼ç›®å‰èµä¸æ”¯æŒ, å¦‚:parquetæ ¼å¼, ç›®å‰åªæ”¯æŒORCFileformatå’ŒAcidOutputFormat(3)è¦æ‰§è¡ŒUpdateçš„è¡¨ä¸­, å»ºè¡¨æ—¶å¿…é¡»æŒ‡å®šå‚æ•°(â€˜transactionalâ€™ = true);ä¸¾ä¾‹: 1create table student (id bigint,name string) clustered by (name) into 2 buckets stored as orc TBLPROPERTIES(&apos;transactional&apos;=&apos;true&apos;); æ›´æ–°è¯­å¥:1update student set id=&apos;444&apos; where name=&apos;tom&apos;; åˆ†æ¡¶è¡¨æµ‹è¯•è¿™ä¸ªä¾‹å­å°±æ˜¯å°†åˆ†åŒºçš„å­—æ®µè¿›è¡Œhashæ•£åˆ—å°†æ•°æ®åˆ†æ¡¶åˆ°åˆ†æ¡¶æ•°ä¸ªæ–‡ä»¶ä¸­å» å¯¼å…¥ä¸€ä¸ªæ–‡ä»¶åˆ°åˆ†æ¡¶è¡¨é‡Œé¢ åˆ›å»ºè¡¨1create table t_buk(id int,name string) clustered by(id) sorted by(id DESC) into 4 buckets row format delimited``fields terminated by ','; åˆ›å»ºæ•°æ®cd /usr/hive/hivedata/ vim buk.txt 1234567891011121,æ•°æ®åº“2,æ•°å­¦3,ä¿¡æ¯ç³»ç»Ÿ4,æ“ä½œç³»ç»Ÿ5,æ•°æ®ç»“æ„6,æ•°æ®no7,æ•°æ®other8,æ•°æ®time9,æ•°æ®æ“ä½œ10,æ•°æ®æŒ–æ˜11,æ•°æ®æŒ–æœº12,æ•°æ®ä¿¡å· è¯»å–æœ¬åœ°æ–‡ä»¶1load data local inpath &apos;/usr/hive/hivedata/buk.txt&apos; into table t_buk; loadæ–¹å¼è¿™æ ·å¯¼å…¥æ•°æ®åˆ°ä¸€ä¸ªåˆ†æ¡¶è¡¨é‡Œé¢ï¼Œæ˜¯ä¸ä¼šä½œå‡ºåˆ†æ¡¶çš„æ“ä½œçš„ï¼Œä¸ä¼šåˆ†æˆæ¡¶æ•°ä¸ªæ–‡ä»¶ï¼Œè¿˜æ˜¯ä¸€ä¸ªæ–‡ä»¶åœ¨hdfsç³»ç»Ÿä¸­ æ³¨æ„ è¦æƒ³å¯¼å…¥åˆ°æ•°æ®åˆ°åˆ†æ¡¶è¡¨é‡Œé¢ï¼Œå¿…é¡»æ˜¯ä¸€ä¸ªæ˜¯å·²ç»æ˜¯åˆ†æ¡¶çš„æ•°æ®ï¼Œæ¯”å¦‚å·²ç»å½¢æˆäº†åˆ†æ¡¶æ•°æ®ä¸ªæ–‡ä»¶ï¼Œæ‰å¯ä»¥å¯¼å…¥åˆ°åˆ†æ¡¶è¡¨é‡Œé¢ï¼Œå¯¼æ•°æ®çš„æ—¶å€™æ˜¯ä¸ä¼šå°†åŸæ¥çš„æ•°æ®å½¢å¼å˜æˆåˆ†æ¡¶çš„æ•°æ®å½¢å¼ hiveåˆ†æ¡¶è¡¨çš„ä½¿ç”¨åœºæ™¯æ‰€ä»¥ä¸€èˆ¬æ˜¯åœ¨ä¸€ä¸ªè¡¨ä¸­æŸ¥è¯¢äº†æ•°æ®ç„¶ååœ¨å¡å…¥åˆ°ä¸€ä¸ªåˆ†åŒºè¡¨é‡Œé¢ï¼ŒæŸ¥è¯¢æ˜¯èµ°mapReduceç¨‹åºï¼Œç„¶åå°†æ•°æ®æŒ‰åˆ†æ¡¶è¡¨ç…§åˆ†æ¡¶çš„ç­–ç•¥å†™å…¥åˆ°åˆ†æ¡¶è¡¨ä¸­ å½¢å¦‚ 1insert into t_buk select * from other â€¦ â€¦; åé¢çš„ æ¸…é™¤æ•°æ® 1truncate table t_buk; åˆ›å»ºä¸€ä¸ªè¡¨æ¥è¯»å–æ•°æ® 12345create table t_p(id int,name string)row format delimitedfields terminated by ',';load data local inpath '/usr/hive/hivedata/buk.txt' into table t_p; insert into table t_buk select id,name from t_p; insert overwirte ä¹Ÿå¯ä»¥ ç»“æœå¦‚ä¸‹1234567891011121314151617Number of reduce tasks is set to 0 since there&apos;s no reduce operatorINFO : number of splits:1INFO : Submitting tokens for job: job_1502537431423_0011INFO : The url to track the job: http://bigdata1:8088/proxy/application_1502537431423_0011/INFO : Starting Job = job_1502537431423_0011, Tracking URL = http://bigdata1:8088/proxy/application_1502537431423_0011/INFO : Kill Command = /home/bigdata/apps/hadoop/bin/hadoop job -kill job_1502537431423_0011INFO : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0INFO : 2017-08-15 21:12:15,669 Stage-1 map = 0%, reduce = 0%INFO : 2017-08-15 21:12:32,386 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.27 secINFO : MapReduce Total cumulative CPU time: 1 seconds 270 msecINFO : Ended Job = job_1502537431423_0011INFO : Stage-4 is selected by condition resolver.INFO : Stage-3 is filtered out by condition resolver.INFO : Stage-5 is filtered out by condition resolver.INFO : Moving data to: hdfs://bigdata1:9000/user/hive/warehouse/t_buk/.hive-staging_hive_2017-08-15_21-12-02_490_4088487413275551800-3/-ext-10000 from hdfs://bigdata1:9000/user/hive/warehouse/t_buk/.hive-staging_hive_2017-08-15_21-12-02_490_4088487413275551800-3/-ext-10002INFO : Loading data to table default.t_buk from hdfs://bigdata1:9000/user/hive/warehouse/t_buk/.hive-staging_hive_2017-08-15_21-12-02_490_4088487413275551800-3/-ext-10000INFO : Table default.t_buk stats: [numFiles=1, numRows=12, totalSize=167, rawDataSize=155] æŸ¥çœ‹hdfsç®¡ç†é¡µé¢ 50070 è¿˜æ˜¯åªæœ‰ä¸€æ–‡ä»¶ï¼Œè¡¨ç¤ºåˆ†æ¡¶ä¸æˆåŠŸï¼Œæ²¡è®¾reduceæ•°é‡ï¼Œä½¿ç”¨é»˜è®¤çš„æ•°é‡1ï¼Œå’Œæˆ‘ä»¬æœŸæœ›åˆ†æ¡¶æ•°é‡ä¸ä¸€è‡´ è®¾ç½®åˆ†æ¡¶å‚æ•°å› ä¸ºæ²¡æœ‰å¯åŠ¨æ¨¡å¼çš„å¼€å…³ï¼Œå¦‚ä¸‹ è®¾ç½®å˜é‡,è®¾ç½®åˆ†æ¡¶ä¸ºtrue, è®¾ç½®reduceæ•°é‡æ˜¯åˆ†æ¡¶çš„æ•°é‡ä¸ªæ•° 12345set hive.enforce.bucketing = true;set mapreduce.job.reduces=4;set hive.exec.mode.local.auto=true;set hive.exec.dynamic.partition=true;set hive.exec.dynamic.partition.mode=nonstrict; é‡æ–°åˆ›å»ºè¡¨å¯ä»¥é€šè¿‡set hive.enforce.bucketingæŸ¥çœ‹æ˜¯å¦è®¾ç½®æˆåŠŸ å…ˆæŸ¥çœ‹sort by (id)ï¼› æ ¹æ®4ä¸ªreduceæ¥å±€éƒ¨æœ‰åºï¼Œæ¯ä¸ªreduceæœ‰åºï¼Œä½†æ˜¯ä»å“ªå„¿æˆªæ–­æ¯ä¸ªreduceå¹¶ä¸ç¡®å®š 1234567891011121314151617181920212223242526272829303132333435363738select id,name from t_p sort by (id);INFO : Number of reduce tasks not specified. Defaulting to jobconf value of: 4INFO : In order to change the average load for a reducer (in bytes):INFO : set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;INFO : In order to limit the maximum number of reducers:INFO : set hive.exec.reducers.max=&lt;number&gt;INFO : In order to set a constant number of reducers:INFO : set mapreduce.job.reduces=&lt;number&gt;INFO : number of splits:1INFO : Submitting tokens for job: job_1502537431423_0012INFO : The url to track the job: http://bigdata1:8088/proxy/application_1502537431423_0012/INFO : Starting Job = job_1502537431423_0012, Tracking URL = http://bigdata1:8088/proxy/application_1502537431423_0012/INFO : Kill Command = /home/bigdata/apps/hadoop/bin/hadoop job -kill job_1502537431423_0012INFO : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 4INFO : 2017-08-15 21:26:14,284 Stage-1 map = 0%, reduce = 0%INFO : 2017-08-15 21:26:24,633 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 2.29 secINFO : 2017-08-15 21:26:38,745 Stage-1 map = 100%, reduce = 50%, Cumulative CPU 6.99 secINFO : 2017-08-15 21:26:43,887 Stage-1 map = 100%, reduce = 67%, Cumulative CPU 6.99 secINFO : 2017-08-15 21:26:46,970 Stage-1 map = 100%, reduce = 75%, Cumulative CPU 9.06 secINFO : 2017-08-15 21:26:50,081 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 10.8 secINFO : MapReduce Total cumulative CPU time: 10 seconds 800 msecINFO : Ended Job = job_1502537431423_0012+-----+----------+--+| id | name |+-----+----------+--+| 4 | æ“ä½œç³»ç»Ÿ || 8 | æ•°æ®time || 12 | æ•°æ®ä¿¡å· || 2 | æ•°å­¦ || 6 | æ•°æ®no || 1 | æ•°æ®åº“ || 3 | ä¿¡æ¯ç³»ç»Ÿ || 5 | æ•°æ®ç»“æ„ || 10 | æ•°æ®æŒ–æ˜ || 11 | æ•°æ®æŒ–æœº || 7 | æ•°æ®other || 9 | æ•°æ®æ“ä½œ |+-----+----------+--+ å†è¯•ä¸€æ¬¡select æ’å…¥ï¼ˆå°†t_buk truncateä¹Ÿå¯ï¼Œä¹Ÿå¯ä½¿ç”¨overwriteå…³é”®å­—ï¼‰ insert overwrite table t_buk select id,name from t_p cluster by (id); å†æŸ¥çœ‹hdfs uié¡µé¢50070 å†åˆ†åˆ«æŸ¥çœ‹è¿™å‡ ä¸ªæ–‡ä»¶1234$HADOOP_HOME/bin/hadoop fs -cat /user/hive/warehouse/t_buk/000000_0$HADOOP_HOME/bin/hadoop fs -cat /user/hive/warehouse/t_buk/000001_0 $HADOOP_HOME/bin/hadoop fs -cat /user/hive/warehouse/t_buk/000002_0 $HADOOP_HOME/bin/hadoop fs -cat /user/hive/warehouse/t_buk/000003_0 å¾—åˆ°ç»“æœ12345678910111213141516[bigdata@master hivedata]$ hadoop fs -cat /user/hive/warehouse/t_buk/000000_0 4,æ“ä½œç³»ç»Ÿ8,æ•°æ®time12,æ•°æ®ä¿¡å·[bigdata@master hivedata]$ hadoop fs -cat /user/hive/warehouse/t_buk/000001_0 1,æ•°æ®åº“5,æ•°æ®ç»“æ„9,æ•°æ®æ“ä½œ[bigdata@master hivedata]$ hadoop fs -cat /user/hive/warehouse/t_buk/000002_0 2,æ•°å­¦6,æ•°æ®no10,æ•°æ®æŒ–æ˜[bigdata@master hivedata]$ hadoop fs -cat /user/hive/warehouse/t_buk/000003_03,ä¿¡æ¯ç³»ç»Ÿ7,æ•°æ®other11,æ•°æ®æŒ–æœº åˆ†æ¡¶è¡¨ç–‘é—®ä¸ºä»€ä¹ˆæ¯ä¸ªæ¡¶é‡Œé¢çš„æ•°æ®æ¡æ•°ä¸ä¸€æ ·1hashæ•£åˆ—çš„æ—¶å€™æ•°æ®å¯èƒ½å°†æ•°æ®æœ‰çš„åˆ†çš„å¤šï¼Œæœ‰çš„åˆ†çš„å°‘ cluster by ï¼ˆidï¼‰ æ ¹æ®idåˆ†æ¡¶ï¼Œæ¡¶å†…æ ¹æ®idæ’åºï¼Œç›¸å½“äº distribute by å’Œ sort byçš„é›†åˆï¼Œåªæ˜¯æŒ‡å®šçš„å­—æ®µéƒ½æ˜¯åŒä¸€ä¸ªç”¨ä¸¤ä¸ªç»„åˆæ›´åŠ å¼ºå¤§ï¼Œåˆ†æ¡¶å­—æ®µæ’åºå­—æ®µå¯ä»¥è®¾ç½®ä¸ºä¸åŒ åˆ†æ¡¶è¡¨çš„æ„ä¹‰ï¼šæé«˜joinæ“ä½œçš„æ•ˆç‡æ¡ˆä¾‹ å¦‚æœaè¡¨å’Œbè¡¨å·²ç»æ˜¯åˆ†æ¡¶è¡¨ï¼Œè€Œä¸”åˆ†æ¡¶çš„å­—æ®µéƒ½æ˜¯æ˜¯idå­—æ®µåšè¿™ä¸ªjoinæ“ä½œæ˜¯ï¼Œè¿˜éœ€è¦åšç¬›å¡å°”ç§¯å—ï¼Ÿ è¿™æ ·ä¸éœ€è¦ï¼Œå› ä¸ºåŒä¸€idå“ˆå¸Œåçš„æ•°æ®æ˜¯ä¸€è‡´çš„ï¼Œè¿™å°±æ˜¯åˆ†æ¡¶è¡¨å­˜åœ¨çš„æ„ä¹‰ æ³¨æ„ åœ¨åˆ†æ¡¶è¡¨ä¸­ä½¿ç”¨order by æ˜¯éå¸¸ä¸å»ºè®®çš„ï¼Œè¿™æ ·ä¼šè®¾ç½®æˆä¸€ä¸ªreduceï¼Œå¼ºè¡Œå°†æ•°æ®å†™å…¥ï¼Œä¸€ä¸ªreduceçš„å†…å­˜ä¼šçˆ†ç‚¸ ä½¿ç”¨cluster by å°±ç­‰åŒäºåˆ†æ¡¶+æ’åº(sort) insert overwrite table student_buck select * from student cluster by(Sno) sort by(Sage); æŠ¥é”™,cluster å’Œ sort ä¸èƒ½å…±å­˜","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://gangtieguo.cn/tags/Hive/"}]},{"title":"Hiveè‡ªå®šä¹‰å‡½æ•°UDFç›¸å…³","slug":"Hiveè‡ªå®šä¹‰å‡½æ•°æµç¨‹","date":"2018-08-11T03:16:01.806Z","updated":"2018-08-13T03:19:44.441Z","comments":true,"path":"2018/08/11/Hiveè‡ªå®šä¹‰å‡½æ•°æµç¨‹/","link":"","permalink":"http://gangtieguo.cn/2018/08/11/Hiveè‡ªå®šä¹‰å‡½æ•°æµç¨‹/","excerpt":"[TOC] UDFå¼€å‘åŠä½¿ç”¨ æ‰“æˆjaråŒ…ä¸Šä¼ åˆ°æœåŠ¡å™¨ï¼Œå°†jaråŒ…æ·»åŠ åˆ°hiveçš„classpath 1æˆ–è€… hive&gt;add JAR /home/hadoop/udf.jar; åˆ›å»ºä¸´æ—¶å‡½æ•°ä¸å¼€å‘å¥½çš„java classå…³è” 1Hive&gt;create temporary function runTime as &apos;me.yao.bigdata.udf.RunTime&apos;; å³å¯åœ¨hqlä¸­ä½¿ç”¨è‡ªå®šä¹‰çš„å‡½æ•°time() Select time(name),age from t_test; add jaråªåœ¨ä¸€æ¬¡ä¼šè¯ä¸­ç”Ÿæ•ˆ","text":"[TOC] UDFå¼€å‘åŠä½¿ç”¨ æ‰“æˆjaråŒ…ä¸Šä¼ åˆ°æœåŠ¡å™¨ï¼Œå°†jaråŒ…æ·»åŠ åˆ°hiveçš„classpath 1æˆ–è€… hive&gt;add JAR /home/hadoop/udf.jar; åˆ›å»ºä¸´æ—¶å‡½æ•°ä¸å¼€å‘å¥½çš„java classå…³è” 1Hive&gt;create temporary function runTime as &apos;me.yao.bigdata.udf.RunTime&apos;; å³å¯åœ¨hqlä¸­ä½¿ç”¨è‡ªå®šä¹‰çš„å‡½æ•°time() Select time(name),age from t_test; add jaråªåœ¨ä¸€æ¬¡ä¼šè¯ä¸­ç”Ÿæ•ˆ transformæ¡ˆä¾‹:å¯ä»¥ä¸éœ€è¦ä¸Šä¼ jaråŒ…1ã€åŠ è½½æ•°æ®å…ˆåŠ è½½rating.json(é“¾æ¥)æ–‡ä»¶åˆ°hiveçš„ä¸€ä¸ªåŸå§‹è¡¨ rat_json 12create table rat_json(line string) row format delimited;load data local inpath '/home/bigdata/apps/hive/hivedata/rating.json' into table rat_json; 2ã€è§£æå­—æ®µéœ€è¦è§£æjsonæ•°æ®æˆå››ä¸ªå­—æ®µï¼Œæ’å…¥ä¸€å¼ æ–°çš„è¡¨ t_rating 12create table t_rating asselect get_json_object(line,'$.movie') as movieid,get_json_object(line,'$.rate')as rate,get_json_object(line,'$.timeStamp')as timestring,get_json_object(line,'$.uid')as uid from rat_json; æˆ–è€… 12insert overwrite table t_ratingselect get_json_object(line,'$.movie') as movieid,get_json_object(line,'$.rate')as rate,get_json_object(line,'$.timeStamp')as timestring,get_json_object(line,'$.uid')as uid from rat_json; 3ã€è½¬æ¢weekdayä½¿ç”¨transform+pythonçš„æ–¹å¼å»è½¬æ¢unixtimeä¸ºweekday å…ˆç¼–è¾‘ä¸€ä¸ªpythonè„šæœ¬æ–‡ä»¶ cd /usr/hive/hivedata/ vim weekday_mapper.py 123456789101112131415#!/bin/pythonimport sysimport datetimefor line in sys.stdin: line = line.strip() movieid, rating, unixtime,userid = line.split('\\t') weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() print '\\t'.join([movieid, rating, str(weekday),userid]) å°†æ–‡ä»¶åŠ å…¥classpathä¿å­˜æ–‡ä»¶ ç„¶åï¼Œå°†æ–‡ä»¶åŠ å…¥hiveçš„classpathï¼š 1hive&gt;add FILE /usr/hive/hivedata/weekday_mapper.py; å†åˆ›å»ºè¡¨123456create TABLE u_data_new asSELECT TRANSFORM (movieid, rate, timestring,uid) USING 'python weekday_mapper.py' AS (movieid, rate, weekday,uid)FROM t_rating; æŸ¥è¯¢è¡¨1select distinct(weekday) from u_data_new limit 10;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://gangtieguo.cn/tags/Hive/"}]},{"title":"Hiveå»ºè¡¨åŠsqlç›¸å…³","slug":"Hive sqlç›¸å…³","date":"2018-08-11T02:31:24.062Z","updated":"2019-06-17T04:40:09.383Z","comments":true,"path":"2018/08/11/Hive sqlç›¸å…³/","link":"","permalink":"http://gangtieguo.cn/2018/08/11/Hive sqlç›¸å…³/","excerpt":"[TOC] hiveä¸»è¦æ˜¯åšç¦»çº¿æ—¥å¿—åˆ†æçš„ï¼Œä¸æ˜¯ä¸ºäº†åšå•è¡Œçš„äº‹åŠ¡æ§åˆ¶çš„æ•°æ®æ–°ç‰ˆhiveä¹Ÿæ”¯æŒå•è¡Œæ•°æ®çš„è¯»å–ï¼Œä½†æ˜¯æ•ˆç‡éå¸¸ä½ï¼Œæ‰€ä»¥ä¹Ÿæ²¡æœ‰ä»€ä¹ˆupdataè¯­å¥","text":"[TOC] hiveä¸»è¦æ˜¯åšç¦»çº¿æ—¥å¿—åˆ†æçš„ï¼Œä¸æ˜¯ä¸ºäº†åšå•è¡Œçš„äº‹åŠ¡æ§åˆ¶çš„æ•°æ®æ–°ç‰ˆhiveä¹Ÿæ”¯æŒå•è¡Œæ•°æ®çš„è¯»å–ï¼Œä½†æ˜¯æ•ˆç‡éå¸¸ä½ï¼Œæ‰€ä»¥ä¹Ÿæ²¡æœ‰ä»€ä¹ˆupdataè¯­å¥ hdfsçš„æ•°æ®æ˜¯æ”¾åœ¨hdfsé‡Œé¢çš„ï¼Œè¡¨çš„æè¿°çš„ç»“æ„å…ƒæ•°æ®ä¿¡æ¯æ˜¯æ”¾åœ¨mysqlé‡Œé¢hdfsä¸­æ•°æ®çš„ä¿¡æ¯åœ¨ä»¥ä¸‹ç±»ä¼¼ç›®å½•/user/hive/warehouse/thishive.db/book/country=japan å¯ä»¥åœ¨hiveçš„å®¢æˆ·ç«¯ç›´æ¥æ•²ç”¨hdfsçš„å‘½ä»¤æŸ¥çœ‹åˆ°1hdfs dfs -ls /hiveç›®å½• æœ¬åœ°æ¨¡å¼set hive.exec.mode.local.auto=true; å»ºè¡¨(é»˜è®¤æ˜¯å†…éƒ¨è¡¨)1create table inner_table(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by '\\t'; å»ºåˆ†åŒºè¡¨1create table outter_table(id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) row format delimited fields terminated by '\\t'; å»ºå¤–éƒ¨è¡¨1create external table td_ext(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by '\\t' location '/td_ext'; localtionæ˜¯è¡¨ç¤ºå­˜æ”¾çš„ä½ç½® å¤åˆ¶è¡¨1create table è¡¨1 like è¡¨2 ; å°†è¡¨2çš„ç»“æ„å¤åˆ¶åˆ°è¡¨1å°†æ–‡ä»¶çš„æ•°æ®å¯¼å…¥åˆ°è¡¨ä¸­ å¯¼å…¥æ•°æ®åˆ°è¡¨ä¸­ ç»™è¡¨å¯¼å…¥æ•°æ®ï¼ˆè‹¥æ˜¯åˆ†åŒºè¡¨ï¼Œåˆ™å¯¼å…¥çš„æ—¶å€™éœ€è¦åŠ partition(#####)ï¼‰ 1load data local inpath '/home/hadoop/mylog.log' into table è¡¨å partition(datestr='2013-09-18') ; å¦‚æœæ˜¯å¯¼å…¥æœ¬åœ°æ–‡ä»¶ï¼Œéœ€è¦åŠ å‚æ•°localï¼Œå¦‚æœæ˜¯hdfsä¸Šçš„è¯ï¼Œåˆ™ä¸åŠ  å¯¼å‡ºhiveè¡¨ä¸­æ•°æ®1insert overwrite local directory &apos;/home/hadoop/student.txt&apos; select * from è¡¨å; ä¸åŠ localè¡¨ç¤ºå¯¼å‡ºåˆ°hdfs å¤–éƒ¨è¡¨å’Œå†…éƒ¨è¡¨çš„åŒºåˆ«drop table å¤–éƒ¨è¡¨ï¼› åªä¼šå°†å¤–éƒ¨è¡¨çš„ç»“æ„ å…ƒæ•°æ®ä¿¡æ¯åˆ é™¤ï¼Œè€Œä¸ä¼šåˆ é™¤å¤–è¡¨çš„æ•°æ®drop table å†…éƒ¨è¡¨ï¼›ä¼šå°†å†…éƒ¨è¡¨çš„ç»“æ„å…ƒæ•°æ®ä¿¡æ¯åŠå…¶æ•°æ®ä¿¡æ¯å…¨éƒ¨åˆ é™¤ ä¿å­˜selectæŸ¥è¯¢ç»“æœçš„å‡ ç§æ–¹å¼ï¼š1234567891011121314151617181920211ã€å°†æŸ¥è¯¢ç»“æœä¿å­˜åˆ°ä¸€å¼ æ–°çš„hiveè¡¨ä¸­create table t_tmpasselect * from t_p;2ã€å°†æŸ¥è¯¢ç»“æœä¿å­˜åˆ°ä¸€å¼ å·²ç»å­˜åœ¨çš„hiveè¡¨ä¸­insert into table t_tmp select * from t_p;3ã€å°†æŸ¥è¯¢ç»“æœä¿å­˜åˆ°æŒ‡å®šçš„æ–‡ä»¶ç›®å½•ï¼ˆå¯ä»¥æ˜¯æœ¬åœ°ï¼Œä¹Ÿå¯ä»¥æ˜¯hdfsï¼‰æœ¬åœ°insert overwrite local directory &apos;/home/hadoop/student.txt&apos;select * from student;å¯¼å…¥åˆ°mysqlinsert overwrite directory &apos;/aaa/test&apos;select * from t_p; åˆ†åŒºè¡¨æ™®é€šè¡¨å’Œåˆ†åŒºè¡¨åŒºåˆ«ï¼šæœ‰å¤§é‡æ•°æ®å¢åŠ çš„éœ€è¦å»ºåˆ†åŒºè¡¨åˆ†åŒºçš„å­—æ®µä¼šè‡ªåŠ¨åŠ åœ¨è¡¨ç»“æ„ä¸Š è¿™ä¸ªæ˜¯å°†å¯¼å…¥åˆ°fruitçš„åˆ†åŒºé‡Œé¢ 1load data local inpath '/home/bigdata/food.txt' overwrite into table book partition (type='fruit')ï¼› åœ¨hdfsé‡Œé¢ï¼Œåˆ†åŒºè¡¨ä¼šå­˜åœ¨å¤šä¸ªä¸åŒçš„ç›®å½•ï¼Œä½†æ˜¯åœ¨æŸ¥è¯¢çš„æ—¶å€™ï¼Œè¿˜æ˜¯å°†å¤šä¸ªåˆ†åŒºè¡¨çš„ä¿¡æ¯èå…¥åˆ°ä¸€ä¸ªè¡¨ä¸­ ä½¿ç”¨å¦‚æœæ˜¯ä½¿ç”¨overwriteå‘½ä»¤ï¼Œå¿…é¡»åŠ stored as textfileï¼› å°æ“ä½œä»¥ä¸‹èµ„æºæ¥è‡ªç½‘ç»œï¼ˆè‹¥æœ‰ä¸åˆé€‚ï¼Œè¯·è”ç³»æˆ‘ï¼‰ students.txt1234567891011121314151617181920212295001,æå‹‡,ç”·,20,CS95002,åˆ˜æ™¨,å¥³,19,IS95003,ç‹æ•,å¥³,22,MA95004,å¼ ç«‹,ç”·,19,IS95005,åˆ˜åˆš,ç”·,18,MA95006,å­™åº†,ç”·,23,CS95007,æ˜“æ€ç²,å¥³,19,MA95008,æå¨œ,å¥³,18,CS95009,æ¢¦åœ†åœ†,å¥³,18,MA95010,å­”å°æ¶›,ç”·,19,CS95011,åŒ…å°æŸ,ç”·,18,MA95012,å­™èŠ±,å¥³,20,CS95013,å†¯ä¼Ÿ,ç”·,21,CS95014,ç‹å°ä¸½,å¥³,19,CS95015,ç‹å›,ç”·,18,MA95016,é’±å›½,ç”·,21,MA95017,ç‹é£å¨Ÿ,å¥³,18,IS95018,ç‹ä¸€,å¥³,19,IS95019,é‚¢å°ä¸½,å¥³,19,IS95020,èµµé’±,ç”·,21,IS95021,å‘¨äºŒ,ç”·,17,MA95022,éƒ‘æ˜,ç”·,20,MA sc.txt1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818295001,1,8195001,2,8595001,3,8895001,4,7095002,2,9095002,3,8095002,4,7195002,5,6095003,1,8295003,3,9095003,5,10095004,1,8095004,2,9295004,4,9195004,5,7095005,1,7095005,2,9295005,3,9995005,6,8795006,1,7295006,2,6295006,3,10095006,4,5995006,5,6095006,6,9895007,3,6895007,4,9195007,5,9495007,6,7895008,1,9895008,3,8995008,6,9195009,2,8195009,4,8995009,6,10095010,2,9895010,5,9095010,6,8095011,1,8195011,2,9195011,3,8195011,4,8695012,1,8195012,3,7895012,4,8595012,6,9895013,1,9895013,2,5895013,4,8895013,5,9395014,1,9195014,2,10095014,4,9895015,1,9195015,3,5995015,4,10095015,6,9595016,1,9295016,2,9995016,4,8295017,4,8295017,5,10095017,6,5895018,1,9595018,2,10095018,3,6795018,4,7895019,1,7795019,2,9095019,3,9195019,4,6795019,5,8795020,1,6695020,2,9995020,5,9395021,2,9395021,5,9195021,6,9995022,3,6995022,4,9395022,5,8295022,6,100 course.txt1234561,æ•°æ®åº“2,æ•°å­¦3,ä¿¡æ¯ç³»ç»Ÿ4,æ“ä½œç³»ç»Ÿ5,æ•°æ®ç»“æ„6,æ•°æ®å¤„ç† å»ºè¡¨1234567create table student(Sno int,Sname string,Sex string,Sage int,Sdept string)row format delimited fields terminated by ','stored as textfile;create table course(Cno int,Cname string) row format delimited fields terminated by ',' stored as textfile;create table sc(Sno int,Cno int,Grade int)row format delimited fields terminated by ',' stored as textfile;load data local inpath '/home/bigdata/apps/hive/hivedata/students.txt' overwrite into table student;load data local inpath '/home/bigdata/apps/hive/hivedata/sc.txt' overwrite into table sc;load data local inpath '/home/bigdata/apps/hive/hivedata/course.txt' overwrite into table course; sqléœ€æ±‚123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081æŸ¥è¯¢å…¨ä½“å­¦ç”Ÿçš„å­¦å·ä¸å§“å hive&gt; select Sno,Sname from student;æŸ¥è¯¢é€‰ä¿®äº†è¯¾ç¨‹çš„å­¦ç”Ÿå§“å hive&gt; select distinct Sname from student inner join sc on student.Sno=Sc.Sno;----hiveçš„group by å’Œé›†åˆå‡½æ•°æŸ¥è¯¢å­¦ç”Ÿçš„æ€»äººæ•° hive&gt; select count(distinct Sno)count from student;è®¡ç®—1å·è¯¾ç¨‹çš„å­¦ç”Ÿå¹³å‡æˆç»© hive&gt; select avg(distinct Grade) from sc where Cno=1;æŸ¥è¯¢å„ç§‘æˆç»©å¹³å‡åˆ† hive&gt; select Cno,avg(Grade) from sc group by Cno; æŸ¥è¯¢é€‰ä¿®1å·è¯¾ç¨‹çš„å­¦ç”Ÿæœ€é«˜åˆ†æ•° select Grade from sc where Cno=1 sort by Grade desc limit 1; (æ³¨æ„æ¯”è¾ƒ:select * from sc where Cno=1 sort by Grade select Grade from sc where Cno=1 order by Grade) æ±‚å„ä¸ªè¯¾ç¨‹å·åŠç›¸åº”çš„é€‰è¯¾äººæ•° hive&gt; select Cno,count(1) from sc group by Cno;æŸ¥è¯¢é€‰ä¿®äº†3é—¨ä»¥ä¸Šçš„è¯¾ç¨‹çš„å­¦ç”Ÿå­¦å· hive&gt; select Sno from (select Sno,count(Cno) CountCno from sc group by Sno)a where a.CountCno&gt;3;æˆ– hive&gt; select Sno from sc group by Sno having count(Cno)&gt;3; ----hiveçš„Order By/Sort By/Distribute By Order By ï¼Œåœ¨strict æ¨¡å¼ä¸‹ï¼ˆhive.mapred.mode=strict),order by è¯­å¥å¿…é¡»è·Ÿç€limitè¯­å¥ï¼Œä½†æ˜¯åœ¨nonstrictä¸‹å°±ä¸æ˜¯å¿…é¡»çš„ï¼Œè¿™æ ·åšçš„ç†ç”±æ˜¯å¿…é¡»æœ‰ä¸€ä¸ªreduceå¯¹æœ€ç»ˆçš„ç»“æœè¿›è¡Œæ’åºï¼Œå¦‚æœæœ€åè¾“å‡ºçš„è¡Œæ•°è¿‡å¤šï¼Œä¸€ä¸ªreduceéœ€è¦èŠ±è´¹å¾ˆé•¿çš„æ—¶é—´ã€‚æŸ¥è¯¢å­¦ç”Ÿä¿¡æ¯ï¼Œç»“æœæŒ‰å­¦å·å…¨å±€æœ‰åº hive&gt; set hive.mapred.mode=strict; &lt;é»˜è®¤nonstrict&gt;hive&gt; select Sno from student order by Sno;FAILED: Error in semantic analysis: 1:33 In strict mode, if ORDER BY is specified, LIMIT must also be specified. Error encountered near token 'Sno' Sort Byï¼Œå®ƒé€šå¸¸å‘ç”Ÿåœ¨æ¯ä¸€ä¸ªredcueé‡Œï¼Œâ€œorder byâ€ å’Œâ€œsort byâ€çš„åŒºåˆ«åœ¨äºï¼Œå‰è€…èƒ½ç»™ä¿è¯è¾“å‡ºéƒ½æ˜¯æœ‰é¡ºåºçš„ï¼Œè€Œåè€…å¦‚æœæœ‰å¤šä¸ªreduceçš„æ—¶å€™åªæ˜¯ä¿è¯äº†è¾“å‡ºçš„éƒ¨åˆ†æœ‰åºã€‚set mapred.reduce.tasks=&lt;number&gt;åœ¨sort byå¯ä»¥æŒ‡å®šï¼Œåœ¨ç”¨sort byçš„æ—¶å€™ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šåˆ—ï¼Œå®ƒä¼šéšæœºçš„åˆ†é…åˆ°ä¸åŒçš„reduceé‡Œå»ã€‚distribute by æŒ‰ç…§æŒ‡å®šçš„å­—æ®µå¯¹æ•°æ®è¿›è¡Œåˆ’åˆ†åˆ°ä¸åŒçš„è¾“å‡ºreduceä¸­ æ­¤æ–¹æ³•ä¼šæ ¹æ®æ€§åˆ«åˆ’åˆ†åˆ°ä¸åŒçš„reduceä¸­ ï¼Œç„¶åæŒ‰å¹´é¾„æ’åºå¹¶è¾“å‡ºåˆ°ä¸åŒçš„æ–‡ä»¶ä¸­ã€‚æŸ¥è¯¢å­¦ç”Ÿä¿¡æ¯ï¼ŒæŒ‰æ€§åˆ«åˆ†åŒºï¼Œåœ¨åˆ†åŒºå†…æŒ‰å¹´é¾„æœ‰åº hive&gt; set mapred.reduce.tasks=2; hive&gt; insert overwrite local directory '/home/hadoop/out' select * from student distribute by Sex sort by Sage;----JoinæŸ¥è¯¢,joinåªæ”¯æŒç­‰å€¼è¿æ¥ æŸ¥è¯¢æ¯ä¸ªå­¦ç”ŸåŠå…¶é€‰ä¿®è¯¾ç¨‹çš„æƒ…å†µ hive&gt; select student.*,sc.* from student join sc on (student.Sno =sc.Sno);æŸ¥è¯¢å­¦ç”Ÿçš„å¾—åˆ†æƒ…å†µã€‚ hive&gt;select student.Sname,course.Cname,sc.Grade from student join sc on student.Sno=sc.Sno join course on sc.cno=course.cno;æŸ¥è¯¢é€‰ä¿®2å·è¯¾ç¨‹ä¸”æˆç»©åœ¨90åˆ†ä»¥ä¸Šçš„æ‰€æœ‰å­¦ç”Ÿã€‚ hive&gt; select student.Sname,sc.Grade from student join sc on student.Sno=sc.Sno where sc.Cno=2 and sc.Grade&gt;90; ----LEFTï¼ŒRIGHT å’Œ FULL OUTER JOIN ,inner join, left semi joinæŸ¥è¯¢æ‰€æœ‰å­¦ç”Ÿçš„ä¿¡æ¯ï¼Œå¦‚æœåœ¨æˆç»©è¡¨ä¸­æœ‰æˆç»©ï¼Œåˆ™è¾“å‡ºæˆç»©è¡¨ä¸­çš„è¯¾ç¨‹å· hive&gt; select student.Sname,sc.Cno from student left outer join sc on student.Sno=sc.Sno; å¦‚æœstudentçš„snoå€¼å¯¹åº”çš„scåœ¨ä¸­æ²¡æœ‰å€¼ï¼Œåˆ™ä¼šè¾“å‡ºstudent.Sname null.å¦‚æœç”¨right out joinä¼šä¿ç•™å³è¾¹çš„å€¼ï¼Œå·¦è¾¹çš„ä¸ºnullã€‚ Join å‘ç”Ÿåœ¨WHERE å­å¥ä¹‹å‰ã€‚å¦‚æœä½ æƒ³é™åˆ¶ join çš„è¾“å‡ºï¼Œåº”è¯¥åœ¨ WHERE å­å¥ä¸­å†™è¿‡æ»¤æ¡ä»¶â€”â€”æˆ–æ˜¯åœ¨join å­å¥ä¸­å†™ã€‚ ----LEFT SEMI JOIN Hive å½“å‰æ²¡æœ‰å®ç° IN/EXISTS å­æŸ¥è¯¢ï¼Œå¯ä»¥ç”¨ LEFT SEMI JOIN é‡å†™å­æŸ¥è¯¢è¯­å¥é‡å†™ä»¥ä¸‹å­æŸ¥è¯¢ä¸ºLEFT SEMI JOIN SELECT a.key, a.value FROM a WHERE a.key exist in (SELECT b.key FROM B);å¯ä»¥è¢«é‡å†™ä¸ºï¼š SELECT a.key, a.val FROM a LEFT SEMI JOIN b on (a.key = b.key)æŸ¥è¯¢ä¸â€œåˆ˜æ™¨â€åœ¨åŒä¸€ä¸ªç³»å­¦ä¹ çš„å­¦ç”Ÿ hive&gt; select s1.Sname from student s1 left semi join student s2 on s1.Sdept=s2.Sdept and s2.Sname='åˆ˜æ™¨';æ³¨æ„æ¯”è¾ƒï¼šselect * from student s1 left join student s2 on s1.Sdept=s2.Sdept and s2.Sname='åˆ˜æ™¨';select * from student s1 right join student s2 on s1.Sdept=s2.Sdept and s2.Sname='åˆ˜æ™¨';select * from student s1 inner join student s2 on s1.Sdept=s2.Sdept and s2.Sname='åˆ˜æ™¨';select * from student s1 left semi join student s2 on s1.Sdept=s2.Sdept and s2.Sname='åˆ˜æ™¨';select s1.Sname from student s1 right semi join student s2 on s1.Sdept=s2.Sdept and s2.Sname='åˆ˜æ™¨';","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://gangtieguo.cn/tags/Hive/"},{"name":"ä½¿ç”¨","slug":"ä½¿ç”¨","permalink":"http://gangtieguo.cn/tags/ä½¿ç”¨/"}]},{"title":"Jsonä¸Scalaç±»å‹çš„ç›¸äº’è½¬æ¢å¤„ç†","slug":"Jsonä¸Scalaç±»å‹çš„ä¸€äº›äº’ç›¸è½¬æ¢å¤„ç†","date":"2018-08-10T17:19:05.443Z","updated":"2019-06-17T04:40:09.388Z","comments":true,"path":"2018/08/11/Jsonä¸Scalaç±»å‹çš„ä¸€äº›äº’ç›¸è½¬æ¢å¤„ç†/","link":"","permalink":"http://gangtieguo.cn/2018/08/11/Jsonä¸Scalaç±»å‹çš„ä¸€äº›äº’ç›¸è½¬æ¢å¤„ç†/","excerpt":"[TOC] åœ¨å¼€å‘è¿‡ç¨‹ä¸­æ—¶å¸¸ä¼šæœ‰å¯¹jsonæ•°æ®çš„ä¸€äº›å¤„ç†ï¼Œç°åšä¸€äº›è®°å½•","text":"[TOC] åœ¨å¼€å‘è¿‡ç¨‹ä¸­æ—¶å¸¸ä¼šæœ‰å¯¹jsonæ•°æ®çš„ä¸€äº›å¤„ç†ï¼Œç°åšä¸€äº›è®°å½• 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import com.alibaba.fastjson.&#123;JSON, JSONArray, JSONObject&#125;import com.fasterxml.jackson.databind.ObjectMapperimport com.fasterxml.jackson.module.scala.DefaultScalaModuleimport net.minidev.json.parser.JSONParserimport scala.collection.JavaConversions.mapAsScalaMapimport scala.collection.mutableimport java.util/** * json utils */object JsonUtils &#123; val mapper: ObjectMapper = new ObjectMapper() def toJsonString(T: Object): String = &#123; mapper.registerModule(DefaultScalaModule) mapper.writeValueAsString(T) &#125; def getArrayFromJson(jsonStr: String) = &#123; JSON.parseArray(jsonStr) &#125; def getObjectFromJson(jsonStr: String): JSONObject = &#123; JSON.parseObject(jsonStr) &#125; /** * é…åˆgetObjectFromJson ä½¿ç”¨æŠŠ JSONObject å˜ä¸º map * @param jsonObj * @return */ def jsonObj2Map(jsonObj:JSONObject): mutable.Map[String, String] = &#123; var map = mutable.Map[String, String]() val itr: util.Iterator[String] = jsonObj.keySet().iterator() while (itr.hasNext) &#123; val key = itr.next() map += ((key, jsonObj.getString(key))) &#125; map &#125; /** * json å­—ç¬¦ä¸²è½¬æˆ Map * #############æœ‰äº›æƒ…å†µä¸‹è½¬æ¢ä¼šæœ‰é—®é¢˜############### * @param json * @return */ def json2Map(json: String): mutable.HashMap[String,String] =&#123; val map : mutable.HashMap[String,String]= mutable.HashMap() val jsonParser =new JSONParser() //å°†stringè½¬åŒ–ä¸ºjsonObject val jsonObj: JSONObject = jsonParser.parse(json).asInstanceOf[JSONObject] //è·å–æ‰€æœ‰é”® val jsonKey = jsonObj.keySet() val iter = jsonKey.iterator() while (iter.hasNext)&#123; val field = iter.next() val value = jsonObj.get(field).toString if(value.startsWith(\"&#123;\")&amp;&amp;value.endsWith(\"&#125;\"))&#123; val value = mapAsScalaMap(jsonObj.get(field).asInstanceOf[util.HashMap[String, String]]) map.put(field,value.toString()) &#125;else&#123; map.put(field,value) &#125; &#125; map &#125; /** * map è½¬æ¢æˆ json å­—ç¬¦ä¸² * @param map * @return */ def map2Json(map : mutable.Map[String,String]): String = &#123; import net.minidev.json.&#123;JSONObject&#125; import scala.collection.JavaConversions.mutableMapAsJavaMap val jsonString = JSONObject.toJSONString(map) jsonString &#125;&#125; æµ‹è¯•å®ä¾‹123456789101112131415161718def main(args: Array[String]) &#123; val json = \"[&#123;\\\"batchid\\\":305322456,\\\"amount\\\":20.0,\\\"count\\\":20&#125;,&#123;\\\"batchid\\\":305322488,\\\"amount\\\":\\\"10.0\\\",\\\"count\\\":\\\"10\\\"&#125;]\" val array: JSONArray = JsonUtils.getArrayFromJson(json) println(array) array.toArray().foreach(json=&gt;&#123; println(json) val jobj = json.asInstanceOf[JSONObject] println(jobj.get(\"batchid\")) &#125;) val jsonStr = \"&#123;\\\"batchid\\\":119,\\\"amount\\\":200.0,\\\"count\\\":200&#125;\" val jsonObj: JSONObject = JsonUtils.getObjectFromJson(jsonStr) println(jsonObj) val jsonObj2: JSONObject = JsonUtils.getObjectFromJson(\"&#123;'name':'Wang','age':18,'tag1':[&#123;'tn1':'100','tn2':'101','ts':'ts01'&#125;,&#123;'tn1':'100','tn2':'101','ts':'ts02'&#125;,&#123;'tn1':'100','tn2':'101','ts':'ts03'&#125;]&#125;\") println(jsonObj2)&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Json","slug":"Json","permalink":"http://gangtieguo.cn/tags/Json/"},{"name":"Scala","slug":"Scala","permalink":"http://gangtieguo.cn/tags/Scala/"}]},{"title":"Sparkè¯»å–HBase","slug":"Sparkè¯»å–Hbase","date":"2018-08-10T16:18:11.706Z","updated":"2019-06-17T04:40:09.402Z","comments":true,"path":"2018/08/11/Sparkè¯»å–Hbase/","link":"","permalink":"http://gangtieguo.cn/2018/08/11/Sparkè¯»å–Hbase/","excerpt":"[TOC] Sparkè¯»å–Hbase","text":"[TOC] Sparkè¯»å–Hbase sparké…ç½®é¦–å…ˆsparkçš„é…ç½® 12345678910111213141516171819202122val array = Array( (\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\"), (\"spark.storage.memoryFraction\", \"0.3\"), (\"spark.memory.useLegacyMode\", \"true\"), (\"spark.shuffle.memoryFraction\", \"0.6\"), (\"spark.shuffle.file.buffer\", \"128k\"), (\"spark.reducer.maxSizeInFlight\", \"96m\"), (\"spark.sql.shuffle.partitions\", \"500\"), (\"spark.default.parallelism\", \"180\"), (\"spark.dynamicAllocation.enabled\", \"false\") ) val conf = new SparkConf().setAll(array) .setJars(Array(\"your.jar\")) val sparkSession: SparkSession = SparkSession .builder .appName(applicationName) .enableHiveSupport() .master(\"spark://master:7077\") .config(conf) .getOrCreate() val sqlContext = sparkSession.sqlContext val sparkContext: SparkContext = sparkSession.sparkContext Hbaseé…ç½®1234567891011121314151617181920212223242526272829303132333435363738394041424344val hBaseConf = HBaseConfiguration.create()var scan = new Scan();scan.addFamily(Bytes.toBytes(\"cf\"));var proto = ProtobufUtil.toScan(scan)var scanToString = Base64.encodeBytes(proto.toByteArray())//ä»¥ä¸ºå…¨å±€æ‰«æçš„æ–¹å¼hBaseConf.set(TableInputFormat.SCAN,scanToString)//å¦‚éœ€è¦è®¾ç½®èµ·æ­¢è¡Œçš„è¯//scan.setStartRow(Bytes.toBytes(\"1111111111111\"))//scan.setStopRow(Bytes.toBytes(\"999999999999999\"))hBaseConf.set(\"hbase.zookeeper.quorum\",\"zk1,zk2,zk3\")hBaseConf.set(\"phoenix.query.timeoutMs\",\"1800000\")hBaseConf.set(\"hbase.regionserver.lease.period\",\"1200000\")hBaseConf.set(\"hbase.rpc.timeout\",\"1200000\")hBaseConf.set(\"hbase.client.scanner.caching\",\"1000\")hBaseConf.set(\"hbase.client.scanner.timeout.period\",\"1200000\")//è¡¨åé…ç½®hBaseConf.set(TableInputFormat.INPUT_TABLE,\"beehive:a_up_rawdata\")// ä»æ•°æ®æºè·å–æ•°æ®val hbaseRDD = sparkContext.newAPIHadoopRDD(hBaseConf,classOf[TableInputFormat],classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],classOf[org.apache.hadoop.hbase.client.Result])//å³å¯å¾—åˆ°è¯»å–HbaseæŸ¥è¯¢çš„RDD val hbaseJsonRdd: RDD[String] = hbaseRDD.filter(t =&gt; broadCast.value.contains(Bytes.toString(t._2.getRow)) //********************************æ“ä½œæ¯ä¸ªåˆ†åŒºçš„æ•°æ®******************************** ).mapPartitions( it=&gt;&#123; it.map(x=&gt;x._2).map(hbaseValue =&gt; &#123; var listBuffer = new ListBuffer[String]() //å¯¹åº”çš„å€¼ val rowkey = Bytes.toString(hbaseValue.getRow) val value: String = Bytes.toString(hbaseValue.getValue(Bytes.toBytes(\"cf\"), Bytes.toBytes(\"å¡«å†™è·å–å“ªä¸€åˆ—\"))) if (null != value ) &#123; //å¦‚æœvalueä¸ä¸ºç©ºåˆ™å†è¿›è¡Œæ“ä½œ &#125; listBuffer &#125;) &#125;).flatMap(r =&gt; r)//æ³¨æ„mapæ“ä½œæ˜¯éœ€è¦å‡½æ•°å†…éƒ¨æœ‰è¿”å›å€¼çš„ï¼Œå¦‚æœåªæ˜¯æ‰“å°çš„è¯ï¼Œæ¢æˆforeachç®—å­ println(s\"hbaseJsonRdd.sizeä¸ºï¼š$&#123;hbaseJsonRdd.count()&#125;\") sparkContext.stop() sparkSession.close() println(\"ALL å·²ç»å…³é—­ï¼Œç¨‹åºç»ˆæ­¢\")","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://gangtieguo.cn/tags/HBase/"},{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"}]},{"title":"Sparkè¯»å–HBaseè§£æjsonåˆ›å»ºä¸´æ—¶è¡¨å½•å…¥åˆ°Hiveè¡¨","slug":"Sparkè¯»å–HBaseè§£æjsonåˆ›å»ºä¸´æ—¶è¡¨å½•å…¥åˆ°Hiveè¡¨","date":"2018-08-10T16:09:46.585Z","updated":"2019-06-17T04:40:09.402Z","comments":true,"path":"2018/08/11/Sparkè¯»å–HBaseè§£æjsonåˆ›å»ºä¸´æ—¶è¡¨å½•å…¥åˆ°Hiveè¡¨/","link":"","permalink":"http://gangtieguo.cn/2018/08/11/Sparkè¯»å–HBaseè§£æjsonåˆ›å»ºä¸´æ—¶è¡¨å½•å…¥åˆ°Hiveè¡¨/","excerpt":"[TOC] ä»‹ç»ï¼šä¸»è¦æ˜¯è¯»å–é€šè¿‡mysqlæŸ¥åˆ°å…³è”å…³ç³»ç„¶åè¯»å–HBASEé‡Œé¢å­˜æ”¾çš„Jsonï¼Œé€šè¿‡è§£æjsonå°†jsonæ•°ç»„å¯¹è±¡é‡Œçš„å…ƒç´ æ‹†åˆ†æˆå•æ¡json,å†å°†jsonæ˜ å°„æˆä¸´æ—¶è¡¨ï¼ŒæŸ¥è¯¢ä¸´æ—¶è¡¨å°†æ•°æ®è½å…¥åˆ°hiveè¡¨ä¸­ æ³¨æ„ï¼šæŸ¥è¯¢HBASEçš„æ—¶å€™ï¼ŒHBaseé›†ç¾¤çš„HMasterï¼ŒHRegionServeréœ€è¦æ˜¯æ­£å¸¸è¿è¡Œ ä¸»è¦å°†å†…å®¹æ‹†åˆ†æˆå‡ å—ï¼Œsparkè¯»å–HBaseï¼Œsparkè§£æjsonå°†jsonæ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ æ‹†æˆä¸€æ¡ï¼ˆæ¯”å¦‚jsonæ•°ç»„æœ‰10ä¸ªå…ƒç´ ï¼Œéœ€è¦è§£æå¹³é“ºæˆ19ä¸ªjsonï¼Œé‚£ä¹ˆå¯¹åº”ä¸´æ—¶è¡¨ä¸­å°±æ˜¯19æ¡è®°å½•ï¼Œå¯¹åº”æŸ¥è¯¢æ’å…¥åˆ°hiveä¹Ÿå°±æ˜¯19æ¡è®°å½•ï¼‰ sparkè¯»å–æœ¬åœ°HBase","text":"[TOC] ä»‹ç»ï¼šä¸»è¦æ˜¯è¯»å–é€šè¿‡mysqlæŸ¥åˆ°å…³è”å…³ç³»ç„¶åè¯»å–HBASEé‡Œé¢å­˜æ”¾çš„Jsonï¼Œé€šè¿‡è§£æjsonå°†jsonæ•°ç»„å¯¹è±¡é‡Œçš„å…ƒç´ æ‹†åˆ†æˆå•æ¡json,å†å°†jsonæ˜ å°„æˆä¸´æ—¶è¡¨ï¼ŒæŸ¥è¯¢ä¸´æ—¶è¡¨å°†æ•°æ®è½å…¥åˆ°hiveè¡¨ä¸­ æ³¨æ„ï¼šæŸ¥è¯¢HBASEçš„æ—¶å€™ï¼ŒHBaseé›†ç¾¤çš„HMasterï¼ŒHRegionServeréœ€è¦æ˜¯æ­£å¸¸è¿è¡Œ ä¸»è¦å°†å†…å®¹æ‹†åˆ†æˆå‡ å—ï¼Œsparkè¯»å–HBaseï¼Œsparkè§£æjsonå°†jsonæ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ æ‹†æˆä¸€æ¡ï¼ˆæ¯”å¦‚jsonæ•°ç»„æœ‰10ä¸ªå…ƒç´ ï¼Œéœ€è¦è§£æå¹³é“ºæˆ19ä¸ªjsonï¼Œé‚£ä¹ˆå¯¹åº”ä¸´æ—¶è¡¨ä¸­å°±æ˜¯19æ¡è®°å½•ï¼Œå¯¹åº”æŸ¥è¯¢æ’å…¥åˆ°hiveä¹Ÿå°±æ˜¯19æ¡è®°å½•ï¼‰ sparkè¯»å–æœ¬åœ°HBase å‚è€ƒ Sparkè¯»å–HBase jsonæ ·ä¾‹ è¯»å–hbasehbaseé‡Œé¢å­˜æ”¾çš„æ˜¯èº«ä»½idä½œä¸ºrowkeyæ¥å­˜æ”¾çš„æ•°æ® JSONã€JSONObjectç±»åŒ…æ˜¯å¼•ç”¨çš„com.alibaba.fastjsonåŒ…ä¸‹çš„ 123456789101112131415161718192021222324252627282930313233343536val hbaseJsonRdd: RDD[String] = hbaseRDD.mapPartitions( it=&gt;&#123; it.map(x=&gt;x._2).map(hbaseValue =&gt; &#123; var listBuffer = new ListBuffer[String]() //å¯¹åº”çš„å€¼ //è·å–key,ä¹Ÿå°±æ˜¯èº«ä»½è¯å·ï¼Œé€šè¿‡èº«ä»½è¯å·åœ¨å¹¿æ’­mapä¸­çš„å€¼ ä¹Ÿå°±æ˜¯risk_request_id val idNum = Bytes.toString(hbaseValue.getRow) val json: String = Bytes.toString(hbaseValue.getValue(Bytes.toBytes(\"cf\"), Bytes.toBytes(s\"273468436_data\"))) if (null != json ) &#123; //********************************è·å–åˆ°jsonä¹‹åè¿›è¡Œè§£æ******************************** try &#123; val jSONObject: JSONObject = JSON.parseObject(json) if (jSONObject != null) &#123; val contactRegion = repostData.getJSONArray(\"contact_region\") if (contactRegion != null) &#123; contactRegion.toArray().foreach(v =&gt; &#123; val arrays = JSON.parseObject(v.toString) //val map = JSON.toJavaObject(arrays,classOf[util.Map[String,String]]) val map: mutable.Map[String, String] = JsonUtils.jsonObj2Map(arrays) //å°†json è½¬ä¸ºMap //å°†******************************** æ—¥æœŸå’ŒrequestId request_idå°è£…åˆ° mapé‡Œé¢********************************ï¼Œå†å°†mapè½¬ä¸ºjson map.put(\"region_id\", \"2\") map.put(\"request_id\", \"1\") map.put(\"region_create_at\", \"0000\") map.put(\"region_update_at\", \"0000\") listBuffer += (JsonUtils.map2Json(map)) &#125;) &#125; &#125; &#125; &#125;catch &#123; case e: Exception =&gt; e.printStackTrace() &#125; &#125; listBuffer &#125;) &#125;).flatMap(r =&gt; r) ä»£ç ä¸­çš„ jsonObj2Map,map2Json æ–¹æ³•å‚ç…§ Jsonä¸Scalaç±»å‹çš„ç›¸äº’è½¬æ¢å¤„ç† è¿™é‡Œæ‹†åˆ†jsonæ•°ç»„æ¯ä¸€ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªjsonï¼Œå­˜æ”¾åœ¨ListBufferé‡Œé¢ï¼Œé€šè¿‡flatMapå‹å¹³rddé‡Œé¢çš„å†…å®¹ã€‚ æ˜ å°„ä¸´æ—¶è¡¨æœ€åå°†å¾—åˆ°çš„jsoné€šè¿‡sparkSqlåˆ›å»ºæˆä¸´æ—¶è¡¨ 1234567 val dataFrame: DataFrame = sqlContext.read.json(hbaseJsonRdd)dataFrame.createOrReplaceTempView(\"tmp_hbase\")//// æµ‹è¯•println(\"++++++++++++++++++++++++++++++hbaseJsonRdd.....åˆ›å»ºä¸´æ—¶è¡¨ æµ‹è¯•æŸ¥è¯¢æ•°æ® ......++++++++++++++++++++++++++++++\")val df = sqlContext.sql(\"select * from tmp_hbase limit 1\")df.show(1) æ’å…¥Hive1234567sqlContext.sql(\"insert into ods.ods_r_juxinli_region_n partition(dt='20180101') select region_id as juxinli_region_id,request_id as juxinli_request_id,\" + \"region_loc as juxinli_rejion_loc ,region_uniq_num_cnt as juxinli_region_uniq_num_cnt ,\" + \"region_call_out_time as juxinli_region_call_out_time,region_call_in_time as juxinli_region_call_in_time,region_call_out_cnt as juxinli_region_call_out_cnt,\" + \"region_call_in_cnt as juxinli_region_call_in_cnt,region_avg_call_in_time as juxinli_region_avg_call_in_time,region_avg_call_out_time as juxinli_region_avg_call_out_time,\" + \"region_call_in_time_pct as juxinli_region_call_in_time_pct,region_call_out_time_pct as juxinli_region_call_out_time_pct ,region_call_in_cnt_pct as juxinli_region_call_in_cnt_pct,\" + \"region_call_out_cnt_pct as juxinli_region_call_out_cnt_pct,region_create_at as juxinli_region_create_at,region_update_at as juxinli_region_update_at from tmp_hbase\") &#125; å…³é—­èµ„æº 12sparkContext.stop()sparkSession.close()","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"},{"name":"SparkSQL","slug":"SparkSQL","permalink":"http://gangtieguo.cn/tags/SparkSQL/"}]},{"title":"HBaseæ‹·è´ç”Ÿäº§ç¯å¢ƒæ•°æ®åˆ°æœ¬åœ°Sparkè§£æè¿è¡Œè°ƒè¯•","slug":"HBaseæ‹·è´ç”Ÿäº§ç¯å¢ƒæ•°æ®åˆ°æœ¬åœ°è¿è¡Œè°ƒè¯•","date":"2018-08-10T10:19:15.453Z","updated":"2019-06-17T04:40:09.382Z","comments":true,"path":"2018/08/10/HBaseæ‹·è´ç”Ÿäº§ç¯å¢ƒæ•°æ®åˆ°æœ¬åœ°è¿è¡Œè°ƒè¯•/","link":"","permalink":"http://gangtieguo.cn/2018/08/10/HBaseæ‹·è´ç”Ÿäº§ç¯å¢ƒæ•°æ®åˆ°æœ¬åœ°è¿è¡Œè°ƒè¯•/","excerpt":"[TOC] ç”±äºçº¿ä¸Šç¯å¢ƒè¦ç»è¿‡è·³æ¿æœºè·³è½¬ï¼Œå¹¶ä¸”æ‰“åŒ…æµ‹è¯•ï¼Œä¸Šä¼ jaråŒ…æ­¥éª¤å¤šï¼Œä¸ç„¶çš„è¯ï¼Œè¦è¿›è¡Œå„ç§ç«¯å£è½¬å‘ï¼Œä¸”æœ‰æƒé™æ§åˆ¶ï¼Œä¸æ˜“åœ¨æœ¬åœ°ideaç¼–è¾‘å™¨ä¸Šè¿›è¡Œç¨‹åºè¿è¡ŒåŠè°ƒè¯• ç°åœ¨æƒ³æ³•æ˜¯ï¼Œå°†çº¿ä¸Šæµ‹è¯•ç¯å¢ƒçš„æ•°æ®æ‹·è´å°éƒ¨åˆ†åˆ°æœ¬åœ°è‡ªå·±æ­å»ºçš„é›†ç¾¤ï¼Œè¿›è¡Œç¨‹åºçš„é€»è¾‘å’ŒåˆæœŸè°ƒè¯• æ­¤è´´å°±æ˜¯è®°å½•ä¸€äº›æ“ä½œ è¿™éƒ½æ˜¯è¦åŸºäºæœ¬åœ°æœ‰HBASEåŠå…¶ä¾èµ–ç»„ä»¶çš„ã€‚ ä¸»è¦æ€è·¯æ˜¯ï¼Œæ‹·è´çº¿ä¸ŠæŸ¥è¯¢çš„ç»“æœåˆ°æ–‡ä»¶hbaseout1.txtï¼Œå°†hbaseout1.txtæ–‡ä»¶szå¯¼å…¥æœ¬åœ° å†åœ¨æœ¬åœ°é›†ç¾¤ä¸Šå°†æ•°æ®æ’å…¥åˆ°hbase","text":"[TOC] ç”±äºçº¿ä¸Šç¯å¢ƒè¦ç»è¿‡è·³æ¿æœºè·³è½¬ï¼Œå¹¶ä¸”æ‰“åŒ…æµ‹è¯•ï¼Œä¸Šä¼ jaråŒ…æ­¥éª¤å¤šï¼Œä¸ç„¶çš„è¯ï¼Œè¦è¿›è¡Œå„ç§ç«¯å£è½¬å‘ï¼Œä¸”æœ‰æƒé™æ§åˆ¶ï¼Œä¸æ˜“åœ¨æœ¬åœ°ideaç¼–è¾‘å™¨ä¸Šè¿›è¡Œç¨‹åºè¿è¡ŒåŠè°ƒè¯• ç°åœ¨æƒ³æ³•æ˜¯ï¼Œå°†çº¿ä¸Šæµ‹è¯•ç¯å¢ƒçš„æ•°æ®æ‹·è´å°éƒ¨åˆ†åˆ°æœ¬åœ°è‡ªå·±æ­å»ºçš„é›†ç¾¤ï¼Œè¿›è¡Œç¨‹åºçš„é€»è¾‘å’ŒåˆæœŸè°ƒè¯• æ­¤è´´å°±æ˜¯è®°å½•ä¸€äº›æ“ä½œ è¿™éƒ½æ˜¯è¦åŸºäºæœ¬åœ°æœ‰HBASEåŠå…¶ä¾èµ–ç»„ä»¶çš„ã€‚ ä¸»è¦æ€è·¯æ˜¯ï¼Œæ‹·è´çº¿ä¸ŠæŸ¥è¯¢çš„ç»“æœåˆ°æ–‡ä»¶hbaseout1.txtï¼Œå°†hbaseout1.txtæ–‡ä»¶szå¯¼å…¥æœ¬åœ° å†åœ¨æœ¬åœ°é›†ç¾¤ä¸Šå°†æ•°æ®æ’å…¥åˆ°hbase 1ã€åˆ›å»ºå’Œçº¿ä¸ŠåŒåé€šç»“æ„çš„è¡¨åœ¨çº¿ä¸Šæ‰§è¡Œ describe &#39;beehive:a_up_rawdata&#39; å¾—åˆ° åœ¨æœ¬åœ°æ‰§è¡Œ 1234hbase shellcreate_namespace &apos;beehive&apos;create &apos;beehive:a_up_rawdata&apos;,&#123;NAME =&gt; &apos;cf&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, COMPRESSION =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;1&apos;&#125; 2ã€æ‹·è´çº¿ä¸Šhbaseæ•°æ®æŸ¥è¯¢ç»“æœå¯¼å…¥æ–‡ä»¶åœ¨çº¿ä¸Šæœºå™¨ä»»æ„ç›®å½•æ‰§è¡Œ 1echo \"get 'beehive:a_up_rawdata','530111199211287371',&#123;COLUMN=&gt;'cf:273468436_data'&#125;\"| hbase shell&gt; hbaseout1.txt è§£æ: get &#39;beehive:a_up_rawdata&#39;,&#39;530111199211287371&#39;,{COLUMN=&gt;&#39;cf:273468436_data&#39;}æ˜¯æ‰§è¡Œçš„hbaseçš„æŸ¥è¯¢è¯­å¥ï¼Œå°†æŸ¥è¯¢çš„ç»“æœå­˜å…¥åˆ°å½“å‰ç›®å½• hbaseout1.txtæ–‡ä»¶ä¸­ æŸ¥è¯¢æ–‡ä»¶ä¸‹è½½åˆ°æœ¬åœ°sz hbaseout1.txt ä¿®æ”¹æ–‡ä»¶å†…å®¹å¯ä»¥æŸ¥çœ‹hbaseout1.txtä¸­å¯ä»¥çœ‹åˆ°ä¼šæœ‰è¡¨å¤´ éœ€è¦å°†è¿™éƒ¨åˆ†è¡¨å¤´æ•°æ®åˆ é™¤ï¼Œç»„æˆæ ‡å‡†çš„å¯¼å…¥æ–‡ä»¶ ä¿®æ”¹æ–‡ä»¶ç¼–ç  åœ¨é€šè¿‡ hbase shell æŸ¥çœ‹ä¸­æ–‡å€¼æ—¶, æ˜¯ unicode ç¼–ç æ ¼å¼ï¼Œä½¿å¾—ç›´æ¥æŸ¥çœ‹ä¸­æ–‡å€¼ä¸å¤ªæ–¹ä¾¿ã€‚å¦‚æœè¦æŸ¥çœ‹éœ€è¦æŠŠ unicode ç¼–ç è¿›è¡Œ decode å°†æŸ¥è¯¢ç»“æœå¯¼å‡ºæ¥ 1234567print ('éœ€è¦è½¬ç å†…å®¹'.decode('utf-8'))å‘½ä»¤æ ·ä¾‹python 2.7 print ('***\\xE4\\xBD\\xA010009 '.decode('utf-8'))python 3 print '***\\xE4\\xBD\\xA010009 '.decode('utf-8') å¯ä»¥æœ‰æ›´å‹å¥½çš„å°†å†…å®¹è®¾ç½®ä¸ºæ–‡ä»¶åï¼Œç„¶åå°†è½¬ç åé‡æ–°å†™å…¥åˆ°ä¸€ä¸ªæ–‡ä»¶ï¼Œåç»­ä¼šæ›´æ–° è½¬ç è¿‡åï¼Œæ–‡å­—æ˜¾ç¤ºæ­£ç¡® é‡æ–°ç»„åˆæ–‡ä»¶ç”±äºå¯¼å…¥åˆ°hbaseå‘½ä»¤ä¸º **æ ¼å¼ï¼šhbase [ç±»][åˆ†éš”ç¬¦] [è¡Œé”®ï¼Œåˆ—æ—][è¡¨] [å¯¼å…¥æ–‡ä»¶] ç”±äºæˆ‘è¿™æ¬¡å¯¼å…¥çš„æ–‡ä»¶é‡Œé¢æœ‰â€œ,â€ï¼Œæ‰€æœ‰å°†åˆ†éš”ç¬¦è®¾ç½®ä¸ºâ€œ|â€ æ›´æ”¹åçš„æ–‡ä»¶æ ¼å¼ä¸º å°†æ–‡ä»¶ä¸Šä¼ åˆ°hdfs 1hadoop fs -put hadoop fs -put hbaseout1.txt /local/ å°†æ•°æ®å¯¼å…¥åˆ°æœ¬åœ°hbase1hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=\"|\" -Dimporttsv.columns=HBASE_ROW_KEY,cf:273468436_data beehive:a_up_rawdata /local/hbaseout2.txt 3ã€æ ¡éªŒæŸ¥çœ‹åœ¨hueä¸ŠæŸ¥çœ‹hbaseå†…å®¹ï¼Œæ˜¾ç¤ºæœ‰æ•°æ® åœ¨hbase shell æŸ¥çœ‹","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://gangtieguo.cn/tags/HBase/"},{"name":"æ“ä½œ","slug":"æ“ä½œ","permalink":"http://gangtieguo.cn/tags/æ“ä½œ/"}]},{"title":"Hbase-shellæ“ä½œ","slug":"Hbase-shellæ“ä½œ","date":"2018-08-10T09:27:05.100Z","updated":"2019-06-17T04:40:09.378Z","comments":true,"path":"2018/08/10/Hbase-shellæ“ä½œ/","link":"","permalink":"http://gangtieguo.cn/2018/08/10/Hbase-shellæ“ä½œ/","excerpt":"[TOC] hbaseä½¿ç”¨å‘½ä»¤è¡Œæ“ä½œï¼Œç®€å•ç›´æ¥ï¼Œæ–¹ä¾¿å¿«æ·ï¼ŒæŒæ¡ä¸€ç‚¹å¿…å¤‡çš„åŸºç¡€å‘½ä»¤ã€‚ HBaseå¯åŠ¨å‘½ä»¤è¡Œ 1$HBASE_HOME/bin/hbase shell","text":"[TOC] hbaseä½¿ç”¨å‘½ä»¤è¡Œæ“ä½œï¼Œç®€å•ç›´æ¥ï¼Œæ–¹ä¾¿å¿«æ·ï¼ŒæŒæ¡ä¸€ç‚¹å¿…å¤‡çš„åŸºç¡€å‘½ä»¤ã€‚ HBaseå¯åŠ¨å‘½ä»¤è¡Œ 1$HBASE_HOME/bin/hbase shell åˆ›å»ºè¡¨1create 'testtable',&#123;NAME=&gt;'cf',VERSIONS=&gt;2&#125;,&#123;NAME=&gt;'cf2',VERSIONS=&gt;2&#125; åˆ›å»ºnamespace1create_namespace 'beehive' æŸ¥çœ‹è¡¨ç»“æ„1disable 'testtable' åˆ é™¤è¡¨1drop 'testtable' ä¿®æ”¹è¡¨1234disable 'testtable'alter 'testtable',&#123;NAME=&gt;'cf',TTL=&gt;'10000000'&#125;,&#123;NAME=&gt;'cf2',TTL=&gt;'10000000'&#125;enable 'testtable'ä¿®æ”¹è¡¨å¿…é¡»å…ˆ disable è¡¨ è¡¨æ•°æ®çš„å¢åˆ æŸ¥æ”¹ï¼šæ·»åŠ æ•°æ®ï¼š1put 'testtable','rowkey1','cf:key1','val1' æŸ¥è¯¢æ•°æ®:12get 'testtable','rowkey1','cf:key1'get 'testtable','rowkey1', &#123;COLUMN=&gt;'cf:key1'&#125; æ‰«æè¡¨:1scan 'testtable',&#123;COLUMNS=&gt;cf:col1,LIMIT=&gt;5&#125; #å¯ä»¥æ·»åŠ STARTROWã€TIMERANGEå’ŒFITLERç­‰é«˜çº§åŠŸèƒ½ æŸ¥è¯¢è¡¨ä¸­çš„æ•°æ®è¡Œæ•°:è¯­æ³•ï¼šcount &lt;table&gt;, {INTERVAL =&gt; intervalNum, CACHE =&gt; cacheNum} 1count 'testtable',&#123;INTERVAL =&gt; 100, CACHE =&gt; 500&#125; åˆ é™¤æ•°æ®:12delete 'testtable','rowkey1','cf:key1'truncate 'testtable'","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://gangtieguo.cn/tags/HBase/"},{"name":"Shell","slug":"Shell","permalink":"http://gangtieguo.cn/tags/Shell/"}]},{"title":"Sparkæœ¬åœ°è°ƒè¯•è¿œç¨‹é›†ç¾¤ç¨‹åº","slug":"Sparkæœ¬åœ°è°ƒè¯•è¿œç¨‹é›†ç¾¤ç¨‹åº","date":"2018-08-07T08:09:43.314Z","updated":"2019-06-17T04:40:09.400Z","comments":true,"path":"2018/08/07/Sparkæœ¬åœ°è°ƒè¯•è¿œç¨‹é›†ç¾¤ç¨‹åº/","link":"","permalink":"http://gangtieguo.cn/2018/08/07/Sparkæœ¬åœ°è°ƒè¯•è¿œç¨‹é›†ç¾¤ç¨‹åº/","excerpt":"[TOC] ç”±äºåœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿›è¡Œè°ƒè¯•sparkç¨‹åºéœ€è¦è¿›è¡Œæ‰“åŒ…å’Œå„ç§è·³æ¿æœºè·³è½¬ï¼Œæœ€å¥½åœ¨æœ¬åœ°æ­ä¸€å¥—é›†ç¾¤æ¥è¿›è¡Œä¸€äº›ä»£ç åŸºç¡€æ£€æŸ¥ã€‚","text":"[TOC] ç”±äºåœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿›è¡Œè°ƒè¯•sparkç¨‹åºéœ€è¦è¿›è¡Œæ‰“åŒ…å’Œå„ç§è·³æ¿æœºè·³è½¬ï¼Œæœ€å¥½åœ¨æœ¬åœ°æ­ä¸€å¥—é›†ç¾¤æ¥è¿›è¡Œä¸€äº›ä»£ç åŸºç¡€æ£€æŸ¥ã€‚ éœ€è¦å°† æœ¬åœ°é›†ç¾¤ä¸­hdfs-site.xml core.site.xml æ‹·è´åˆ°æœ¬åœ°å·¥ç¨‹çš„resourceæ–‡ä»¶å¤¹ä¸‹ï¼Œè¿™æ ·ä¼šåº”ç”¨è¿™äº›é…ç½®ï¼Œæ³¨æ„è¦åœ¨æäº¤åˆ°ç”Ÿäº§ç¯å¢ƒçš„æ—¶å€™ï¼Œæ›¿æ¢æˆå¯¹åº”ç¯å¢ƒçš„é…ç½®æ–‡ä»¶ 12345678910111213141516171819202122val array = Array( (\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\"), (\"spark.storage.memoryFraction\", \"0.3\"), (\"spark.memory.useLegacyMode\", \"true\"), (\"spark.shuffle.memoryFraction\", \"0.6\"), (\"spark.shuffle.file.buffer\", \"128k\"), (\"spark.reducer.maxSizeInFlight\", \"96m\"), (\"spark.sql.shuffle.partitions\", \"500\"), (\"spark.default.parallelism\", \"180\"), (\"spark.dynamicAllocation.enabled\", \"false\") ) val conf = new SparkConf().setAll(array) .setJars(Array(\"your.jar\")) val sparkSession: SparkSession = SparkSession .builder .appName(applicationName) .enableHiveSupport() .master(\"spark://master:7077\") .config(conf) .getOrCreate() val sqlContext = sparkSession.sqlContext val sparkContext: SparkContext = sparkSession.sparkContext","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"}]},{"title":"Zookeeperçš„é…ç½®å®¹å™¨çš„æ­å»º","slug":"zookeeperé…ç½®","date":"2018-08-06T19:06:03.170Z","updated":"2019-06-17T04:40:09.403Z","comments":true,"path":"2018/08/07/zookeeperé…ç½®/","link":"","permalink":"http://gangtieguo.cn/2018/08/07/zookeeperé…ç½®/","excerpt":"[TOC] åœ¨usrç›®å½•ä¸‹ä¸‹è½½zkåŒ…ï¼Œå¹¶ä¸”è§£å‹åˆ°/usr/ç›®å½•ï¼Œæ”¹åä¸ºzkï¼Œæ‰€ä»¥$ZK_HOMEä¸º/usr/zk åˆ›å»ºç›®å½•123mkdir -p /usr/zk/datamkdir -p /usr/zk/logstouch /usr/zk/data/myid","text":"[TOC] åœ¨usrç›®å½•ä¸‹ä¸‹è½½zkåŒ…ï¼Œå¹¶ä¸”è§£å‹åˆ°/usr/ç›®å½•ï¼Œæ”¹åä¸ºzkï¼Œæ‰€ä»¥$ZK_HOMEä¸º/usr/zk åˆ›å»ºç›®å½•123mkdir -p /usr/zk/datamkdir -p /usr/zk/logstouch /usr/zk/data/myid æ›´æ”¹é…ç½®æ–‡ä»¶vim $ZK_HOME/conf/zoo.cfg 12345dataDir=/usr/zk/datadataLogDir=/usr/zk/logsserver.1=zk1:2888:3888server.2=zk2:2888:3888server.3=zk3:2888:3888 zookeeperéœ€è¦å…¨éƒ¨å·¦å³èŠ‚ç‚¹éƒ½å¯åŠ¨æ‰ä¼šé€‰ä¸¾leaderï¼Œfollower æ‰€æœ‰èŠ‚ç‚¹å¯åŠ¨çš„è„šæœ¬ 1234567# !/bin/bashecho 1 &gt; $ZK_HOME/data/myid$ZK_HOME/bin/zkServer.sh startssh root@zk2 \"echo 2 &gt; /usr/zk/data/myid\"ssh root@zk2 \"/usr/zk/bin/zkServer.sh start\"ssh root@zk3 \"echo 3 &gt; /usr/zk/data/myid\"ssh root@zk3 \"/usr/zk/bin/zkServer.sh start\" zké—®é¢˜1ã€ç”±äº zk è¿è¡Œä¸€æ®µæ—¶é—´åï¼Œä¼šäº§ç”Ÿå¤§é‡çš„æ—¥å¿—æ–‡ä»¶ï¼ŒæŠŠç£ç›˜ç©ºé—´å æ»¡ï¼Œå¯¼è‡´æ•´ä¸ªæœºå™¨è¿›ç¨‹éƒ½ä¸èƒ½æ´»åŠ¨äº†ï¼Œæ‰€ä»¥éœ€è¦å®šæœŸæ¸…ç†è¿™äº›æ—¥å¿—æ–‡ä»¶ï¼Œæ–¹æ³•å¦‚ä¸‹ï¼š 1ï¼‰ã€å†™ä¸€ä¸ªè„šæœ¬æ–‡ä»¶ cleanup.sh å†…å®¹å¦‚ä¸‹ï¼š 12345678java -cp zookeeper.jar:lib/slf4j-api-1.6.1.jar:lib/slf4j-log4j12-1.6.1.jar:lib/log4j-1.2.15.jar:conf org.apache.zookeeper.server.PurgeTxnLog &lt;dataDir&gt; &lt;snapDir&gt; -n &lt;count&gt;å…¶ä¸­ï¼š dataDirï¼šå³ä¸Šé¢é…ç½®çš„ dataDir çš„ç›®å½• snapDirï¼šå³ä¸Šé¢é…ç½®çš„ dataLogDir çš„ç›®å½• countï¼šä¿ç•™å‰å‡ ä¸ªæ—¥å¿—æ–‡ä»¶ï¼Œé»˜è®¤ä¸º 3 2ï¼‰ã€é€šè¿‡ crontab å†™å®šæ—¶ä»»åŠ¡ï¼Œæ¥å®Œæˆå®šæ—¶æ¸…ç†æ—¥å¿—çš„éœ€æ±‚ 123crontab -e 0 0 * * /opt/zookeeper-3.4.10/bin/cleanup.shHBase Master é«˜å¯ç”¨ï¼ˆHAï¼‰ï¼ˆhttp://www.cnblogs.com/captainlucky/p/4710642.htmlï¼‰HMaster æ²¡æœ‰å•ç‚¹é—®é¢˜ï¼ŒHBase ä¸­å¯ä»¥å¯åŠ¨å¤šä¸ª HMasterï¼Œé€šè¿‡ Zookeeper çš„ Master Election æœºåˆ¶ä¿è¯æ€»æœ‰ä¸€ä¸ª Master è¿è¡Œã€‚ æ‰€ä»¥è¿™é‡Œè¦é…ç½® HBase é«˜å¯ç”¨çš„è¯ï¼Œåªéœ€è¦å¯åŠ¨ä¸¤ä¸ª HMasterï¼Œè®© Zookeeper è‡ªå·±å»é€‰æ‹©ä¸€ä¸ª Master Acitveã€‚","categories":[{"name":"å®‰è£…éƒ¨ç½²","slug":"å®‰è£…éƒ¨ç½²","permalink":"http://gangtieguo.cn/categories/å®‰è£…éƒ¨ç½²/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"},{"name":"zk","slug":"zk","permalink":"http://gangtieguo.cn/tags/zk/"}]},{"title":"","slug":"docker-machineå¼‚å¸¸é—®é¢˜","date":"2018-08-01T12:20:03.591Z","updated":"2018-08-01T18:24:04.223Z","comments":true,"path":"2018/08/01/docker-machineå¼‚å¸¸é—®é¢˜/","link":"","permalink":"http://gangtieguo.cn/2018/08/01/docker-machineå¼‚å¸¸é—®é¢˜/","excerpt":"","text":"Docker-machineè«åä¸èƒ½è®¿é—®ï¼Ÿ 1Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.29/containers/json: dial unix /var/run/docker.sock: connect: permission denied 12Starting &quot;default&quot;...generic driver does not support start","categories":[],"tags":[]},{"title":"Linuxå®‰è£…mysql","slug":"Linuxå®‰è£…mysql","date":"2018-07-24T15:42:18.647Z","updated":"2019-06-17T04:40:09.391Z","comments":true,"path":"2018/07/24/Linuxå®‰è£…mysql/","link":"","permalink":"http://gangtieguo.cn/2018/07/24/Linuxå®‰è£…mysql/","excerpt":"åœ¨linux yumå®‰è£…mysql","text":"åœ¨linux yumå®‰è£…mysql 12345678910111213141516171819yum install -y mysql-serverchkconfig --add mysqldchkconfig mysqld onchkconfig --list mysqldservice mysqld startmysql -u root -pEnter password: //é»˜è®¤å¯†ç ä¸ºç©ºï¼Œè¾“å…¥åå›è½¦å³å¯set password for root@localhost=password('root'); å¯†ç è®¾ç½®ä¸ºrootset password for root@=password('root');é»˜è®¤æƒ…å†µä¸‹Mysqlåªå…è®¸æœ¬åœ°ç™»å½•ï¼Œæ‰€ä»¥åªéœ€é…ç½®root@localhostå°±å¥½è®¾ç½®æ‰€æœ‰ipè®¿é—®å¯†ç ä¸ºrootset password for root@%=password('root'); å¯†ç è®¾ç½®ä¸ºroot ï¼ˆå…¶å®è¿™ä¸€æ­¥å¯ä»¥ä¸é…ï¼‰è®¾ç½®masterè®¿é—®å¯†ç ä¸ºrootset password for root@master=password('root'); å¯†ç è®¾ç½®ä¸ºroot ï¼ˆå…¶å®è¿™ä¸€æ­¥å¯ä»¥ä¸é…ï¼‰æŸ¥è¯¢å¯†ç select user,host,password from mysql.user; æŸ¥çœ‹å¯†ç æ˜¯å¦è®¾ç½®æˆåŠŸè®¾ç½®æ‰€æœ‰ipå¯ä»¥é€šè¿‡rootè®¿é—®GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO 'hive'@'%' IDENTIFIED BY 'hive' WITH GRANT OPTION;","categories":[{"name":"å®‰è£…éƒ¨ç½²","slug":"å®‰è£…éƒ¨ç½²","permalink":"http://gangtieguo.cn/categories/å®‰è£…éƒ¨ç½²/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://gangtieguo.cn/tags/Linux/"},{"name":"Mysql","slug":"Mysql","permalink":"http://gangtieguo.cn/tags/Mysql/"}]},{"title":"Docker-yaosong5/bigdatabaseé•œåƒåˆ¶ä½œæ­å»ºåŠå…¶è¿è¡Œå‘½ä»¤","slug":"Docker-yaosong5:bigdatabaseé•œåƒåˆ¶ä½œæ­å»ºåŠå…¶è¿è¡Œå‘½ä»¤","date":"2018-07-23T15:40:07.620Z","updated":"2019-03-11T07:53:05.815Z","comments":true,"path":"2018/07/23/Docker-yaosong5:bigdatabaseé•œåƒåˆ¶ä½œæ­å»ºåŠå…¶è¿è¡Œå‘½ä»¤/","link":"","permalink":"http://gangtieguo.cn/2018/07/23/Docker-yaosong5:bigdatabaseé•œåƒåˆ¶ä½œæ­å»ºåŠå…¶è¿è¡Œå‘½ä»¤/","excerpt":"","text":"[TOC] åœ¨centosbase1.0çš„åŸºç¡€ä¸Šè®¾ç½®å˜é‡åœ¨centosbase1.0çš„åŸºç¡€ä¸Šå®‰è£…ï¼Œmysqlï¼ˆmysqlå®‰è£…å‚è€ƒmysqlå®‰è£…æ–‡ç« ï¼‰é‡Œé¢å®‰è£…hiveç”¨æˆ·ï¼Œhueç”¨æˆ·ç­‰ç­‰ï¼Œ åˆ›å»ºæ•°æ®åº“hueï¼Œhiveç­‰ï¼ˆä¸ºåç»­åšå‡†å¤‡ï¼‰ 12create database hive;create database hue; å¼€æœºè‡ªå¯åŠ¨mysql `chkconfig --add mysqld` ä¿å­˜ä¸ºé•œåƒyaosong5/centosbase:2.0docker commit -m &quot;centos ssh mysql \båˆ›å»ºäº†hiveåº“ï¼Œhueåº“&quot; ctos yaosong5/centosbase:2.0docker commit -m &quot;centos ssh mysql \båˆ›å»ºäº†hiveåº“ï¼Œhueåº“&quot; ctos yaosong5/centosbase:3.0docker commit -m &quot;centos ssh mysql \båˆ›å»ºäº†hiveåº“ï¼Œhueåº“ï¼Œæ›´æ”¹äº†/JAVA_HOME&quot; ctos yaosong5/centosbase:4.0åæ›´æ”¹ä¸º3.0é‡æ–°é…ç½®äº†java_homeå°†æ ‡ç­¾4.0 æ›´æ”¹ä¸º centosbase:1.0è¿˜æ˜¯åˆ›å»º bigdatabase:1.0 å‡†å¤‡åˆ›å»ºè½¯è¿æ¥123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263ln -s /Users/yaosong/Yao/DockerSource/sparkSource/hadoop /Users/yaosong/Yao/share/source/hadoopln -s /Users/yaosong/Yao/DockerSource/sparkSource/hbase /Users/yaosong/Yao/share/source/hbaseln -s /Users/yaosong/Yao/DockerSource/sparkSource/hive /Users/yaosong/Yao/share/source/hiveln -s /Users/yaosong/Yao/DockerSource/sparkSource/kafka /Users/yaosong/Yao/share/source/kafkaln -s /Users/yaosong/Yao/DockerSource/sparkSource/scala /Users/yaosong/Yao/share/source/scalaln -s /Users/yaosong/Yao/DockerSource/sparkSource/spark /Users/yaosong/Yao/share/source/sparkln -s /Users/yaosong/Yao/DockerSource/sparkSource/zk /Users/yaosong/Yao/share/source/zkln -s /Users/yaosong/Yao/DockerSource/hueSource/ant /Users/yaosong/Yao/share/source/antln -s /Users/yaosong/Yao/DockerSource/hueSource/hue4 /Users/yaosong/Yao/share/source/hue4ln -s /Users/yaosong/Yao/DockerSource/hueSource/maven /Users/yaosong/Yao/share/source/mavenln -s /Users/yaosong/Yao/DockerSource/flinkSource/flink /Users/yaosong/Yao/share/source/flinkln -s /Users/yaosong/Yao/DockerSource/flinkSource/flink-hadoop /Users/yaosong/Yao/share/source/flink-hadoopln -s /Users/yaosong/Yao/DockerSource/elkSource/es /Users/yaosong/Yao/share/source/esln -s /Users/yaosong/Yao/DockerSource/elkSource/filebeat /Users/yaosong/Yao/share/source/filebeatln -s /Users/yaosong/Yao/DockerSource/elkSource/kibana /Users/yaosong/Yao/share/source/kibanaln -s /Users/yaosong/Yao/DockerSource/elkSource/logstash /Users/yaosong/Yao/share/source/logstashln -s /Users/yaosong/Yao/DockerSource/elkSource/node /Users/yaosong/Yao/share/source/nodeln -s /Users/yaosong/Yao/DockerSource/hueSource/jdk /Users/yaosong/Yao/share/source/jdkåè°ƒæ•´ln -s /Users/yaosong/Yao/DockerSource/sparkSource/spark-2.3.1-bin-hadoop2.7 /Users/yaosong/Yao/share/source/sparkln -s /Users/yaosong/Yao/DockerSource/sparkSource/hadoop-3.0.3 /Users/yaosong/Yao/share/source/hadooprm -rf /Users/yaosong/Yao/share/source/hadoopln -s /Users/yaosong/Yao/DockerSource/sparkSource/hadoop-3.1.0 /Users/yaosong/Yao/share/source/hadooprm -rf /Users/yaosong/Yao/share/source/hadoopln -s /Users/yaosong/Yao/DockerSource/sparkSource/hadoop-2.9.0 /Users/yaosong/Yao/share/source/hadooprm -rf /Users/yaosong/Yao/share/source/hadoopln -s /Users/yaosong/Yao/DockerSource/sparkSource/hadoop /Users/yaosong/Yao/share/source/hadoopln -s /Users/yaosong/Yao/DockerSource/elkSource/elasticsearch-head-master /Users/yaosong/Yao/share/source/elasticsearch-head-masterln -s /Users/yaosong/Yao/DockerSource/sparkSource/kafkaall/kafka1 /Users/yaosong/Yao/share/source/ln -s /Users/yaosong/Yao/DockerSource/sparkSource/kafkaall/kafka2 /Users/yaosong/Yao/share/source/ln -s /Users/yaosong/Yao/DockerSource/sparkSource/kafkaall/kafka3 /Users/yaosong/Yao/share/source/ln -s /Users/yaosong/Yao/DockerSource/elkSource/es1/ /Users/yaosong/Yao/share/source/ln -s /Users/yaosong/Yao/DockerSource/elkSource/es2/ /Users/yaosong/Yao/share/source/ln -s /Users/yaosong/Yao/DockerSource/elkSource/es3/ /Users/yaosong/Yao/share/source/ln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/kafka-start-all.sh /Users/yaosong/Yao/share/shell/kafka-start-all.shln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/es-start.sh /Users/yaosong/Yao/share/shell/es-start.shln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/filebeat-start.sh /Users/yaosong/Yao/share/shell/filebeat-start.shln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/hadoop-start.sh /Users/yaosong/Yao/share/shell/hadoop-start.shln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/hbase-start.sh /Users/yaosong/Yao/share/shell/hbase-start.shln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/hbasezkStart.sh /Users/yaosong/Yao/share/shell/hbasezkStart.shln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/header-start.sh /Users/yaosong/Yao/share/shell/header-start.shln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/hue-start.sh /Users/yaosong/Yao/share/shell/hue-start.shln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/kibana-start.sh /Users/yaosong/Yao/share/shell/kibana-start.shln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/logstash-start.sh /Users/yaosong/Yao/share/shell/logstash-start.shln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/spark-start.sh /Users/yaosong/Yao/share/shell/spark-start.shln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/zk-start.sh /Users/yaosong/Yao/share/shell/zk-start.shln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/hivebeeline.sh /Users/yaosong/Yao/share/shell/hivebeeline.shln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/hivemetastore.sh /Users/yaosong/Yao/share/shell/hivemetastore.shln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/hiveservers2.sh /Users/yaosong/Yao/share/shell/hiveservers2.shln -s /Users/yaosong/Yao/å¯åŠ¨è„šæœ¬/hivestart.sh /Users/yaosong/Yao/share/shell/hivestart.sh åœ¨virtualboxä¸­è®¾ç½®å…±äº«æ–‡ä»¶å¤¹çš„shareåç§°å¯¹åº”macçš„ç›®å½•è™šæ‹Ÿæœºä¸­çš„ç›®å½•sudo mount -t vboxsf share /Users/yaosong/Yao/share/sudo mount -t vboxsf yaosong /Users/yaosong sudo mount -t vboxsf vagrant /Users/yaosong ä»¥åç›¸å½“äºæ“ä½œ/Users/yaosong/Yao/share/å°±æ˜¯æ“ä½œçš„macç³»ç»Ÿ/Users/yaosong/Yao/share/ åˆ›å»ºdockerfile123456789101112131415161718192021222324252627282930313233343536373839404142FROM yaosong5/centosbase:3.0ENV JAVA_HOME=/usr/java/jdkENV PATH=$JAVA_HOME/bin:$PATHENV CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarENV SCALA_HOME=/usr/scala/ENV HADOOP_HOME=/usr/hadoopENV HADOOP_CONFIG_HOME=$HADOOP_HOME/etc/hadoopENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopENV PATH=$PATH:$HADOOP_HOME/binENV PATH=$PATH:$HADOOP_HOME/sbinENV SPARK_LOCAL_DIRS=/usr/sparkENV SPARK_DRIVER_MEMORY=1GENV SPARK_HOME=/usr/sparkENV PATH=$SPARK_HOME/bin:$PATHENV PATH=$SPARK_HOME/sbin:$PATHENV MAVEN_HOME=/usr/mavenENV PATH=$&#123;PATH&#125;:$&#123;MAVEN_HOME&#125;/binENV HIVE_HOME=/usr/hiveENV PATH=$HIVE_HOME/bin:$PATHENV KAFKA_HOME=/usr/kafkaENV PATH=$KAFKA_HOME/bin:$PATHENV ANT_HOME=/usr/antENV PATH=$JAVA_HOME/bin:$ANT_HOME/bin:$PATHENV ANT_HOME PATHENV HUE_HOME=/usr/hue4ENV ZK_HOME=/usr/zkENV HBASE_HOME=/usr/hbaseENV PATH=$HBASE_HOME/bin:$PATHENV PATH=$ZK_HOME/bin:$PATH#elkENV ES_HOME=/usr/esENV PATH=$ES_HOME/bin:$PATHENV KIBANA_HOME=/usr/kibanaENV PATH=$KIBANA_HOME/bin:$PATHENV LOGSTASH_HOME=/usr/logstashENV PATH=$LOGSTASH_HOME/bin:$PATHENV NODE_HOME=/usr/node-v6.12.0-linux-x64ENV PATH=$NODE_HOME/bin:$PATHENV NODE_PATH=$NODE_HOME/lib/node_modules#flinkENV FLINK_HOME=/usr/flinkENV PATH=$FLINK_HOME/bin:$PATH æ„å»ºé•œåƒdocker build -f Dockerfile -t yaosong5/bigdatabase:1.0 . åˆ†åˆ«æ„å»ºå®¹å™¨å’Œé•œåƒ ï¼ˆé‡‡ç”¨æŒ‚è½½çš„æ–¹å¼ï¼‰åˆ†åˆ«çš„å¯åŠ¨å‘½ä»¤ å…ˆåŠ è½½åå¯åŠ¨ä¸€ä¸ªhadoopé›†ç¾¤åˆ›å»ºä¸€ä¸ªmasterå®¹å™¨ä¸å«å¯åŠ¨å‚æ•°ï¼Œåªæ˜¯ç”¨åŸå§‹çš„é…ç½®æ–‡ä»¶ï¼Œåªå¯åŠ¨å®¹å™¨ï¼Œä¸ä½¿ç”¨ä»»ä½•å‘½ä»¤å¯åŠ¨ç»„ä»¶ ç”±äºä¸‹é¢ä¸‰ä¸ªç»„ä»¶éœ€è¦æŒ‚è½½ç›¸åŒçš„ç›®å½•é‚£ä¹ˆåˆ›å»ºä¸€ä¸ªå…±äº«çš„æ•°æ®å·é•œåƒ å‚è€ƒ ï¼šhttps://blog.csdn.net/magerguo/article/details/72514813https://blog.csdn.net/u013571243/article/details/51751367 docker run -it â€“rm â€“name datavol -h datavol -v /Users/yaosong/Yao/share/source/:/usr/ busybox:latest /bin/shdocker run -it â€“rm â€“name datavol -h datavol -v /Users/yaosong/Yao/share/source/:/usr/ busybox:latest /bin/shrmè¯¦è§£http://dockone.io/article/152 https://blog.csdn.net/taiyangdao/article/details/73076770 1234docker run -itd --name datavol -h datavol \\-v /Users/yaosong/Yao/share/source/hadoop:/usr/hadoop \\-v /Users/yaosong/Yao/share/data/hadoop/name:/usr/hadoop/namenode \\busybox:latest /bin/sh æ•°æ®å®¹å™¨å¸¸è§çš„ä½¿ç”¨åœºæ™¯æ˜¯ä½¿ç”¨çº¯æ•°æ®å®¹å™¨æ¥æŒä¹…åŒ–æ•°æ®åº“ã€é…ç½®æ–‡ä»¶æˆ–è€…æ•°æ®æ–‡ä»¶ç­‰ã€‚ å®˜æ–¹çš„æ–‡æ¡£ ä¸Šæœ‰è¯¦ç»†çš„è§£é‡Šã€‚ä¾‹å¦‚ï¼š 123&gt; $ docker run --name dbdata postgres echo &quot;Data-only container for postgres&quot;&gt;&gt; è¯¥å‘½ä»¤å°†ä¼šåˆ›å»ºä¸€ä¸ªå·²ç»åŒ…å«åœ¨ Dockerfile é‡Œå®šä¹‰è¿‡ Volume çš„ postgres é•œåƒï¼Œè¿è¡Œ 12&gt; echo&gt; å‘½ä»¤ç„¶åé€€å‡ºã€‚å½“æˆ‘ä»¬è¿è¡Œ 12&gt; docker ps&gt; å‘½ä»¤æ—¶ï¼Œ 12&gt; echo&gt; å¯ä»¥å¸®åŠ©æˆ‘ä»¬è¯†åˆ«æŸé•œåƒçš„ç”¨é€”ã€‚æˆ‘ä»¬å¯ä»¥ç”¨ 12&gt; -volumes-from&gt; ä½¿ç”¨æ•°æ®å®¹å™¨çš„ä¸¤ä¸ªæ³¨æ„ç‚¹ï¼š ä¸è¦è¿è¡Œæ•°æ®å®¹å™¨ï¼Œè¿™çº¯ç²¹æ˜¯åœ¨æµªè´¹èµ„æºã€‚ ä¸è¦ä¸ºäº†æ•°æ®å®¹å™¨è€Œä½¿ç”¨ â€œæœ€å°çš„é•œåƒâ€ï¼Œå¦‚busyboxæˆ–scratchï¼Œåªä½¿ç”¨æ•°æ®åº“é•œåƒæœ¬èº«å°±å¯ä»¥äº†ã€‚ä½ å·²ç»æ‹¥æœ‰è¯¥é•œåƒï¼Œæ‰€ä»¥å¹¶ä¸éœ€è¦å ç”¨é¢å¤–çš„ç©ºé—´ã€‚ å‚è€ƒhttp://dockone.io/article/128 å†ä½¿ç”¨ â€“volumes-from datavolç¤ºä¾‹ ï¼šdocker run -it â€“net=br â€“volumes-from dataVol ubuntu64 /bin/bash ä½¿ç”¨å…¬ç”¨æŒ‚è½½é•œåƒåˆ›å»ºåŸºç¡€é•œåƒ 12345678docker run -itd --name datavol -h datavol \\-v /Users/yaosong/Yao/share/source/hadoop:/usr/hadoop \\-v /Users/yaosong/Yao/share/data/hadoop/name:/usr/hadoop/namenode \\-v /Users/yaosong/Yao/share/source/spark:/usr/spark \\-v /Users/yaosong/Yao/share/source/flink:/usr/flink \\-v /Users/yaosong/Yao/share/source/ant:/usr/ant \\-v /Users/yaosong/Yao/share/source/maven:/usr/maven \\busybox:latest /bin/sh master slave01 slave021234567891011121314151617181920212223docker run -itd --net=br --name master -h master --volumes-from datavol \\-v /Users/yaosong/Yao/share/shell/hadoop-start.sh:/hadoop-start.sh yaosong5/bigadatabase:1.0 &amp;&gt; /dev/nulldocker run -itd --name slave01 --net=br -h slave01 --volumes-from datavol \\-v /Users/yaosong/Yao/share/data/hadoop/data1:/usr/hadoop/datanode \\-v /Users/yaosong/Yao/share/source/hadoop:/usr/hadoop yaosong5/bigadatabase:1.0 &amp;&gt; /dev/nulldocker run -itd --name slave02 --net=br -h slave02 --volumes-from datavol \\-v /Users/yaosong/Yao/share/data/hadoop/data2:/usr/hadoop/datanode \\-v /Users/yaosong/Yao/share/source/hadoop:/usr/hadoop yaosong5/bigadatabase:1.0 &amp;&gt; /dev/nulldocker run -itd --net=br --name master -h master --volumes-from datavol \\-v /Users/yaosong/Yao/share/shell/hadoop-start.sh:/hadoop-start.sh yaosong5/centosbase:1.0 &amp;&gt; /dev/nulldocker run -itd --name slave01 --net=br -h slave01 --volumes-from datavol \\-v /Users/yaosong/Yao/share/data/hadoop/data1:/usr/hadoop/datanode \\-v /Users/yaosong/Yao/share/source/hadoop:/usr/hadoop yaosong5/centosbase:1.0 &amp;&gt; /dev/nulldocker run -itd --name slave02 --net=br -h slave02 --volumes-from datavol \\-v /Users/yaosong/Yao/share/data/hadoop/data2:/usr/hadoop/datanode \\-v /Users/yaosong/Yao/share/source/hadoop:/usr/hadoop yaosong5/centosbase:1.0 &amp;&gt; /dev/null åˆ›å»ºå¤§èŠ‚ç‚¹ ä½¿ç”¨çš„1.1å°è£…äº†hue 1.1å°±æ˜¯2.0 12345678910111213docker run -itd --net=br --name master -h master --volumes-from datavol \\-v /Users/yaosong/Yao/share/shell/spark-start.sh:/spark-start.sh \\-v /Users/yaosong/Yao/share/shell/hadoop-start.sh:/hadoop-start.sh \\-v /Users/yaosong/Yao/share/source/hue4/desktop/conf/hue.ini:/usr/hue4/desktop/conf/hue.ini \\yaosong5/bigadatabase:1.0 &amp;&gt; /dev/nulldocker run -itd --name slave01 --net=br -h slave01 --volumes-from datavol \\-v /Users/yaosong/Yao/share/hadoop/data1:/usr/hadoop/datanode \\ yaosong5/bigadatabase:1.0 &amp;&gt; /dev/nulldocker run -itd --name slave02 --net=br -h slave02 --volumes-from datavol \\-v /Users/yaosong/Yao/share/hadoop/data2:/usr/hadoop/datanode \\yaosong5/bigadatabase:1.0 &amp;&gt; /dev/null åŸç”Ÿå®¹å™¨è‡ªèº«æŒ‚è½½é•œåƒmaster slave01 slave0212345678910111213141516171819docker run -itd --net=br --name master -h master \\-v /Users/yaosong/Yao/share/data/hadoop/name:/usr/hadoop/namenode \\-v /Users/yaosong/Yao/share/source/hadoop:/usr/hadoop \\-v /Users/yaosong/Yao/share/shell/hadoop-start.sh:/hadoop-start.sh yaosong5/bigadatabase:1.0 &amp;&gt; /dev/nulldocker run -itd --name slave01 \\--net=br -h slave01 \\-v /Users/yaosong/Yao/share/source/hadoop:/usr/hadoop \\-v /Users/yaosong/Yao/share/data/hadoop/name:/usr/hadoop/namenode \\-v /Users/yaosong/Yao/share/data/hadoop/data1:/usr/hadoop/datanode \\-v /Users/yaosong/Yao/share/source/hadoop:/usr/hadoop yaosong5/bigadatabase:1.0 &amp;&gt; /dev/nulldocker run -itd --name slave02 \\--net=br -h slave02 \\-v /Users/yaosong/Yao/share/source/hadoop:/usr/hadoop \\-v /Users/yaosong/Yao/share/data/hadoop/name:/usr/hadoop/namenode \\-v /Users/yaosong/Yao/share/data/hadoop/data2:/usr/hadoop/datanode \\-v /Users/yaosong/Yao/share/source/hadoop:/usr/hadoop yaosong5/bigadatabase:1.0 &amp;&gt; /dev/null zk/Users/yaosong/Yao/share/data/zk/zk1/data/myid/Users/yaosong/Yao/share/data/zk/zk2/data/myid/Users/yaosong/Yao/share/data/zk/zk3/data/myid/Users/yaosong/Yao/share/shell/zk-start.sh 123456789101112131415docker run -itd --name zk1 --net=br -h zk1 \\-v /Users/yaosong/Yao/share/source/zk:/usr/zk \\-v /Users/yaosong/Yao/share/shell/zk-start.sh:/zk-start.sh \\-v /Users/yaosong/Yao/share/data/zk/zk1/data/myid:/usr/zk/data/myid \\yaosong5/bigadatabase:1.0 &amp;&gt; /dev/nulldocker run -itd --name zk2 --net=br -h zk2 \\-v /Users/yaosong/Yao/share/source/zk:/usr/zk \\-v /Users/yaosong/Yao/share/data/zk/zk2/data/myid:/usr/zk/data/myid \\yaosong5/bigadatabase:1.0 &amp;&gt; /dev/nulldocker run -itd --name zk3 --net=br -h zk3 \\-v /Users/yaosong/Yao/share/source/zk:/usr/zk \\-v /Users/yaosong/Yao/share/data/zk/zk3/data/myid:/usr/zk/data/myid \\yaosong5/bigadatabase:1.0 &amp;&gt; /dev/null \bhbase1234567891011121314151617docker run -itd --name hbase1 --net=br -h hbase1 \\-v /Users/yaosong/Yao/share/source/hbase:/usr/hbase \\-v /Users/yaosong/Yao/share/shell/hbase-start.sh:/hbase-start.sh \\-v /Users/yaosong/Yao/share/shell/hbasezkStart.sh:/hbasezkStart.sh \\yaosong5/bigadatabase:1.0 &amp;&gt; /dev/nulldocker run -itd --name hbase2 --net=br -h hbase2 \\-v /Users/yaosong/Yao/share/source/hbase:/usr/hbase \\-v /Users/yaosong/Yao/share/shell/hbase-start.sh:/hbase-start.sh \\-v /Users/yaosong/Yao/share/shell/hbasezkStart.sh:/hbasezkStart.sh \\yaosong5/bigadatabase:1.0 &amp;&gt; /dev/nulldocker run -itd --name hbase3 --net=br -h hbase3 \\-v /Users/yaosong/Yao/share/source/hbase:/usr/hbase \\-v /Users/yaosong/Yao/share/shell/hbase-start.sh:/hbase-start.sh \\-v /Users/yaosong/Yao/share/shell/hbasezkStart.sh:/hbasezkStart.sh \\yaosong5/bigadatabase:1.0 &amp;&gt; /dev/null hive12345678910111213141516docker run -itd --name hive --net=br -h hive --volumes-from hivedatavol \\-v /Users/yaosong/Yao/share/source/hive:/usr/hive \\-v /Users/yaosong/Yao/share/shell/hive-start-metastore.sh:/hive-start-metastore.sh \\-v /Users/yaosong/Yao/share/shell/hive-start-servers2.sh:/hive-start-servers2.sh \\-v /Users/yaosong/Yao/share/shell/hive-start.sh:/hive-start.sh \\-v /Users/yaosong/Yao/share/shell/hivebeeline.sh:/hivebeeline.sh \\ yaosong5/bigadatabase:1.0 &amp;&gt; /dev/null docker run -itd --name hive --net=br -h hive \\-v /Users/yaosong/Yao/share/source/hive:/usr/hive \\-v /Users/yaosong/Yao/share/source/hadoop:/usr/hadoop \\-v /Users/yaosong/Yao/share/shell/hive-start-metastore.sh:/hive-start-metastore.sh \\-v /Users/yaosong/Yao/share/shell/hive-start-servers2.sh:/hive-start-servers2.sh \\-v /Users/yaosong/Yao/share/shell/hive-start.sh:/hive-start.sh \\-v /Users/yaosong/Yao/share/shell/hivebeeline.sh:/hivebeeline.sh \\ yaosong5/bigadatabase:1.0 &amp;&gt; /dev/null kafka/Users/yaosong/Yao/share/shell/kafka-start-all.sh /Users/yaosong/Yao/share/source/kafka 123456789101112docker run -itd --name kafka1 --net=br -h kafka1 \\-v /Users/yaosong/Yao/share/source/kafka:/usr/kafka \\-v /Users/yaosong/Yao/share/shell/kafka-start-all.sh:/kafka-start-all.sh \\yaosong5/bigadatabase:1.0 &amp;&gt; /dev/nulldocker run -itd --name kafka2 --net=br -h kafka2 \\-v /Users/yaosong/Yao/share/source/kafka:/usr/kafka \\yaosong5/bigadatabase:1.0 &amp;&gt; /dev/nulldocker run -itd --name kafka3 --net=br -h kafka3 \\-v /Users/yaosong/Yao/share/source/kafka:/usr/kafka \\yaosong5/bigadatabase:1.0 &amp;&gt; /dev/null spark12345678910111213docker run -itd --name spark1 --net=br -h spark1 --volumes-from datavol \\-v /Users/yaosong/Yao/share/source/spark:/usr/spark \\-v /Users/yaosong/Yao/share/shell/spark-start.sh:/spark-start.sh \\yaosong5/bigadatabase:1.0 &amp;&gt; /dev/nulldocker run -itd --name spark2 --net=br -h spark2 --volumes-from datavol \\-v /Users/yaosong/Yao/share/source/spark:/usr/spark \\yaosong5/bigadatabase:1.0 &amp;&gt; /dev/nulldocker run -itd --name spark3 --net=br -h spark3 --volumes-from datavol \\-v /Users/yaosong/Yao/share/source/spark:/usr/spark \\yaosong5/bigadatabase:1.0 &amp;&gt; /dev/null ç”¨ &amp;&gt; /dev/nullè¿™ç§æ–¹å¼sshdæœåŠ¡æ‰ä¼šå¯åŠ¨ åœæ­¢ and åˆ é™¤å®¹å™¨ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566docker start masterdocker start slave01docker start slave02docker stop masterdocker stop slave01docker stop slave02docker rm masterdocker rm slave01docker rm slave02docker stop datavol &amp;&amp; docker rm datavoldocker stop hivedocker rm hivedocker stop zk1docker stop zk2docker stop zk3docker rm zk1docker rm zk2docker rm zk3docker stop hbase1docker stop hbase2docker stop hbase3docker rm hbase1docker rm hbase2docker rm hbase3docker stop kafka1docker stop kafka2docker stop kafka3docker rm kafka1docker rm kafka2docker rm kafka3docker start spark1docker start spark2docker start spark3docker stop spark1docker stop spark2docker stop spark3docker rm spark1docker rm spark2docker rm spark3docker stop hadoop1docker stop hadoop2docker stop hadoop3docker rm hadoop1docker rm hadoop2docker rm hadoop3docker stop elk1docker stop elk2docker stop elk3docker rm elk1docker rm elk2docker rm elk3 composeç¼–æ’è¯´çš„","categories":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"}]},{"title":"Hiveæ­å»ºåŠå¯åŠ¨","slug":"Hiveæ­å»ºåŠå¯åŠ¨","date":"2018-07-19T18:51:21.978Z","updated":"2018-08-15T18:14:43.446Z","comments":true,"path":"2018/07/20/Hiveæ­å»ºåŠå¯åŠ¨/","link":"","permalink":"http://gangtieguo.cn/2018/07/20/Hiveæ­å»ºåŠå¯åŠ¨/","excerpt":"[TOC] é…ç½®HOMEä¸‹è½½hiveåŒ…ï¼Œå¹¶è§£å‹ 1http://archive.apache.org/dist/ ln -s hive-2.1.1 /usr/hive vi ~/.bashrc 1234567export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar$HIVE_HOME/bin/hiveexport HIVE_HOME=/usr/hivePATH=$HIVE_HOME/bin:$PATH#hiveä¾èµ–äºhadoop(å¯ä»¥ä¸è¿è¡Œåœ¨åŒä¸€ä¸»æœºï¼Œä½†æ˜¯éœ€è¦hadoopçš„é…ç½®)$HADOOP_HOME=/usr/hadoop source ~/.bashrc","text":"[TOC] é…ç½®HOMEä¸‹è½½hiveåŒ…ï¼Œå¹¶è§£å‹ 1http://archive.apache.org/dist/ ln -s hive-2.1.1 /usr/hive vi ~/.bashrc 1234567export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar$HIVE_HOME/bin/hiveexport HIVE_HOME=/usr/hivePATH=$HIVE_HOME/bin:$PATH#hiveä¾èµ–äºhadoop(å¯ä»¥ä¸è¿è¡Œåœ¨åŒä¸€ä¸»æœºï¼Œä½†æ˜¯éœ€è¦hadoopçš„é…ç½®)$HADOOP_HOME=/usr/hadoop source ~/.bashrc å®‰è£…mysql Hiveå…ƒæ•°æ®ä»‹ç»Hive å°†å…ƒæ•°æ®å­˜å‚¨åœ¨ RDBMS ä¸­ï¼Œä¸€èˆ¬å¸¸ç”¨ MySQL å’Œ Derbyã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒHive å…ƒæ•°æ®ä¿å­˜åœ¨å†…åµŒçš„ Derby æ•°æ®åº“ä¸­ï¼Œåªèƒ½å…è®¸ä¸€ä¸ªä¼šè¯è¿æ¥ï¼Œåªé€‚åˆç®€å•çš„æµ‹è¯•ã€‚å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ä¸é€‚ç”¨ï¼Œ ä¸ºäº†æ”¯æŒå¤šç”¨æˆ·ä¼šè¯ï¼Œåˆ™éœ€è¦ä¸€ä¸ªç‹¬ç«‹çš„å…ƒæ•°æ®åº“ï¼Œä½¿ç”¨ MySQL ä½œä¸ºå…ƒæ•°æ®åº“ï¼ŒHive å†…éƒ¨å¯¹ MySQL æä¾›äº†å¾ˆå¥½çš„æ”¯æŒï¼Œé…ç½®ä¸€ä¸ªç‹¬ç«‹çš„å…ƒæ•°æ®åº“ 1234567891011121314151617181920212223242526yum install -y mysql-serverchkconfig --add mysqldchkconfig mysqld onchkconfig --list mysqldservice mysqld startmysql -u root -pEnter password: //é»˜è®¤å¯†ç ä¸ºç©ºï¼Œè¾“å…¥åå›è½¦å³å¯set password for root@localhost=password('root'); å¯†ç è®¾ç½®ä¸ºrootset password for root@=password('root');é»˜è®¤æƒ…å†µä¸‹Mysqlåªå…è®¸æœ¬åœ°ç™»å½•ï¼Œæ‰€ä»¥åªéœ€é…ç½®root@localhostå°±å¥½è®¾ç½®æ‰€æœ‰ipè®¿é—®å¯†ç ä¸ºrootset password for root@%=password('root'); å¯†ç è®¾ç½®ä¸ºroot ï¼ˆå…¶å®è¿™ä¸€æ­¥å¯ä»¥ä¸é…ï¼‰è®¾ç½®masterè®¿é—®å¯†ç ä¸ºrootset password for root@master=password('root'); å¯†ç è®¾ç½®ä¸ºroot ï¼ˆå…¶å®è¿™ä¸€æ­¥å¯ä»¥ä¸é…ï¼‰æŸ¥è¯¢å¯†ç select user,host,password from mysql.user; æŸ¥çœ‹å¯†ç æ˜¯å¦è®¾ç½®æˆåŠŸè®¾ç½®æ‰€æœ‰ipå¯ä»¥é€šè¿‡rootè®¿é—®GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO 'hive'@'%' IDENTIFIED BY 'hive' WITH GRANT OPTION;mysql -uroot -prootcreate user 'hive' identified by 'hive';create user 'hive'@'%' identified by 'hive';create database hive; é…ç½®Hivemkdir iotmp cp hive-default.xml.template hive-site.xml vim hive-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;configuration&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive?characterEncoding=UTF-8&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/usr/hive/iotmp&lt;/value&gt; &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/usr/hive/iotmp&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/usr/hive/iotmp&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://master:9083&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; Hive çš„å…ƒæ•°æ®å¯ä»¥å­˜å‚¨åœ¨æœ¬åœ°çš„ MySQL ä¸­ï¼Œä½†æ˜¯å¤§å¤šæ•°æƒ…å†µä¼šæ˜¯ä¸€ä¸ª mysql é›†ç¾¤ï¼Œè€Œä¸”ä¸åœ¨æœ¬åœ°ã€‚æ‰€ä»¥åœ¨ hive ä¸­éœ€è¦å¼€å¯è¿œç¨‹ metastoreã€‚ç”±äºæˆ‘æ˜¯æœ¬åœ°çš„ mysqlï¼Œæˆ‘å°±ä¸é…ç½®ä¸‹åˆ—å±æ€§äº†ï¼Œä½†æ˜¯å¦‚æœæ˜¯è¿œç¨‹çš„ metastoreï¼Œé…ç½®ä¸‹é¢çš„å±æ€§ã€‚123456789101112&lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;&lt;/value&gt; &lt;description&gt;Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.transport.mode&lt;/name&gt; &lt;value&gt;http&lt;/value&gt; &lt;description&gt;Server transport mode. \"binary\" or \"http\".&lt;/description&gt;&lt;/property&gt;é“¾æ¥ï¼šhttps://www.jianshu.com/p/87b76a686216 Hiveå‘½ä»¤å¯åŠ¨hive123$HIVE_HOME/bin/hiveå¦‚æœè°ƒè¯•ï¼Œå¯ä»¥åŠ ä¸Šå‚æ•°$HIVE_HOME/bin/hivehive -hiveconf hive.root.logger=DEBUG,console å¯åŠ¨hiveserver2123$HIVE_HOME/bin/hive --service hiveserver2or nohup $HIVE_HOME/bin/hiveserver2 1&gt;/var/log/hiveserver.log 2&gt;/var/log/hiveserver.err &amp; hiveserverç«¯å£å·é»˜è®¤æ˜¯10000 hiveserver2æ˜¯å¦å¯åŠ¨netstat -nl|grep 10000 beelineå·¥å…·æµ‹è¯•ä½¿ç”¨jdbcæ–¹å¼è¿æ¥å¯ä»¥åœ¨éƒ¨ç½²äº†hiveä»»æ„èŠ‚ç‚¹ä¸Šç”¨beelineå»è¿æ¥ 12345678$HIVE_HOME/bin/beeline -u jdbc:hive2://hive:10000or$HIVE_HOME/bin/beeline -u jdbc:hive2://hive:10000 -n bigdata æœ€åä¸€ä¸ªå‚æ•°æ˜¯ç”¨æˆ·or $HIVE_HOME/bin/beeline å›è½¦ï¼Œè¿›å…¥beelineçš„å‘½ä»¤ç•Œé¢ è¾“å…¥å‘½ä»¤è¿æ¥hiveserver2 beeline&gt; !connect jdbc:hive2://hive:10000 ä½¿ç”¨beelineé€šè¿‡jdbcè¿æ¥ä¸Šä¹‹åå°±å¯ä»¥åƒclientä¸€æ ·æ“ä½œã€‚ hiveserver2ä¼šåŒæ—¶å¯åŠ¨ä¸€ä¸ªwebuiï¼Œç«¯å£å·é»˜è®¤ä¸º10002ï¼Œå¯ä»¥é€šè¿‡http://localhost:10002/è®¿é—®ç•Œé¢ä¸­å¯ä»¥çœ‹åˆ°Session/Query/Softwareç­‰ä¿¡æ¯ã€‚(æ­¤ç½‘é¡µåªå¯æŸ¥çœ‹ï¼Œä¸å¯ä»¥æ“ä½œhiveæ•°æ®ä»“åº“) å‚è€ƒhttps://blog.csdn.net/lblblblblzdx/article/details/79760959 å‚è€ƒhttps://www.cnblogs.com/netuml/p/7841387.html hiveå…ƒæ•°æ®åº“ï¼ˆå°±æ˜¯hiveåº“ï¼‰ä¸­æœ‰å‡ ä¸ªè¡¨ï¼ˆæœ‰å¥½å‡ åä¸ªï¼Œä½†åªéœ€è¦è®°ä½å…¶ä¸­å‡ ä¸ªå°±å¯ä»¥äº†ï¼‰ï¼Œæœ‰å“ªäº›å­—æ®µå…ƒæ•°æ®åº“æœ€é‡è¦çš„ä½œç”¨æ˜¯ä¿å­˜ä¸€äº›ä¿¡æ¯ï¼Œhiveçš„æè¿°ä¿¡æ¯ï¼Œæ¯”å¦‚è¯´hiveæœ‰å‡ ä¸ªdatabaseï¼Œæœ‰å‡ ä¸ªè¡¨ï¼Œè¿™äº›è¡¨å¯¹åº”çš„hdfsçš„åœ°å€åœ¨å“ªå„¿ï¼Œè¡¨æœ‰å‡ ä¸ªå­—æ®µï¼Œå»ºäº†å‡ ä¸ªåˆ†åŒºï¼Œåˆ›å»ºäº†å‡ ä¸ªè‡ªå®šä¹‰å‡½æ•° columns_v2è¡¨è®°å½•æ˜¯ä¸»å¤–é”®å…³ç³»è¡¨ï¼Œfuncsè¡¨ï¼Œæ˜¯å­˜çš„åˆ›å»ºäº†ä»€ä¹ˆå‡½æ•°partitionè¡¨æ˜¯è®°å½•çš„åˆ›å»ºäº†ä»€ä¹ˆåˆ†åŒº æŠ¥é”™ æŠ¥é”™ Hive 2.3.3 MetaException(message:Version information not found in metastore.) 1schematool -initSchema -dbType mysql å‚è€ƒhttps://stackoverflow.com/questions/50230515/hive-2-3-3-metaexceptionmessageversion-information-not-found-in-metastore http://sishuok.com/forum/blogPost/list/6221.html https://blog.csdn.net/nokia_hp/article/details/79054079","categories":[{"name":"å®‰è£…éƒ¨ç½²","slug":"å®‰è£…éƒ¨ç½²","permalink":"http://gangtieguo.cn/categories/å®‰è£…éƒ¨ç½²/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"},{"name":"Hive","slug":"Hive","permalink":"http://gangtieguo.cn/tags/Hive/"}]},{"title":"Docker-æ„å»ºå…å¯†sshå…å¯†é•œåƒ","slug":"Docker-æ„å»ºå…å¯†ç™»å½•é•œåƒ","date":"2018-07-19T18:49:08.996Z","updated":"2019-03-06T03:08:51.862Z","comments":true,"path":"2018/07/20/Docker-æ„å»ºå…å¯†ç™»å½•é•œåƒ/","link":"","permalink":"http://gangtieguo.cn/2018/07/20/Docker-æ„å»ºå…å¯†ç™»å½•é•œåƒ/","excerpt":"å…å¯†ç™»å½•å‚è€ƒçš„æ˜¯http://www.shushilvshe.com/data/docker-ssh.htmlæ–‡ä¸­æ¶‰åŠå‘½ä»¤12345sudo yum -y install openssh-server openssh-clientsssh-keygen -t rsacp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keyschmod 700 ~/.sshchmod 600 ~/.ssh/authorized_keys","text":"å…å¯†ç™»å½•å‚è€ƒçš„æ˜¯http://www.shushilvshe.com/data/docker-ssh.htmlæ–‡ä¸­æ¶‰åŠå‘½ä»¤12345sudo yum -y install openssh-server openssh-clientsssh-keygen -t rsacp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keyschmod 700 ~/.sshchmod 600 ~/.ssh/authorized_keys vim /etc/ssh/sshd_config æ‰¾åˆ°ä»¥ä¸‹å†…å®¹ï¼Œå¹¶å»æ‰æ³¨é‡Šç¬¦â€#â€œ 123RSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keys vim /etc/ssh/ssh_config 123Host * StrictHostKeyChecking no UserKnownHostsFile=/dev/null â€‹ æ­¤æ–‡ä¹Ÿå¯å‚è€ƒ http://www.voidcn.com/article/p-gxkeusey-ma.html https://blog.csdn.net/a85820069/article/details/78745899å‘ä½¿ç”¨ docker run -i -t â€“name c1 centos6.6:basic /bin/bash è¿è¡Œå®¹å™¨ï¼Œsshd æœåŠ¡æ˜¯ä¸å¼€å¯çš„ï¼Œå¿…é¡»å…ˆ - d åœ¨ç”¨ exec åˆ‡å…¥ã€‚ https://www.cnblogs.com/aiweixiao/p/5516974.html 1.ã€æŸ¥çœ‹æ˜¯å¦å¯åŠ¨ã€‘ å¯åŠ¨ SSH æœåŠ¡ â€œ/etc/init.d/sshd startâ€ã€‚ç„¶åç”¨ netstat -antulp | grep ssh çœ‹æ˜¯å¦èƒ½çœ‹åˆ°ç›¸å…³ä¿¡æ¯å°±å¯ä»¥äº†ã€‚ 2.ã€è®¾ç½®è‡ªåŠ¨å¯åŠ¨ã€‘ å¦‚ä½•è®¾ç½®æŠŠ ssh ç­‰ä¸€äº›æœåŠ¡éšç³»ç»Ÿå¼€æœºè‡ªåŠ¨å¯åŠ¨ï¼Ÿ æ–¹æ³•ä¸€ï¼š[root@localhost ~]# vi /etc/rc.local åŠ å…¥ï¼šservice sshd start æˆ– /etc/init.d/sshd start chmod 777 /etc/ssh/ssh_host_ecdsa_key 12345678910111213141516# å…å¯†ç™»å½•mkdir -p /root/.ssh touch /root/.ssh/config echo &quot;StrictHostKeyChecking no&quot; &gt; /root/.ssh/config sed -i &quot;a UserKnownHostsFile /dev/null&quot; /root/.ssh/config # å¼€æœºå¯åŠ¨RUN yum install -y openssh-server sudo RUN sed -i &apos;s/UsePAM yes/UsePAM no/g&apos; /etc/ssh/sshd_config # ä¸‹é¢è¿™ä¸¤å¥æ¯”è¾ƒç‰¹æ®Šï¼Œåœ¨centos6ä¸Šå¿…é¡»è¦æœ‰ï¼Œå¦åˆ™åˆ›å»ºå‡ºæ¥çš„å®¹å™¨sshdä¸èƒ½ç™»å½•RUN ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key RUN ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key sshæœåŠ¡æ–‡ç« å½“ä¸­çš„12345678910111213curl http://mirrors.aliyun.com/repo/Centos-6.repo &gt; /etc/yum.repos.d/CentOS-Base-6-aliyun.repomv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bakyum makecacheyum install -y net-tools which openssh-clients openssh-server iproute.x86_64 wgetservice sshd startsed -i 's/UsePAM yes/UsePAM no/g' /etc/ssh/sshd_configsed -ri 's/session required pam_loginuid.so/#session required pam_loginuid.so/g' /etc/pam.d/sshdchkconfig sshd oncd ~;ssh-keygen -t rsa -P '' -f ~/.ssh/id_dsa;cd .ssh;cat id_dsa.pub &gt;&gt; authorized_keys","categories":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"}]},{"title":"Docker-machineã€engineã€swarmé›†ç¾¤ç­‰ç›¸å…³","slug":"docker-machineç­‰çš„å…³ç³»","date":"2018-07-18T06:26:45.439Z","updated":"2018-07-27T08:34:52.084Z","comments":true,"path":"2018/07/18/docker-machineç­‰çš„å…³ç³»/","link":"","permalink":"http://gangtieguo.cn/2018/07/18/docker-machineç­‰çš„å…³ç³»/","excerpt":"","text":"[TOC] ç›®æ ‡ï¼š ææ‡‚dockerçš„å·¥ä½œæœºåˆ¶ï¼Œï¼ˆç½‘ç»œæœºåˆ¶åé¢æ¥ææ‡‚ï¼‰ dockerçš„é›†ç¾¤ï¼Œç›‘æ§æœºåˆ¶ å¦‚ä½•å¯¹é•œåƒæ›´æ”¹æ›´å°çš„é…ç½®ï¼Œå¯¹å¤–éƒ¨æ–‡ä»¶è¿›è¡Œä¿®æ”¹è¯»å–å°±å¯ä»¥è¿è¡Œdockerå®¹å™¨ã€å‚è€ƒã€‘https://toutiao.io/posts/rgo564/previewï¼Œæœ‰åˆ›å»ºswarmé›†ç¾¤çš„å‘½ä»¤ï¼Œé€šè¿‡ipåŠ å…¥é›†ç¾¤çš„å‘½ä»¤ Docker Engineï¼šä½œä¸º Docker é•œåƒæ„å»ºä¸å®¹å™¨åŒ–å¯åŠ¨çš„å·¥ä½œå¼•æ“ï¼›Docker Machineï¼šå®‰è£…ä¸ç®¡ç† Docker Engine çš„å·¥å…·ï¼›Docker Swarmï¼šæ˜¯ Docker è‡ª 1.12 åè‡ªå¸¦çš„é›†ç¾¤æŠ€æœ¯ï¼Œå°†å¤šä¸ªç‹¬ç«‹çš„ Docker Engine åˆ©ç”¨ Swarm æŠ€æœ¯è¿›è¡Œé›†ç¾¤åŒ–ç®¡ç†ï¼›ç®€å•æ¥è®²ï¼ŒDocker é›†ç¾¤çš„å®ç°æ˜¯é€šè¿‡ Docker Machine åœ¨å¤šä¸»æœºæˆ–è™šæ‹Ÿæœºä¸Šåˆ›å»ºå¤šä¸ª Docker Engineï¼Œåœ¨åˆ©ç”¨ Docker Swarm ä»¥æŸä¸ª Engine ä½œä¸ºé›†ç¾¤çš„åˆå§‹ç®¡ç†èŠ‚ç‚¹ï¼Œå…¶ä»– Engine ä»¥ç®¡ç†æˆ–å·¥ä½œèŠ‚ç‚¹çš„æ–¹å¼åŠ å…¥ï¼Œå½¢æˆå®Œæˆçš„é›†ç¾¤ç¯å¢ƒã€‚ swarm æ˜¯é›†ç¾¤å·¥å…·ï¼Œcompose æ˜¯è¿è¡Œå®¹å™¨çš„å¿«æ·å·¥å…· æœ¬æ¥å°±èƒ½ç»“åˆä½¿ç”¨ï¼Œä»–ä»¬ä¹‹é—´ä¸å†²çªï¼Œå†åŠ  machine å¯ä»¥åˆ›å»ºå¤šä¸ª docker è™šæ‹Ÿç¯å¢ƒï¼ˆå¦‚æœä½ æ²¡æœ‰å¤šå°æœåŠ¡å™¨æµ‹è¯•çš„è¯ï¼‰ä¸‰è€…ä¸€èµ·ï¼Œå»ºç«‹å¼€å‘æµ‹è¯•é›†ç¾¤ç¯å¢ƒç®€å•æ–¹ä¾¿å¿«æ·ï¼ http://dockone.io/question/160Machineï¼šè§£å†³çš„æ˜¯æ“ä½œç³»ç»Ÿå¼‚æ„å®‰è£… Docker å›°éš¾çš„é—®é¢˜ï¼Œæ²¡æœ‰ Machine çš„æ—¶å€™ï¼ŒCentOS æ˜¯ä¸€ç§ï¼ŒUbuntu åˆæ˜¯ä¸€ç§ï¼ŒAWS åˆæ˜¯ä¸€ç§ã€‚æœ‰äº† Machineï¼Œæ‰€æœ‰çš„ç³»ç»Ÿéƒ½æ˜¯ä¸€æ ·çš„å®‰è£…æ–¹å¼ã€‚ Swarmï¼šæˆ‘ä»¬æœ‰äº† Machine å°±æ„å‘³ç€æœ‰äº† docker ç¯å¢ƒï¼Œä½†æ˜¯é‚£æ˜¯å•æœºçš„ï¼Œè€Œé€šå¸¸æˆ‘ä»¬çš„åº”ç”¨éƒ½æ˜¯é›†ç¾¤çš„ã€‚è¿™æ­£æ˜¯ Swarm è¦åšçš„äº‹æƒ…ï¼Œç»™ä½ æä¾› docker é›†ç¾¤ç¯å¢ƒå’Œè°ƒåº¦ç­–ç•¥ç­‰ã€‚ Composeï¼šæœ‰äº†ç¯å¢ƒï¼Œæˆ‘ä»¬ä¸‹ä¸€æ­¥è¦åšä»€ä¹ˆï¼Ÿéƒ¨ç½²åº”ç”¨å•Šã€‚ç„¶åæˆ‘ä»¬éœ€è¦ docker run image1ã€docker run image2â€¦ ä¸€æ¬¡ä¸€æ¬¡ä¸åŒå…¶çƒ¦çš„é‡å¤è¿™äº›æ“ä½œï¼Œæ¯æ¬¡éƒ½å†™å¤§é‡çš„å‘½ä»¤å‚æ•°ã€‚Compose ç®€åŒ–äº†è¿™ä¸ªæµç¨‹ï¼Œåªéœ€è¦æŠŠè¿™äº›å†…å®¹å›ºè¯åˆ° docker-compose.yml ä¸­ã€‚ ç›®å‰ Machineã€Swarmã€Compose å·²ç»å¯ä»¥ç»“åˆä½¿ç”¨ï¼Œåˆ›å»ºé›†ç¾¤ç¯å¢ƒï¼Œç®€å•çš„åœ¨ä¸Šé¢éƒ¨ç½²åº”ç”¨ã€‚ä½†æ˜¯è¿˜ä¸å®Œå–„ï¼Œæ¯”å¦‚å¯¹äºæœ‰ link çš„åº”ç”¨ï¼Œå®ƒä»¬åªèƒ½è·‘åœ¨ Swarm é›†ç¾¤çš„ä¸€ä¸ªæœºå™¨ä¸Šï¼Œå³ä½¿ä½ çš„é›†ç¾¤æœ‰å¾ˆå¤šæœºå™¨ã€‚å¯ä»¥å‚è€ƒæˆ‘çš„å¦ä¸€ä¸ªé—®é¢˜ã€‚ SocketPlane æ˜¯ Docker æœ€è¿‘æ”¶è´­çš„äº§å“ï¼ŒçŒœæƒ³åº”è¯¥æ˜¯ä¸ºäº†å¼ºåŒ– Docker çš„ç½‘ç»œåŠŸèƒ½ï¼Œæ¯”å¦‚æä¾›åŸç”Ÿè·¨ä¸»æœºçš„ç½‘ç»œå®šåˆ¶ã€å¼ºåŒ– Swarm å’Œ Compose çš„ç»“åˆç­‰ã€‚ dockeræŒ‚è½½https://www.cnblogs.com/ivictor/p/4834864.html å®¿ä¸»æœºç›®å½•å’Œå®¹å™¨ç›®å½•çš„å…³ç³» Docker-engine ï¼š å½“äººä»¬è¯´ â€œDockerâ€ æ—¶ï¼Œä»–ä»¬é€šå¸¸æ„å‘³ç€ Docker Engineï¼Œå³ç”± Docker å®ˆæŠ¤ç¨‹åºç»„æˆçš„å®¢æˆ·ç«¯ - æœåŠ¡å™¨åº”ç”¨ç¨‹åºï¼ŒæŒ‡å®šç”¨äºä¸å®ˆæŠ¤ç¨‹åºè¿›è¡Œäº¤äº’çš„æ¥å£çš„ REST API ä»¥åŠä¸å®ˆæŠ¤ç¨‹åºï¼ˆé€šè¿‡ REST API åŒ…è£…å™¨ï¼‰é€šä¿¡çš„å‘½ä»¤è¡Œç•Œé¢ï¼ˆCLIï¼‰å®¢æˆ·ç«¯ã€‚Docker Engine ä» CLI æ¥å—dockerå‘½ä»¤ï¼Œä¾‹å¦‚docker run ï¼Œdocker psåˆ—å‡ºè¿è¡Œçš„å®¹å™¨ï¼Œdocker imagesä»¥åˆ—å‡ºé•œåƒç­‰ Docker-machine: Mac OS X é»˜è®¤æ˜¯æ²¡æœ‰è™šæ‹Ÿæœºçš„ï¼Œæ‰€ä»¥è¦ä½¿ç”¨ Docker Machineï¼Œéœ€è¦äº‹å…ˆå®‰è£…å¥½ VirtualBoxï¼Œè¿™åœ¨ Docker çš„å®˜æ–¹æ–‡æ¡£ä¸Šæœ‰ç›¸å…³è¯´æ˜Docker Machine æ˜¯ä¸€ä¸ªå·¥å…·ï¼Œç”¨æ¥åœ¨è™šæ‹Ÿä¸»æœºä¸Šå®‰è£… Docker Engineå¹¶ä½¿ç”¨ docker-machine å‘½ä»¤æ¥ç®¡ç†è¿™äº›è™šæ‹Ÿä¸»æœº Docker MachineDocker Machine æ˜¯ç”¨æ¥é…ç½®å’Œç®¡ç† Docker Engine çš„å·¥å…·ã€‚Docker Machine ä¸»è¦æœ‰ä¸¤ä¸ªç”¨é€”ï¼šä¸€æ˜¯ä¸ºäº†èƒ½åœ¨æ—§çš„ Mac å’Œ Windows ç³»ç»Ÿä¸Šä½¿ç”¨ Dokcerã€‚äºŒæ˜¯ä¸ºäº†å¯ä»¥ç®¡ç†è¿œç¨‹çš„ Dockerã€‚ è¦çŸ¥é“ï¼ŒDocker Engine æ˜¯åªèƒ½åœ¨ Linux å†…æ ¸ä¸Šè¿è¡Œçš„ã€‚ä¸ºä»€ä¹ˆ Docker Machine å¯ä»¥ä½¿å¾—åœ¨æ—§çš„ Mac å’Œ Windows ç³»ç»Ÿä¸Šä¹Ÿèƒ½ä½¿ç”¨ Docker å‘¢ï¼Ÿå› ä¸ºï¼Œå®‰è£… Docker Toolbox ä¼šå®‰è£… Docker Machine å’Œ Virtual Box è™šæ‹Ÿæœºï¼Œå®ƒä¼šé…ç½®ä¸€ä¸ª Docker Engine åˆ°ä¸€ä¸ªè™šæ‹Ÿæœºé‡Œçš„ä¸»æœºä¸Šï¼ˆLinux å†…æ ¸ï¼Œé»˜è®¤åå­—æ˜¯ defaultï¼Œåˆç§°æ‰˜ç®¡ä¸»æœºï¼‰ï¼Œåç»­ Docker å®¹å™¨å°±æ˜¯è¿è¡Œåœ¨è¿™ä¸ªæ‰˜ç®¡ä¸»æœºé‡Œçš„ Docker Engine ä¹‹ä¸Šã€‚ å°† Machine CLI æŒ‡å‘æ­£åœ¨è¿è¡Œçš„æ‰˜ç®¡ä¸»æœºï¼Œå°±å¯ä»¥ç›´æ¥è¯¥è¿è¡Œdockerå‘½ä»¤åˆ°è¯¥ä¸»æœºä¸Šã€‚è¿™é‡Œè²Œä¼¼ä¸œè¥¿ä¸å°‘ï¼šdocker-machine env defaultä¼šæ˜¾ç¤º default ä¸»æœºçš„ç¯å¢ƒä¿¡æ¯ä»¥åŠå¯¹åº”çš„é…ç½®å‘½ä»¤æŒ‡å¼•ï¼š 1234export DOCKER_TLS_VERIFY=\"1\"export DOCKER_HOST=\"tcp://192.168.99.100:2376\"export DOCKER_CERT_PATH=\"/Users/Eason/.docker/machine/machines/default\"export DOCKER_MACHINE_NAME=\"default\" Run this command to configure your shell:1eval &quot;$(docker-machine env default)&quot; æç¤ºå¾ˆæ˜ç™½äº†ï¼Œåœ¨ SHELL é‡Œè¿è¡Œ eval â€œ$(docker-machine env default) å°±å¯ä»¥æŠŠå½“å‰ Machine CLI æŒ‡å‘ default ä¸»æœºäº†ã€‚å¥½å¤„å°±æ˜¯ï¼Œå½“æˆ‘ä»¬æœ‰å¤šä¸ªä¸»æœºæ—¶ï¼Œè¿™æ ·ä¸€è¡Œå‘½ä»¤å°±èƒ½åˆ‡æ¢ç¯å¢ƒæŒ‡å‘ä¸åŒçš„ä¸»æœºï¼Œç„¶ååç»­è¿è¡Œçš„dockerå‘½åä¹Ÿæ˜¯è¿è¡Œåˆ°å¯¹åº”çš„ Docker Enigne ä¸Šã€‚ æ³¨ï¼šDocker for Mac å’Œ Docker for Windows ä»¥åŠ (https://docs.docker.com/toolbox/overview/) éƒ½æ˜¯è‡ªå¸¦ Docker Machine å·¥å…·çš„ã€‚ è¿™ä¸€èŠ‚çš„ä¸ªäººç†è§£ï¼Œä¹Ÿä¸çŸ¥é“æˆ‘å†™å¾—å¤§å®¶çœ‹ä¸çœ‹å¾—æ‡‚ã€‚å…¶å®ä¸»è¦è¾¨æç‚¹å°±æ˜¯ï¼šContainer å®¹å™¨æ˜¯ Image é•œåƒè¿è¡Œåœ¨å†…å­˜ä¸­çš„å®ä¾‹ï¼ŒContainer å®¹å™¨è¦è¿è¡Œåœ¨ Docker Engine ä¹‹ä¸Šï¼Œç„¶è€Œ Docker Engine ä»…é™è¿è¡Œåœ¨ Linux å†…æ ¸ä¹‹ä¸Šã€‚Docker Machie æ˜¯é…ç½®å’Œç®¡ç† Docker Engine çš„å·¥å…·ï¼Œå¹¶ä¸”å¯ä»¥ä¸ºæ—§ç‰ˆçš„ Mac å’Œ Windows ç³»ç»Ÿæä¾› Docker Engine çš„è¿è¡Œç¯å¢ƒ\b Docker-machine:12Docker Machine æ˜¯ä¸€ä¸ªå·¥å…·ï¼Œç”¨æ¥åœ¨è™šæ‹Ÿä¸»æœºä¸Šå®‰è£… Docker Engineå¹¶ä½¿ç”¨ docker-machine å‘½ä»¤æ¥ç®¡ç†è¿™äº›è™šæ‹Ÿä¸»æœº swarmswarm æ˜¯é›†ç¾¤å·¥å…·ï¼Œcompose æ˜¯è¿è¡Œå®¹å™¨çš„å¿«æ·å·¥å…· æœ¬æ¥å°±èƒ½ç»“åˆä½¿ç”¨ï¼Œä»–ä»¬ä¹‹é—´ä¸å†²çªï¼Œå†åŠ  machine å¯ä»¥åˆ›å»ºå¤šä¸ª docker è™šæ‹Ÿç¯å¢ƒï¼ˆå¦‚æœä½ æ²¡æœ‰å¤šå°æœåŠ¡å™¨æµ‹è¯•çš„è¯ï¼‰ä¸‰è€…ä¸€èµ·ï¼Œ http://dockone.io/question/160 Machineï¼šè§£å†³çš„æ˜¯æ“ä½œç³»ç»Ÿå¼‚æ„å®‰è£… Docker å›°éš¾çš„é—®é¢˜ï¼Œæ²¡æœ‰ Machine çš„æ—¶å€™ï¼ŒCentOS æ˜¯ä¸€ç§ï¼ŒUbuntu åˆæ˜¯ä¸€ç§ï¼ŒAWS åˆæ˜¯ä¸€ç§ã€‚æœ‰äº† Machineï¼Œæ‰€æœ‰çš„ç³»ç»Ÿéƒ½æ˜¯ä¸€æ ·çš„å®‰è£…æ–¹å¼ã€‚ Swarmï¼šæˆ‘ä»¬æœ‰äº† Machine å°±æ„å‘³ç€æœ‰äº† docker ç¯å¢ƒï¼Œä½†æ˜¯é‚£æ˜¯å•æœºçš„ï¼Œè€Œé€šå¸¸æˆ‘ä»¬çš„åº”ç”¨éƒ½æ˜¯é›†ç¾¤çš„ã€‚è¿™æ­£æ˜¯ Swarm è¦åšçš„äº‹æƒ…ï¼Œç»™ä½ æä¾› docker é›†ç¾¤ç¯å¢ƒå’Œè°ƒåº¦ç­–ç•¥ç­‰ã€‚ Composeï¼šæœ‰äº†ç¯å¢ƒï¼Œæˆ‘ä»¬ä¸‹ä¸€æ­¥è¦åšä»€ä¹ˆï¼Ÿéƒ¨ç½²åº”ç”¨å•Šã€‚ç„¶åæˆ‘ä»¬éœ€è¦ docker run image1ã€docker run image2â€¦ ä¸€æ¬¡ä¸€æ¬¡ä¸åŒå…¶çƒ¦çš„é‡å¤è¿™äº›æ“ä½œï¼Œæ¯æ¬¡éƒ½å†™å¤§é‡çš„å‘½ä»¤å‚æ•°ã€‚Compose ç®€åŒ–äº†è¿™ä¸ªæµç¨‹ï¼Œåªéœ€è¦æŠŠè¿™äº›å†…å®¹å›ºè¯åˆ° docker-compose.yml ä¸­ã€‚ ç›®å‰ Machineã€Swarmã€Compose å·²ç»å¯ä»¥ç»“åˆä½¿ç”¨ï¼Œåˆ›å»ºé›†ç¾¤ç¯å¢ƒï¼Œç®€å•çš„åœ¨ä¸Šé¢éƒ¨ç½²åº”ç”¨ã€‚ä½†æ˜¯è¿˜ä¸å®Œå–„ï¼Œæ¯”å¦‚å¯¹äºæœ‰ link çš„åº”ç”¨ï¼Œå®ƒä»¬åªèƒ½è·‘åœ¨ Swarm é›†ç¾¤çš„ä¸€ä¸ªæœºå™¨ä¸Šï¼Œå³ä½¿ä½ çš„é›†ç¾¤æœ‰å¾ˆå¤šæœºå™¨ã€‚å¯ä»¥å‚è€ƒæˆ‘çš„å¦ä¸€ä¸ªé—®é¢˜ã€‚ SocketPlane æ˜¯ Docker æœ€è¿‘æ”¶è´­çš„äº§å“ï¼ŒçŒœæƒ³åº”è¯¥æ˜¯ä¸ºäº†å¼ºåŒ– Docker çš„ç½‘ç»œåŠŸèƒ½ï¼Œæ¯”å¦‚æä¾›åŸç”Ÿè·¨ä¸»æœºçš„ç½‘ç»œå®šåˆ¶ã€å¼ºåŒ– Swarm å’Œ Compose çš„ç»“åˆç­‰ã€‚ dockeræŒ‚è½½https://www.cnblogs.com/ivictor/p/4834864.html å®¿ä¸»æœºç›®å½•å’Œå®¹å™¨ç›®å½•çš„å…³ç³» swarmé›†ç¾¤æ­å»ºçš„æ­¥éª¤é¦–å…ˆ docker-machine sshåœ¨default ä½¿ç”¨å‘½ä»¤ docker swarm init","categories":[{"name":"åŸç†ç†è§£","slug":"åŸç†ç†è§£","permalink":"http://gangtieguo.cn/categories/åŸç†ç†è§£/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"}]},{"title":"CoudearManageræ­å»º","slug":"CoudearManageræ­å»º","date":"2018-07-10T14:10:04.881Z","updated":"2018-08-10T19:05:49.436Z","comments":true,"path":"2018/07/10/CoudearManageræ­å»º/","link":"","permalink":"http://gangtieguo.cn/2018/07/10/CoudearManageræ­å»º/","excerpt":"[TOC] åˆ›å»ºåŸºç¡€å®¹å™¨1docker run -itd --net=br --name cm --hostname cm yaosong5/centosbase:1.0 &amp;&gt; /dev/null å°†ä¸‹è½½çš„åŒ…è¿›è¡Œè§£å‹ç„¶åè¿›è¡Œæ‹·è´12345docker cp /Users/yaosong/Yao/cloudera-manager-el6-cm5.9.0_x86_64/cloudera cm:/opt/docker cp /Users/yaosong/Yao/cloudera-manager-el6-cm5.9.0_x86_64/cm-5.9.0 cm:/opt/docker cp /Users/yaosong/Yao/mysql-connector-java-5.1.40-bin.jar cm:/opt/cm-5.9.0/share/cmf/lib/docker cp /Users/yaosong/Yao/mysql-connector-java.jar cm:/usr/share/java/docker cp /Users/yaosong/Yao/jdk1.8 cm:/usr/local/","text":"[TOC] åˆ›å»ºåŸºç¡€å®¹å™¨1docker run -itd --net=br --name cm --hostname cm yaosong5/centosbase:1.0 &amp;&gt; /dev/null å°†ä¸‹è½½çš„åŒ…è¿›è¡Œè§£å‹ç„¶åè¿›è¡Œæ‹·è´12345docker cp /Users/yaosong/Yao/cloudera-manager-el6-cm5.9.0_x86_64/cloudera cm:/opt/docker cp /Users/yaosong/Yao/cloudera-manager-el6-cm5.9.0_x86_64/cm-5.9.0 cm:/opt/docker cp /Users/yaosong/Yao/mysql-connector-java-5.1.40-bin.jar cm:/opt/cm-5.9.0/share/cmf/lib/docker cp /Users/yaosong/Yao/mysql-connector-java.jar cm:/usr/share/java/docker cp /Users/yaosong/Yao/jdk1.8 cm:/usr/local/ å°† parcel æ–‡ä»¶æ”¾è‡³ /opt/cloudera/parcel-repo123docker cp /Users/yaosong/Yao/CDH-5.9.0-1.cdh5.9.0.p0.23-el6.parcel cm:/opt/cloudera/parcel-repodocker cp /Users/yaosong/Yao/CDH-5.9.0-1.cdh5.9.0.p0.23-el6.parcel.sha cm:/opt/cloudera/parcel-repodocker cp /Users/yaosong/Yao/manifest.json cm:/opt/cloudera/parcel-repo vim /etc/profile 123export JAVA_HOME=/usr/local/jdk1.8export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar åˆå§‹åŒ– mysql åº“1/opt/cm-5.9.0/share/cmf/schema/scm_prepare_database.sh mysql cm -hlocalhost -uroot -proot --scm-host localhost scm scm scm åˆ›å»ºç”¨æˆ·ï¼ˆæ‰€æœ‰èŠ‚ç‚¹æ‰§è¡Œï¼‰1useradd --system --home=/opt/cm-5.9.0/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm Agent é…ç½®vim /opt/cm-5.9.0/etc/cloudera-scm-agent/config.ini å°† server_host æ”¹ä¸ºä¸»èŠ‚ç‚¹ä¸»æœºå 1server_host=cm1 å®‰è£…mysqlchkconfig mysqld on è®¾ç½®å…è®¸è¿œç¨‹ç™»å½• 12mysql -u root -p GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION; åˆ›å»ºCMç”¨çš„æ•°æ®åº“å®‰è£…é›†ç¾¤æ—¶æŒ‰éœ€åˆ›å»ºï¼Œè¯¦è§ç¬¬ä¸ƒç« ç¬¬13æ­¥ 12345678910--hiveæ•°æ®åº“create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;--oozieæ•°æ®åº“create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;--hueæ•°æ®åº“create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database monitor DEFAULT CHARSET utf8 COLLATE utf8_general_ci; Clouderaæ¨èè®¾ç½®åœ¨è¯•å®‰è£…çš„è¿‡ç¨‹ï¼Œå‘ç°Clouderaç»™å‡ºäº†ä¸€äº›è­¦å‘Š èº«ä¸ºä¸€ä¸ªæœ‰æ´ç™–çš„ç å†œï¼Œè‡ªç„¶æ˜¯è¿é»„è‰²çš„æ„Ÿå¹å·éƒ½è¦æ¶ˆç­çš„ã€‚å› æ­¤åœ¨å®‰è£…CM/CDHä¹‹å‰å°±å…ˆå…¨éƒ¨è®¾ç½®å¥½ã€‚ 1ã€è®¾ç½®swapç©ºé—´vim /etc/sysctl.confæœ«å°¾åŠ ä¸Švm.swappiness=10 2ã€å…³é—­å¤§é¡µé¢å‹ç¼©è¯•è¿‡åªè®¾ç½®defragï¼Œä½†è²Œä¼¼ä¸ªåˆ«èŠ‚ç‚¹è¿˜æ˜¯ä¼šæœ‰è­¦å‘Šï¼Œå¹²è„†å…¨éƒ¨è®¾ç½® vim /etc/rc.localæœ«å°¾åŠ ä¸Š(æ°¸ä¹…ç”Ÿæ•ˆ)echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defrag å¯åŠ¨cloudera manager æœåŠ¡/opt/cm-5.9.0/etc/init.d/cloudera-scm-server start /opt/cm-5.9.0/etc/init.d/cloudera-scm-agent start ç«¯å£ 7180 ä¿å­˜ä¸ºé•œåƒdocker commit -m &quot;cloudera manger image&quot; cm yaosong5/cm59:1.0 åˆ›å»ºå®¹å™¨docker run -itd --net=br --name cm1 --hostname cm1 yaosong5/cm59:1.0 &amp;&gt; /dev/null docker run -itd --net=br --name cm2 --hostname cm2 yaosong5/cm59:1.0 &amp;&gt; /dev/null docker run -itd --net=br --name cm3 --hostname cm3 yaosong5/cm59:1.0 &amp;&gt; /dev/null 1234567docker stop cm1docker stop cm2docker stop cm3docker rm cm1docker rm cm2docker rm cm3 è°ƒé”™ .SearchRepositoryManager: No read permission to the server storage directory [/var/lib/cloudera-scm-server]2018-07-11 10:20:39,788 ERROR SearchRepositoryManager-0:com.cloudera.server.web.cmf.search.components.SearchRepositoryManager: No write permission to the server storage directory [/var/lib/cloudera-scm-server] é“¾æ¥hueè¿æ¥ä¸ä¸ŠèŠ‚ç‚¹çš„ cm-5.x.0/log/cloudera-scm-server/cloudera-scm-server.logï¼Œä¸€èˆ¬æƒ…å†µä¸‹åº”è¯¥ä¼šè¯´åˆ° ImportError:libxslt.so.1:cannot open shared object file:No such file ordirectory 1yum -y install libxml2-python æç¤ºhueæµ‹è¯•è¿æ¥è¿æ¥ä¸ä¸Šï¼Œå®‰è£…ä¾èµ–ï¼š1yum install libxml2-python mod_ssl install krb5-devel cyrus-sasl-gssapi cyrus-sasl-deve libxml2-devel libxslt-devel mysql mysql-devel openldap-devel python-devel python-simplejson sqlite-devel -y","categories":[{"name":"å®‰è£…éƒ¨ç½²","slug":"å®‰è£…éƒ¨ç½²","permalink":"http://gangtieguo.cn/categories/å®‰è£…éƒ¨ç½²/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"},{"name":"CDH","slug":"CDH","permalink":"http://gangtieguo.cn/tags/CDH/"}]},{"title":"Docker-Ambarié•œåƒæ­å»º","slug":"Docker-yaosong5:Ambaré•œåƒiåˆ¶ä½œ","date":"2018-07-09T17:08:44.939Z","updated":"2019-03-06T03:07:21.049Z","comments":true,"path":"2018/07/10/Docker-yaosong5:Ambaré•œåƒiåˆ¶ä½œ/","link":"","permalink":"http://gangtieguo.cn/2018/07/10/Docker-yaosong5:Ambaré•œåƒiåˆ¶ä½œ/","excerpt":"[TOC] åˆ›å»ºåŸºç¡€å®¹å™¨1docker run -itd --net=br --name ambari-agent --hostname ambari-agent yaosong5/centosbase:1.0 &amp;&gt; /dev/null å…³é—­ selinux , éœ€è¦é‡å¯vim /etc/selinux/config 1SELINUX=disabled","text":"[TOC] åˆ›å»ºåŸºç¡€å®¹å™¨1docker run -itd --net=br --name ambari-agent --hostname ambari-agent yaosong5/centosbase:1.0 &amp;&gt; /dev/null å…³é—­ selinux , éœ€è¦é‡å¯vim /etc/selinux/config 1SELINUX=disabled serverç«¯æ›´æ¢yumæº12wget http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.0.1/ambari.repocp ambari.repo /etc/yum.repos.d å®‰è£…ä¾èµ–åŠå…¶server123yum install epel-release yum repolistyum install ambari-server å¯åŠ¨åˆå§‹åŒ–ambari-server setupä¼šæœ‰ä¸€è¿ä¸²çš„æç¤º ä¼šæç¤ºå®‰è£… jdkï¼Œç½‘é€Ÿå¥½çš„å¯ä»¥ç¡®å®šï¼Œå¦åˆ™å¯ä»¥ä¸‹è½½ jdk-6u31-linux-x64.binï¼Œæ”¾åˆ° /var/lib/ambari-server/resources/ ä¸‹é¢ï¼Œå¯ä»¥æŒ‡å®šå·²ç»å®‰è£…çš„jdkæ¥ç€ä¼šæç¤ºé…ç½®ç”¨çš„æ•°æ®åº“ï¼Œå¯ä»¥é€‰æ‹© Oracle æˆ– postgresqlï¼Œé€‰æ‹© n ä¼šæŒ‰é»˜è®¤é…ç½®æ•°æ®åº“ç±»å‹ï¼špostgresqlæ•°æ®åº“ï¼šambariç”¨æˆ·åï¼šambariå¯†ç ï¼šbigdataå¦‚æœæç¤º Oracle JDK licenseï¼Œyesç­‰å¾…å®‰è£…å®Œæˆ agentç«¯å®‰è£… ambari-agent 12yum install -y ambari-agentchkconfig --add ambari-agent å°† ambari.server ä¸Šçš„ 3 ä¸ª. repo æ–‡ä»¶å¤åˆ¶åˆ° hadoop é›†ç¾¤çš„ä¸‰å°æœåŠ¡å™¨ä¸Šï¼›å¹¶å®Œæˆ yum æºæ›´æ–°çš„å‘½ä»¤ã€‚ å®‰è£… ambari-agentï¼šåœ¨é›†ç¾¤çš„ 3 å°ç”µè„‘ä¸Šæ‰§è¡Œæ·»åŠ ï¼Œå¹¶æ·»åŠ æˆå¼€æœºè‡ªå¯åŠ¨æœåŠ¡ï¼š yum install -y ambari-agent chkconfig â€“add ambari-agent sudo ambari-agent start åˆ†åˆ«å¯åŠ¨server agentåœ¨serverå’Œagentä¸Šåˆ†åˆ«æ‰§è¡Œ 12ambari-agent startambari-server start è®¿é—®http://192.168.1.133:8080 ç”¨æˆ·åå¯†ç : admin,admin ä¿å­˜å®¹å™¨ä¸ºé•œåƒ1docker commit -m \"bigdata:ambari-server\" --author=\"yaosong\" ambr yaosong5/ambari-server:1.0 æ ¹æ®é•œåƒåˆ›å»ºå®¹å™¨123docker run -itd --net=br --name ambari1 --hostname ambari1 yaosong5/ambari-server:1.0 &amp;&gt; /dev/nulldocker run -itd --net=br --name ambari2 --hostname ambari2 yaosong5/ambari-server:1.0 &amp;&gt; /dev/nulldocker run -itd --net=br --name ambari3 --hostname ambari3 yaosong5/ambari-server:1.0 &amp;&gt; /dev/null åœæ­¢andåˆ é™¤å®¹å™¨1234567docker stop ambari1docker stop ambari2docker stop ambari3docker rm ambari1docker rm ambari2docker rm ambari3","categories":[{"name":"å®‰è£…éƒ¨ç½²","slug":"å®‰è£…éƒ¨ç½²","permalink":"http://gangtieguo.cn/categories/å®‰è£…éƒ¨ç½²/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"},{"name":"Ambari","slug":"Ambari","permalink":"http://gangtieguo.cn/tags/Ambari/"}]},{"title":"Docker-yaosong5/Hueæ­å»ºåŠè¿è¡Œå‘½ä»¤","slug":"Docker-yaosongt:hueæ­å»ºåŠå…¶è¿è¡Œå‘½ä»¤","date":"2018-07-09T16:37:50.903Z","updated":"2019-06-17T04:40:09.370Z","comments":true,"path":"2018/07/10/Docker-yaosongt:hueæ­å»ºåŠå…¶è¿è¡Œå‘½ä»¤/","link":"","permalink":"http://gangtieguo.cn/2018/07/10/Docker-yaosongt:hueæ­å»ºåŠå…¶è¿è¡Œå‘½ä»¤/","excerpt":"[TOC] æœ¬æ¬¡é‡‡ç”¨çš„ant mavenæ¥ç¼–è¯‘hueå¯åŠ¨ä¸€ä¸ªåŸºç¡€å®¹å™¨docker run -itd --net=br --name hue --hostname hue yaosong5/centosbase:1.0 &amp;&gt; /dev/null","text":"[TOC] æœ¬æ¬¡é‡‡ç”¨çš„ant mavenæ¥ç¼–è¯‘hueå¯åŠ¨ä¸€ä¸ªåŸºç¡€å®¹å™¨docker run -itd --net=br --name hue --hostname hue yaosong5/centosbase:1.0 &amp;&gt; /dev/null æ‹·è´æºåŒ…å°†antã€hue4.0.0ã€antã€mavenç­‰ä¸‹è½½åˆ°æœ¬åœ°ç»“ä¸šåï¼Œå†æ‹·è´åˆ°å®¹å™¨ï¼ˆè¿™æ ·æ›´å¿«é€Ÿï¼‰ docker cp /Users/yaosong/Yao/ant 4115ea59088e:/ docker cp /Users/yaosong/Yao/maven 4115ea59088e:/ docker cp /Users/yaosong/Yao/hue4 4115ea59088e:/usr/ é…ç½®HOME12345678910vim ~/.bashrcåŠ å…¥MAVEN_HOME=/mavenexport MAVEN_HOMEexport PATH=$&#123;PATH&#125;:$&#123;MAVEN_HOME&#125;/binANT_HOME=/antPATH=$JAVA_HOME/bin:$ANT_HOME/bin:$PATHHUE_HOME=/hue4ä½¿å…¶ç”Ÿæ•ˆsource ~/.bashrc å®‰è£…ä¾èµ–ï¼Œç¼–è¯‘hueéœ€è¦å®‰è£…ä¸€äº›ä¾èµ–yum install gmp-devel -y å‚è€ƒ http://www.aizhuanji.com/a/0Vo0qEMW.html è‹¥è§£å†³ä¸äº†1yum install asciidoc cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-plain gcc gcc-c++ krb5-devel libffi-devel libtidy libxml2-devel libxslt-devel make mysql-devel openldap-devel sqlite-devel openssl-devel gmp-devel -y å‚è€ƒé“¾æ¥ï¼šhttps://www.jianshu.com/p/417788238e3d \bç¼–è¯‘å®‰è£…hueé¦–å…ˆç¼–è¯‘ Hueï¼Œå¹¶åœ¨è¦å®‰è£… Hue çš„èŠ‚ç‚¹ä¸Šåˆ›å»º Hue ç”¨æˆ·å’Œ hue ç»„ åˆ›å»º Hue ç”¨æˆ·1234groupadd hueuseradd hue -g huecd $HUE_HOMEchown -R hue:hue * æ³¨ï¼šéœ€è¦æ³¨æ„çš„æ˜¯ hue åœ¨ç¼–è¯‘æ—¶æœ‰ä¸¤ç§æ–¹å¼:1.é€šè¿‡mavenã€antç¼–è¯‘ 2.é€šè¿‡pythonç¼–è¯‘ï¼ˆåœ¨centos6.5å› ä¸ºè‡ªèº«pythonä¸º2.6.6ç‰ˆæœ¬å’Œhueç¼–è¯‘éœ€è¦2.7ç‰ˆæœ¬ä¼šæœ‰ä¸€ç‚¹å°å†²çªï¼Œæ•…é‡‡ç”¨1ï¼‰ä¸¤ç§æ–¹å¼éƒ½æ˜¯åœ¨hueç›®å½•ä¸‹ make appsï¼Œåªæ˜¯ç¬¬ä¸€ç§æ–¹å¼è¦å…ˆé…ç½®mavenã€antçš„ç¯å¢ƒè€Œå·² 12cd $HUE_HOMEmake apps å‚è€ƒ ï¼šhttps://blog.csdn.net/u012802702/article/details/68071244 å¦‚æœæŠ¥é”™ 1/usr/hue4/Makefile.vars:42: *** &quot;Error: must have python development packages for 2.6 or 2.7. Could not find Python.h. Please install python2.6-devel or python2.7-devel&quot;. Stop. éœ€æ‰§è¡Œ 1234å¯ä»¥å…ˆæŸ¥çœ‹ä¸€ä¸‹å« python-devel çš„åŒ…yum search python | grep python-devel64 ä½å®‰è£… python-devel.x86_64ï¼Œ32 ä½å®‰è£… python-devel.i686ï¼Œæˆ‘è¿™é‡Œå®‰è£…:sudo yum install python-devel.x86_64 -y æ›´æ”¹hueçš„é…ç½®æ–‡ä»¶vim $HUE_HOME/desktop/conf/hue.ini mysql1234567[[database]] host=master port=3306 engine=mysql user=root password=root name=hue hive123hive_server_host=masterhive_server_port=10000hive_conf_dir=$HIVE_HOME/conf hadoop-hdfs12345fs_defaultfs=hdfs://master:9000logical_name=masterwebhdfs_url=http://master:50070/webhdfs/v1hadoop_hdfs_home=$HADOOP_HOMEhadoop_conf_dir=$HADOOP_HOME/etc/hadoop hadoop-yarnåœ¨ [hadoop].[[yarn_clusters]].[[[default]]] ä¸‹ 1234resourcemanager_host=masterresourcemanager_port=8032resourcemanager_api_url=http://master:8088proxy_api_url=http://master:8088 hbaseåœ¨ [hbase] èŠ‚ç‚¹ä¸‹ 123hbase_clusters=(HBASE|master:9090)hbase_conf_dir=$HBASE_HOME/confuse_doas=true å¤§æ•°æ®å„ç»„ä»¶æ»¡è¶³hueè¿›è¡Œç›¸åº”é…ç½®å®‰è£…mysqlç”±äºéœ€è¦hueéœ€è¦å­˜æ”¾ä¸€äº›å…ƒæ•°æ®æ•…å®‰è£…mysql 12345678910111213yum install -y mysql-serverservice mysqld startmysql -u root -pEnter password: //é»˜è®¤å¯†ç ä¸ºç©ºï¼Œè¾“å…¥åå›è½¦å³å¯set password for root@localhost=password('root'); å¯†ç è®¾ç½®ä¸ºrooté»˜è®¤æƒ…å†µä¸‹Mysqlåªå…è®¸æœ¬åœ°ç™»å½•ï¼Œæ‰€ä»¥åªéœ€é…ç½®root@localhostå°±å¥½set password for root@%=password('root'); å¯†ç è®¾ç½®ä¸ºroot ï¼ˆå…¶å®è¿™ä¸€æ­¥å¯ä»¥ä¸é…ï¼‰set password for root@master=password('root'); å¯†ç è®¾ç½®ä¸ºroot ï¼ˆå…¶å®è¿™ä¸€æ­¥å¯ä»¥ä¸é…ï¼‰select user,host,password from mysql.user; æŸ¥çœ‹å¯†ç æ˜¯å¦è®¾ç½®æˆåŠŸGRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;create database hue; æŠ¥é”™DatabaseError DatabaseError:(1146,â€Table â€˜hue.desktop_settingsâ€™ doesnâ€™t existâ€)-åˆå§‹åŒ–mysql å®Œæˆä»¥ä¸Šçš„è¿™ä¸ªé…ç½®ï¼Œå¯åŠ¨ Hue, é€šè¿‡æµè§ˆå™¨è®¿é—®ï¼Œä¼šå‘ç”Ÿé”™è¯¯ï¼ŒåŸå› æ˜¯ mysql æ•°æ®åº“æ²¡æœ‰è¢«åˆå§‹åŒ–DatabaseError: (1146,â€Table â€˜hue.desktop_settingsâ€™ doesnâ€™t existâ€)æ‰§è¡Œä»¥ä¸‹æŒ‡ä»¤å¯¹ hue æ•°æ®åº“è¿›è¡Œåˆå§‹åŒ– 123cd $HUE_HOME/build/env/bin/hue syncdbbin/hue migrate æ­¤å¤–éœ€è¦æ³¨æ„çš„æ˜¯å¦‚æœä½¿ç”¨çš„æ˜¯ï¼š$HUE_HOME/build/env/bin/hue syncdb --noinput åˆ™ä¸ä¼šè®©è¾“å…¥åˆå§‹çš„ç”¨æˆ·åå’Œå¯†ç ï¼Œåªæœ‰åœ¨é¦–æ¬¡ç™»å½•æ—¶æ‰ä¼šè®©è¾“å…¥ï¼Œä½œä¸ºè¶…çº§ç®¡ç†å‘˜è´¦æˆ·ã€‚\b hdfshdfs-site.xmlå¢åŠ ä¸€ä¸ªå€¼å¼€å¯ hdfs çš„ web äº¤äº’123456 &lt;!--HUE å¢åŠ ä¸€ä¸ªå€¼å¼€å¯ hdfs çš„ web äº¤äº’--&gt;&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; &lt;!--HUE å¢åŠ ä¸€ä¸ªå€¼å¼€å¯ hdfs çš„ web äº¤äº’--&gt; core-site.xml12345678910&lt;!--ã€Šä¸ºäº†è®© hue èƒ½å¤Ÿè®¿é—® hdfsï¼Œéœ€è¦åœ¨ hdfs-site.xml é‡Œé¢é…ç½®ä¸€äº›å†…å®¹--&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;!--ã€Šä¸ºäº†è®© hue èƒ½å¤Ÿè®¿é—® hdfsï¼Œéœ€è¦åœ¨ hdfs-site.xml é‡Œé¢é…ç½®ä¸€äº›å†…å®¹--&gt; hbasehbase-site.xml 12345678910&lt;!-- hueæ”¯æŒ --&gt;&lt;property&gt; &lt;name&gt;hbase.thrift.support.proxyuser&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.regionserver.thrift.http&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- hueæ”¯æŒ --&gt; hue è®¿é—® hbase æ˜¯ç”¨çš„ thriftserverï¼Œå¹¶ä¸”æ˜¯ thrift1ï¼Œä¸æ˜¯ thrift2ï¼Œæ‰€ä»¥è¦åœ¨ master ä¸Šé¢å¯åŠ¨ thrif1 1$HBASE_HOME/bin/hbase-daemon.sh start thrift å‚è€ƒ https://blog.csdn.net/Dante_003/article/details/78889084 è¯»å–hbaseé—®é¢˜ä¸ºè§£å†³è®¿é—®Failed to authenticate to HBase Thrift Server, check authentication configurations.éœ€è¦åœ¨hueçš„é…ç½®æ–‡ä»¶ä¸­é…ç½® 1use_doas=true å‚è€ƒhttp://gethue.com/hbase-browsing-with-doas-impersonation-and-kerberos/ è‹¥ä»¥ä¸Šé…ç½®æœªèƒ½è§£å†³é—®é¢˜ï¼Œè¿˜éœ€è¦å°†core-site.xmlæ‹·è´åˆ°hbase/confï¼Œå¹¶æ·»åŠ ä»¥ä¸‹å†…å®¹ 12345678&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hbase.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hbase.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; [å‚è€ƒ]https://blog.csdn.net/u012802702/article/details/68071244 hiveHue ä¸æ¡†æ¶ Hive çš„é›†æˆå¼€å¯ Hive Remote MetaStorenohup $HIVE_HOME/bin/hive --service metastore &amp; hive åªéœ€å¯åŠ¨ hiveserver2ï¼Œthriftserver çš„ 10000 ç«¯å£å¯åŠ¨å³å¯123nohup $HIVE_HOME/bin/hiveserver2 &amp;æˆ–è€…nohup HIVE_HOME/bin/hive --service hiveserver2 &amp; è§£å†³ hue ui ç•Œé¢æŸ¥è¯¢ä¸­æ–‡ä¹±ç é—®é¢˜åœ¨ [[[mysql]]]èŠ‚ç‚¹ä¸‹ 1options=&#123;\"init_command\":\"SET NAMES'utf8'\"&#125; [å‚è€ƒ]https://blog.csdn.net/u012802702/article/details/68071244 ä¾èµ–çš„ç»„ä»¶å¯åŠ¨Mysqlservice mysqld start hadoopstart-all.sh hiveç„¶åéœ€è¦åŒæ—¶å¯åŠ¨ hive çš„ metastore å’Œ hiveserve2 12nohup hive --service metastore &amp;nohup hive --service hiveserver2 &amp; hbaseHue éœ€è¦è¯»å– HBase çš„æ•°æ®æ˜¯ä½¿ç”¨ thrift çš„æ–¹å¼ï¼Œé»˜è®¤ HBase çš„ thrift æœåŠ¡æ²¡æœ‰å¼€å¯ï¼Œæ‰€æœ‰éœ€è¦æ‰‹åŠ¨é¢å¤–å¼€å¯ thrift æœåŠ¡ã€‚ å¯åŠ¨ thrift service$HBASE_HOME/bin/hbase-daemon.sh start thrift thrift service é»˜è®¤ä½¿ç”¨çš„æ˜¯ 9090 ç«¯å£ï¼Œä½¿ç”¨å¦‚ä¸‹å‘½ä»¤æŸ¥çœ‹ç«¯å£æ˜¯å¦è¢«å ç”¨ netstat -nl|grep 9090 ä¾èµ–å¯åŠ¨çš„è„šæœ¬1234567891011#!/bin/bash#å¯åŠ¨mysqlservice mysqld start#å¯åŠ¨hadoopsh /hadoop-start.sh#å¯åŠ¨hivesh /hive-start-servers2.sh#å¯åŠ¨ thrift service$HBASE_HOME/bin/hbase-daemon.sh start thrift#å¯åŠ¨huenohup $HUE_HOME/build/env/bin/supervisor &amp; hueå¯åŠ¨å‘½ä»¤1$HUE_HOME/build/env/bin/supervisor &amp; (æ³¨ï¼šæƒ³è¦åå°æ‰§è¡Œå°±æ˜¯ $HUE_HOME/build/env/bin/supervisor &amp; ) æˆ–è€… 1$HUE_HOME/build/env/bin/hue runserver_plus 0.0.0.0:8888 ã€å‚è€ƒã€‘https://blog.csdn.net/hexinghua0126/article/details/80338779 hueçš„\bwebæœåŠ¡ç«¯å£ï¼š8888 hueåœæ­¢å‘½ä»¤pkill -U hue æŠ¥é”™1ã€å¦‚æœä¿®æ”¹é…ç½®æ–‡ä»¶åï¼Œå¯åŠ¨åæ— æ³•è¿›äºº hue ç•Œé¢ 12345å¯èƒ½æ˜¯é…ç½®æ–‡ä»¶è¢«é”ä½äº†cd $HUE_HOME/desktop/confls â€“arm â€“rf hue.ini.swpæˆ–è€… hadoopã€hive ç­‰æœåŠ¡æ²¡æœ‰å¯åŠ¨èµ·æ¥ 2ã€åœ¨ hue\bç•Œé¢å¼‚å¸¸ï¼Œå¯¼è‡´ hive æ— æ³•ä½¿ç”¨å®‰è£…æ’ä»¶ï¼šyum install cyrus-sasl-plain cyrus-sasl-devel cyrus-sasl-gssapi æ“ä½œé•œåƒä¿å­˜ä¸ºé•œåƒdocker commit -m &quot;hue&quot; hue yaosong5/hue4:1.0 åˆ›å»ºå®¹å™¨docker run -itd --net=br --name gethue --hostname gethue gethue/hue:latest &amp;&gt; /dev/null\bæ˜ å°„å®¿ä¸»æœºçš„\b\bhostsæ–‡ä»¶åŠå…¶hueçš„é…ç½®æ–‡ä»¶æ–¹å¼å¯åŠ¨å®¹å™¨12docker run --name=hue -d --net=br -v /etc/hosts/:/etc/hosts -v $PWD/pseudo-distributed.ini:/hue/desktop/conf/pseudo-distributed.ini yaosong5/hue4:1.0 â€“net=brä¸º\bäº†å®¿ä¸»æœºå’Œå®¹å™¨ä¹‹å‰ipè‡ªç”±è®¿é—®æ‰€æ­å»ºçš„ç½‘ç»œæ¨¡å¼ï¼Œå¦‚æœ‰éœ€æ±‚\bè¯·å‚è€ƒ å…¶ä»–å‚è€ƒ 1docker run --name=hue -d --net=br -v /etc/hosts/:/etc/hosts -v $PWD/pseudo-distributed.ini:/hue/desktop/conf/pseudo-distributed.ini gethue/hue:latest å‚è€ƒï¼šhttps://blog.csdn.net/Dante_003/article/details/78889084","categories":[{"name":"å®‰è£…éƒ¨ç½²","slug":"å®‰è£…éƒ¨ç½²","permalink":"http://gangtieguo.cn/categories/å®‰è£…éƒ¨ç½²/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"},{"name":"Hue","slug":"Hue","permalink":"http://gangtieguo.cn/tags/Hue/"}]},{"title":"ESæµ‹è¯•å‘½ä»¤","slug":"ESæµ‹è¯•å‘½ä»¤","date":"2018-07-09T16:11:03.158Z","updated":"2019-06-17T04:40:09.375Z","comments":true,"path":"2018/07/10/ESæµ‹è¯•å‘½ä»¤/","link":"","permalink":"http://gangtieguo.cn/2018/07/10/ESæµ‹è¯•å‘½ä»¤/","excerpt":"[TOC] ç®€å•å‘½ä»¤æµ‹è¯•å’Œå±•ç¤ºesçš„åŠŸèƒ½","text":"[TOC] ç®€å•å‘½ä»¤æµ‹è¯•å’Œå±•ç¤ºesçš„åŠŸèƒ½ æ’å…¥è®°å½• 1234567891011curl -H &quot;Content-Type: application/json&quot; -XPUT &apos;http://localhost:9200/store/books/1&apos; -d &apos;&#123;&quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;,&quot;name&quot; : &#123; &quot;first&quot; : &quot;Zachary&quot;, &quot;last&quot; : &quot;Tong&quot;&#125;,&quot;publish_date&quot;:&quot;2015-02-06&quot;,&quot;price&quot;:&quot;49.99&quot;&#125;&apos; åœ¨æ·»åŠ ä¸€ä¸ªä¹¦çš„ä¿¡æ¯123456789curl -H &quot;Content-Type: application/json&quot; -XPUT &apos;http://elk1:9200/store/books/2&apos; -d &apos;&#123;&quot;title&quot;: &quot;Elasticsearch Blueprints&quot;,&quot;name&quot; : &#123; &quot;first&quot; : &quot;Vineeth&quot;, &quot;last&quot; : &quot;Mohan&quot;&#125;,&quot;publish_date&quot;:&quot;2015-06-06&quot;,&quot;price&quot;:&quot;35.99&quot;&#125;&apos; é€šè¿‡IDè·å¾—æ–‡æ¡£ä¿¡æ¯ 1curl -H &quot;Content-Type: application/json&quot; -XGET &apos;http://elk1:9200/store/books/1&apos; 12345678910111213141516curl -H &quot;Content-Type: application/json&quot; -XGET &apos;http://elk1:9200/store/books/_search&apos; -d &apos;&#123;&quot;query&quot; : &#123;&quot;filtered&quot; : &#123; &quot;query&quot; : &#123; &quot;match_all&quot; : &#123;&#125; &#125;, &quot;filter&quot; : &#123; &quot;term&quot; : &#123; &quot;price&quot; : 35.99 &#125; &#125; &#125;&#125;&#125;&apos; åœ¨æµè§ˆ 1234567891011curl -H &quot;Content-Type: application/json&quot; -XPUT &apos;http://elk1:9200/store/books/1&apos; -d &apos;&#123;&quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;,&quot;name&quot; : &#123; &quot;first&quot; : &quot;Zachary&quot;, &quot;last&quot; : &quot;Tong&quot;&#125;,&quot;publish_date&quot;:&quot;2015-02-06&quot;,&quot;price&quot;:&quot;49.99&quot;&#125;&apos; 1curl -H &quot;Content-Type: application/json&quot; -XPUT &apos;http://127.0.0.1:9200/kc22k2_test&apos; -d 1curl -XPUT elk1:9200/test 12345678910111213141516curl -XGET &apos;http://elk1:9200/_cluster/state?pretty&apos;&#123; &quot;error&quot; : &#123;&quot;root_cause&quot; : [ &#123; &quot;type&quot; : &quot;master_not_discovered_exception&quot;, &quot;reason&quot; : null &#125;],&quot;type&quot; : &quot;master_not_discovered_exception&quot;,&quot;reason&quot; : null &#125;, &quot;status&quot; : 503&#125;","categories":[{"name":"å®‰è£…éƒ¨ç½²","slug":"å®‰è£…éƒ¨ç½²","permalink":"http://gangtieguo.cn/categories/å®‰è£…éƒ¨ç½²/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"},{"name":"ELK","slug":"ELK","permalink":"http://gangtieguo.cn/tags/ELK/"},{"name":"es","slug":"es","permalink":"http://gangtieguo.cn/tags/es/"}]},{"title":"yaosong5/bigdata:2.0-Hadoop&Spark&hive&zk&hueç»„åˆå®¹å™¨çš„æ­å»º","slug":"Docker-yaosong5:bigdataé•œåƒåˆ¶ä½œ","date":"2018-07-08T17:06:20.114Z","updated":"2019-03-12T02:48:25.729Z","comments":true,"path":"2018/07/09/Docker-yaosong5:bigdataé•œåƒåˆ¶ä½œ/","link":"","permalink":"http://gangtieguo.cn/2018/07/09/Docker-yaosong5:bigdataé•œåƒåˆ¶ä½œ/","excerpt":"[TOC] é…ç½®centosé›†ç¾¤ hadoop sparkç»„ä»¶å¯åŠ¨å®¹å™¨å„ç»„ä»¶ç‰ˆæœ¬å¯¹åº”hbase1.2 hive ç‰ˆæœ¬ 2.0.0 hbase1.x ZooKeeper 3.4.x is required as of HBase 1.0.0 æ–°å»ºå®¹å™¨ï¼Œä¸ºå‡å°‘å·¥ä½œé‡ï¼Œå¼•ç”¨çš„æ˜¯æœ‰sshæœåŠ¡çš„Dockeré•œåƒkinogmt/centos-ssh:6.7ï¼Œç”Ÿæˆå®¹å™¨osä¸ºåŸºå‡†ã€‚ 1docker run -itd --name bigdata --hostname bigdata kinogmt/centos-ssh:6.7 &amp;&gt; /dev/null æ³¨æ„å¿…é¡»è¦ä»¥-dæ–¹å¼å¯åŠ¨ï¼Œä¸ç„¶sshdæœåŠ¡ä¸ä¼šå¯åŠ¨ï¼Œè¿™ç®—æ˜¯ä¸€ä¸ªå°bug","text":"[TOC] é…ç½®centosé›†ç¾¤ hadoop sparkç»„ä»¶å¯åŠ¨å®¹å™¨å„ç»„ä»¶ç‰ˆæœ¬å¯¹åº”hbase1.2 hive ç‰ˆæœ¬ 2.0.0 hbase1.x ZooKeeper 3.4.x is required as of HBase 1.0.0 æ–°å»ºå®¹å™¨ï¼Œä¸ºå‡å°‘å·¥ä½œé‡ï¼Œå¼•ç”¨çš„æ˜¯æœ‰sshæœåŠ¡çš„Dockeré•œåƒkinogmt/centos-ssh:6.7ï¼Œç”Ÿæˆå®¹å™¨osä¸ºåŸºå‡†ã€‚ 1docker run -itd --name bigdata --hostname bigdata kinogmt/centos-ssh:6.7 &amp;&gt; /dev/null æ³¨æ„å¿…é¡»è¦ä»¥-dæ–¹å¼å¯åŠ¨ï¼Œä¸ç„¶sshdæœåŠ¡ä¸ä¼šå¯åŠ¨ï¼Œè¿™ç®—æ˜¯ä¸€ä¸ªå°bug åœ¨å®¹å™¨ä¸­ä¸‹è½½éœ€è¦çš„elkçš„æºåŒ…ï¼Œåšè§£å‹å°±ä¸èµ˜è¿°ï¼Œå¾ˆå¤šæ¡ˆä¾‹æ•™ç¨‹ã€‚ æˆ‘æ˜¯é‡‡ç”¨çš„ä¸‹è½½åˆ°å®¿ä¸»æœºï¼Œè§£å‹åï¼Œç”¨ â€œdocker cp è§£å‹åŒ…ç›®å½• os:/usr/loca/â€œæ¥ä¼ åˆ°å®¹å™¨å†…ï¼Œæ¯”åœ¨å®¹å™¨å†…ä¸‹è½½é€Ÿåº¦æ›´å¿« æ‹·è´æ–‡ä»¶åˆ°å®¹å™¨ å‘½ä»¤æ ¼å¼docker cp æœ¬åœ°æ–‡ä»¶è·¯å¾„ å®¹å™¨idæˆ–è€…å®¹å™¨åç§°: å°†æ‰€æœ‰ç»„ä»¶ä¸‹è½½è§£å‹å¹¶æ‹·è´åˆ°å®¹å™¨ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849docker cp /Users/yaosong/Downloads/hadoop-2.8.0.tar.gz bigdata:/docker cp /Users/yaosong/Downloads/spark-2.2.0-bin-without-hadoop.tgz bigdata:/docker cp /Users/yaosong/Downloads/jdk-8u144-linux-x64.rpm bigdata:/docker cp /Users/yaosong/Downloads/spark-2.1.0-bin-hadoop2.6.tgz bigdata:/docker cp /Users/yaosong/Yao/sparkæºåŒ…/hive bigdata:/usrdocker cp /Users/yaosong/Downloads/jdk-8u144-linux-x64.rpm bigdata:/docker cp /Users/yaosong/Downloads/hadoop-2.8.0.tar.gz bigdata:/docker cp /Users/yaosong/Downloads/spark-2.2.0-bin-without-hadoop.tgz bigdata:/docker cp /Users/yaosong/Downloads/spark-2.1.0-bin-hadoop2.6.tgz bigdata:/docker cp /Users/yaosong/Yao/sparkæºåŒ…/hbase bigdata:/usrdocker cp /Users/yaosong/Yao/sparkæºåŒ…/zk bigdata:/usrdocker cp /Users/yaosong/Yao/ant bigdata:/usrdocker cp /Users/yaosong/Yao/maven bigdata:/usrdocker cp /Users/yaosong/Yao/hue4 bigdata:/usråˆ›å»ºhomevim /etc/profilemac: vim ~/.bashrcæ·»åŠ ä»¥ä¸‹å†…å®¹ export JAVA_HOME=/usr/java/jdk export PATH=$JAVA_HOME:$PATH export SCALA_HOME=/usr/scala-2.12.3/ export HADOOP_HOME=/usr/hadoop export HADOOP_CONFIG_HOME=$HADOOP_HOME/etc/hadoop export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin export SPARK_DIST_CLASSPATH=$(hadoop classpath) SPARK_MASTER_IP=master SPARK_LOCAL_DIRS=/usr/spark SPARK_DRIVER_MEMORY=1G export SPARK_HOME=/usr/spark export PATH=$SPARK_HOME/bin:$PATH export PATH=$SPARK_HOME/sbin:$PATH MAVEN_HOME=/usr/maven export MAVEN_HOME export PATH=$&#123;PATH&#125;:$&#123;MAVEN_HOME&#125;/bin ANT_HOME=/usr/ant PATH=$JAVA_HOME/bin:$ANT_HOME/bin:$PATH export ANT_HOME PATH HUE_HOME=/usr/hue4 export ZK_HOME=/usr/zk export HBASE_HOME=/usr/hbase export PATH=$HBASE_HOME/bin:$PATH export PATH=$ZK_HOME/bin:$PATH å®‰è£…åˆ›å»º hadoop é›†ç¾¤æ‰€éœ€ç›®å½•ï¼šåœ¨ä»¥ä¸‹é…ç½®æ–‡ä»¶ä¸­ä¼šæœ‰ä»¥ä¸‹ç›®å½• 12345cd $HADOOP_HOME;mkdir tmpmkdir namenodemkdir datanodecd $HADOOP_CONFIG_HOME/ æ›´æ”¹é…ç½®æ–‡ä»¶cd $HADOOP_CONFIG_HOME/ or cd $HADOOP_HOME/etc/hadoop hdfs slaves12slave01slave02 core-site.xmlï¼š12345678910111213141516171819202122232425262728293031323334&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/hadoop/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;description&gt;The name of the default file system. A URI whose scheme and authority determine the FileSystem implementation. &lt;/description&gt; &lt;/property&gt; &lt;!--hiveçš„é…ç½®ï¼Œå‚è€ƒhttps://blog.csdn.net/lblblblblzdx/article/details/79760959--&gt; &lt;property&gt; &lt;name&gt;hive.server2.authentication&lt;/name&gt; &lt;value&gt;NONE&lt;/value&gt; &lt;/property&gt; &lt;!--hiveçš„é…ç½®hadoopä»£ç†ç”¨æˆ· rootç”¨æˆ·æäº¤çš„ä»»åŠ¡å¯ä»¥åœ¨ä»»æ„æœºå™¨ä¸Šä»¥ä»»æ„ç»„çš„æ‰€æœ‰ç”¨æˆ·çš„èº«ä»½æ‰§è¡Œã€‚--&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;!--HUE å¢åŠ ä¸€ä¸ªå€¼å¼€å¯ hdfs çš„ web äº¤äº’--&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--HUE å¢åŠ ä¸€ä¸ªå€¼å¼€å¯ hdfs çš„ web äº¤äº’--&gt; hdfs-site.xmlï¼š12345678910111213141516171819202122232425262728293031&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;description&gt;Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/usr/hadoop/namenode&lt;/value&gt; &lt;final&gt;true&lt;/final&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/usr/hadoop/datanode&lt;/value&gt; &lt;final&gt;true&lt;/final&gt;&lt;/property&gt;&lt;!--ã€Šä¸ºäº†è®© hue èƒ½å¤Ÿè®¿é—® hdfsï¼Œéœ€è¦åœ¨ hdfs-site.xml é‡Œé¢é…ç½®ä¸€äº›å†…å®¹--&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;!--ã€Šä¸ºäº†è®© hue èƒ½å¤Ÿè®¿é—® hdfsï¼Œéœ€è¦åœ¨ hdfs-site.xml é‡Œé¢é…ç½®ä¸€äº›å†…å®¹--&gt; mapred-site.xmlï¼š12345678&lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;master:9001&lt;/value&gt; &lt;description&gt;The host and port that the MapReduce job tracker runs at. If \"local\", then jobs are run in-process as a single map and reduce task. &lt;/description&gt;&lt;/property&gt; yarn-site.xmlï¼š12345678910111213141516171819202122232425&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;Whether virtual memory limits will be enforced for containers&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;256&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt;&lt;/property&gt; å¦‚æœæ˜¯hadoop3ä»¥ä¸Šç‰ˆæœ¬ï¼Œéœ€è¦åœ¨start-dfs.sh start-yarn.shä¸­å¼€å¤´ç©ºç™½å¤„åˆ†åˆ«é…ç½®ä¸€ä¸‹å†…å®¹ 12345678910111213vim $HADOOP_HOME/sbin/start-dfs.shHDFS_DATANODE_USER=rootHADOOP_SECURE_DN_USER=hdfsHDFS_NAMENODE_USER=rootHDFS_SECONDARYNAMENODE_USER=rootvim $HADOOP_HOME/sbin/start-yarn.shYARN_RESOURCEMANAGER_USER=rootHADOOP_SECURE_DN_USER=rootYARN_NODEMANAGER_USER=yarnYARN_PROXYSERVER_USER=root æ ¼å¼åŒ–namenode1$HADOOP_HOME/bin/hadoop namenode -format å¯åŠ¨é›†ç¾¤$HADOOP_HOME/sbin/start-all.sh æµ‹è¯• 1yarn 8088ç«¯å£ http://yourip:8088 1hdfs 50070ç«¯å£ hdfs3.0ä¸º9870 http://yourip:50070 sparkåªéœ€è¦åœ¨slavesä¸­æ·»åŠ 12slave01slave02 sparkUIç«¯å£8080 æµ‹è¯•sparké›†ç¾¤å¯åŠ¨spark 1$HADOOP_HOME/bin/start-all.sh å®˜ç½‘å‘½ä»¤12345678$SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi \\--master yarn \\--deploy-mode cluster \\--driver-memory 512m \\--executor-memory 512m \\--executor-cores 1 \\$SPARK_HOME/examples/jars/spark-examples*.jar \\10 æ‰§è¡Œspark on yarnå‘½ä»¤è¡Œæ¨¡å¼123456789spark-shell --master yarn --deploy-mode client --driver-memory 1g --executor-memory 1g --executor-cores 1spark-shell --master yarn --deploy-mode client --driver-memory 512m --executor-memory 512m --executor-cores 1spark-shell --master yarn --deploy-mode client --driver-memory 475m --executor-memory 475m --executor-cores 1spark-shell --master yarn --deploy-mode client --driver-memory 350m --executor-memory 350m --executor-cores 1spark-shell --master yarn --deploy-mode client --driver-memory 650m --executor-memory 650m --executor-cores 1 åˆ›å»ºé•œåƒ1docker commit -m \"bigdataåŸºç¡€ç»„ä»¶é•œåƒ\" bigdata yaosong5/bigdata:2.0 2.0é•œåƒå¯¹æ¯”1.0ï¼š1.0åªæœ‰hadoop-å¾…ç¡®å®š åˆ›å»ºå®¹å™¨123docker run -itd --net=br --name master --hostname master yaosong5/bigdata:2.0 &amp;&gt; /dev/nulldocker run -itd --net=br --name slave01 --hostname slave01 yaosong5/bigdata:2.0 &amp;&gt; /dev/nulldocker run -itd --net=br --name slave02 --hostname slave02 yaosong5/bigdata:2.0 &amp;&gt; /dev/null åœæ­¢and åˆ é™¤å®¹å™¨123456docker stop masterdocker stop slave01docker stop slave02docker rm masterdocker rm slave01docker rm slave02 w","categories":[{"name":"å®‰è£…éƒ¨ç½²","slug":"å®‰è£…éƒ¨ç½²","permalink":"http://gangtieguo.cn/categories/å®‰è£…éƒ¨ç½²/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://gangtieguo.cn/tags/Hadoop/"},{"name":"Spark","slug":"Spark","permalink":"http://gangtieguo.cn/tags/Spark/"}]},{"title":"HBaseå®¹å™¨çš„æ­å»º","slug":"hbasezkå®¹å™¨çš„æ­å»º","date":"2018-07-08T16:01:32.819Z","updated":"2019-06-17T04:40:09.380Z","comments":true,"path":"2018/07/09/hbasezkå®¹å™¨çš„æ­å»º/","link":"","permalink":"http://gangtieguo.cn/2018/07/09/hbasezkå®¹å™¨çš„æ­å»º/","excerpt":"[TOC] åˆ›å»ºhbaseé•œåƒæ‹·è´æºç 12345docker cp /Users/yaosong/Yao/hbase 8019587d559b:/usr/docker cp /Users/yaosong/Yao/zk 8019587d559b:/usr/docker cp /Users/yaosong/Yao/hbasezkStart.sh 8019587d559b:/usr/docker cp /Users/yaosong/Yao/hbase-start.sh 8019587d559b:/usr/docker cp /Users/yaosong/Yao/zk-start.sh 8019587d559b:/usr/ å‚è€ƒhttps://www.cnblogs.com/netbloomy/p/6677883.html","text":"[TOC] åˆ›å»ºhbaseé•œåƒæ‹·è´æºç 12345docker cp /Users/yaosong/Yao/hbase 8019587d559b:/usr/docker cp /Users/yaosong/Yao/zk 8019587d559b:/usr/docker cp /Users/yaosong/Yao/hbasezkStart.sh 8019587d559b:/usr/docker cp /Users/yaosong/Yao/hbase-start.sh 8019587d559b:/usr/docker cp /Users/yaosong/Yao/zk-start.sh 8019587d559b:/usr/ å‚è€ƒhttps://www.cnblogs.com/netbloomy/p/6677883.html è§£å‹tar -zxvf hbase-1.3.0-bin.tar.gzè¿›å…¥ hbase çš„é…ç½®ç›®å½•ï¼Œåœ¨ hbase-env.sh æ–‡ä»¶é‡Œé¢åŠ å…¥ java ç¯å¢ƒå˜é‡. å³ï¼š 12vim hbase-env.shexport JAVA_HOME=JAVA_HOME=/usr/java/jdk å…³é—­ HBase è‡ªå¸¦çš„ Zookeeper, ä½¿ç”¨ Zookeeper é›†ç¾¤ï¼š 12vim hbase-env.shexport HBASE_MANAGES_ZK=false hbase-site.xml 123456789101112131415161718192021222324&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hbasezk1,hbasezk2,hbasezk3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/usr/hbase/tmp/zk/data&lt;/value&gt; &lt;/property&gt; &lt;!-- webuiçš„é…ç½® --&gt; &lt;property&gt; &lt;name&gt;hbase.master.info.port&lt;/name&gt; &lt;value&gt;60010&lt;/value&gt; &lt;/property&gt; &lt;!-- webuiæ–°å¢çš„é…ç½® --&gt;&lt;/configuration&gt; åˆ›å»ºzkçš„datadirç›®å½• mkdir -p /usr/hbase/tmp/zk/data ç¼–è¾‘é…ç½®ç›®å½•ä¸‹é¢çš„æ–‡ä»¶ regionservers. å‘½ä»¤ï¼š vim $HBASE_HOME/config/regionservers åŠ å…¥å¦‚ä¸‹å†…å®¹ï¼š hbasezk1 hbasezk2 hbasezk3 æŠŠ Hbase å¤åˆ¶åˆ°å…¶ä»–æœºå™¨scp å¼€å¯ hbase æœåŠ¡ã€‚å‘½ä»¤å¦‚ä¸‹ï¼š å“ªå°ä¸Šè¿è¡Œå“ªå°å°±ä¸ºhmaster $HBASE_HOME/bin/start-hbase.sh åœ¨ hbasezk1,2,3 ä¸­çš„ä»»æ„ä¸€å°æœºå™¨ä½¿ç”¨ $HBASE_HOME/bin/hbase shell è¿›å…¥ hbase è‡ªå¸¦çš„ shell ç¯å¢ƒï¼Œç„¶åä½¿ç”¨å‘½ä»¤ version ç­‰ï¼Œè¿›è¡ŒæŸ¥çœ‹ hbase ä¿¡æ¯åŠå»ºç«‹è¡¨ç­‰æ“ä½œã€‚ è¦é…ç½® HBase é«˜å¯ç”¨çš„è¯ï¼Œåªéœ€è¦å¯åŠ¨ä¸¤ä¸ª HMasterï¼Œè®© Zookeeper è‡ªå·±å»é€‰æ‹©ä¸€ä¸ª Master Acitveã€‚","categories":[{"name":"å®‰è£…éƒ¨ç½²","slug":"å®‰è£…éƒ¨ç½²","permalink":"http://gangtieguo.cn/categories/å®‰è£…éƒ¨ç½²/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"},{"name":"HBase","slug":"HBase","permalink":"http://gangtieguo.cn/tags/HBase/"}]},{"title":"FLINKå®¹å™¨çš„æ­å»º","slug":"Docker-yaosong5:flinkåˆ¶ä½œåŠå…¶è¿è¡Œå‘½ä»¤","date":"2018-07-08T16:00:22.635Z","updated":"2018-08-07T02:52:58.322Z","comments":true,"path":"2018/07/09/Docker-yaosong5:flinkåˆ¶ä½œåŠå…¶è¿è¡Œå‘½ä»¤/","link":"","permalink":"http://gangtieguo.cn/2018/07/09/Docker-yaosong5:flinkåˆ¶ä½œåŠå…¶è¿è¡Œå‘½ä»¤/","excerpt":"[TOC] æ¥æºå®¹å™¨ flkæ–°å»ºå®¹å™¨ï¼Œä¸ºå‡å°‘å·¥ä½œé‡ï¼Œå¼•ç”¨çš„æ˜¯æœ‰sshæœåŠ¡çš„Dockeré•œåƒkinogmt/centos-ssh:6.7ï¼Œç”Ÿæˆå®¹å™¨osä¸ºåŸºå‡†ã€‚ 1docker run -itd --name flk --hostname flk kinogmt/centos-ssh:6.7 &amp;&gt; /dev/null æ³¨æ„å¿…é¡»è¦ä»¥-dæ–¹å¼å¯åŠ¨ï¼Œä¸ç„¶sshdæœåŠ¡ä¸ä¼šå¯åŠ¨ï¼Œè¿™ç®—æ˜¯ä¸€ä¸ªå°bug","text":"[TOC] æ¥æºå®¹å™¨ flkæ–°å»ºå®¹å™¨ï¼Œä¸ºå‡å°‘å·¥ä½œé‡ï¼Œå¼•ç”¨çš„æ˜¯æœ‰sshæœåŠ¡çš„Dockeré•œåƒkinogmt/centos-ssh:6.7ï¼Œç”Ÿæˆå®¹å™¨osä¸ºåŸºå‡†ã€‚ 1docker run -itd --name flk --hostname flk kinogmt/centos-ssh:6.7 &amp;&gt; /dev/null æ³¨æ„å¿…é¡»è¦ä»¥-dæ–¹å¼å¯åŠ¨ï¼Œä¸ç„¶sshdæœåŠ¡ä¸ä¼šå¯åŠ¨ï¼Œè¿™ç®—æ˜¯ä¸€ä¸ªå°bug åœ¨å®¹å™¨ä¸­ä¸‹è½½éœ€è¦çš„elkçš„æºåŒ…ã€‚åšè§£å‹å°±ä¸èµ˜è¿°ï¼Œå¾ˆå¤šæ¡ˆä¾‹æ•™ç¨‹ã€‚ æˆ‘æ˜¯é‡‡ç”¨çš„ä¸‹è½½åˆ°å®¿ä¸»æœºï¼Œè§£å‹åï¼Œç”¨ â€œdocker cp è§£å‹åŒ…ç›®å½• os:/usr/loca/â€œæ¥ä¼ åˆ°å®¹å™¨å†…ï¼Œæ¯”åœ¨å®¹å™¨å†…ä¸‹è½½é€Ÿåº¦æ›´å¿« å¤åˆ¶æºåŒ…123&gt; docker cp /Users/yaosong/Yao/hadoop flk:/usr/&gt; docker cp /Users/yaosong/Yao/flink flk:/usr/&gt; é…ç½®homeexport JAVA_HOME=/usr/java/jdk1.8.0_144/ export PATH=$JAVA_HOME/bin:$PATH export FLINK_HOME=/usr/flink export HADOOP_HOME=/usr/hadoop export HADOOP_CONFIG_HOME=$HADOOP_HOME/etc/hadoop export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin export PATH=$PATH:$FLINK_HOME/bin é…ç½®hadoop å‚è€ƒï¼š []: http://gangtieguo.cn/2018/07/20/Docker%E4%B8%ADhadoop%20spark%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/ â€œDocker ä¸­ hadoopï¼Œspark é•œåƒæ­å»ºâ€ flink.yml 12345678910111213141516 high-availability: zookeeper# The path where metadata for master recovery is persisted. While ZooKeeper stores# the small ground truth for checkpoint and leader election, this location stores# the larger objects, like persisted dataflow graphs.# # Must be a durable file system that is accessible from all nodes# (like HDFS, S3, Ceph, nfs, ...) # high-availability.storageDir: hdfs:///flink/ha/# The list of ZooKeeper quorum peers that coordinate the high-availability# setup. This must be a list of the form:# &quot;host1:clientPort,host2:clientPort,...&quot; (default clientPort: 2181)#high-availability.zookeeper.quorum: zk1:2181,zk2:2181,zk3:2181 Master 1master:8081 slave 12slave01slave02 zoo.cfg 123server.1=zk1:2888:3888server.2=zk2:2888:3888server.3=zk3:2888:3888 éœ€è¦åœ¨yarn-site.xmlä¸­é…ç½® 12345678&lt;property&gt; &lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;/name&gt; &lt;value&gt;4&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;8&lt;/value&gt;&lt;/property&gt; ä¿å­˜é•œåƒ1docker commit -m \"bigdata:flink,hadoop\" --author=\"yaosong\" flk yao/flinkonyarn:1.0 è·å¾—flk å®¹å™¨123docker run -itd --net=br --name flk1 --hostname flk1 yao/flinkonyarn:1.0 &amp;&gt; /dev/nulldocker run -itd --net=br --name flk2 --hostname flk2 yao/flinkonyarn:1.0 &amp;&gt; /dev/nulldocker run -itd --net=br --name flk3 --hostname flk3 yao/flinkonyarn:1.0 &amp;&gt; /dev/null åœæ­¢/åˆ é™¤flk å®¹å™¨123456docker stop flk1docker stop flk2docker stop flk3docker rm flk1docker rm flk2docker rm flk3 å®˜æ–¹ï¼šwordcountflinkæµ‹è¯•å‘½ä»¤ç”±äºåœ¨æœ¬åœ°æ­å»ºï¼Œæœºå™¨é…ç½®æœ‰é™ï¼Œæ•…è®¾ç½®ä¸åŒå‚æ•°å‘½ä»¤æ¥è¿è¡Œå®˜æ–¹wordcount 1234567891011flink run -m yarn-cluster $FLINK_HOME/examples/batch/WordCount.jarflink run -m yarn-cluster -ynd 2 $FLINK_HOME/examples/batch/WordCount.jarflink run -m yarn-cluster -yn 4 $FLINK_HOME/examples/batch/WordCount.jarflink run -m yarn-cluster -yn 6 $FLINK_HOME/examples/batch/WordCount.jarflink run -m yarn-cluster -yn 8 $FLINK_HOME/examples/batch/WordCount.jarflink run -m yarn-cluster -yn 10 $FLINK_HOME/examples/batch/WordCount.jar","categories":[{"name":"å®‰è£…éƒ¨ç½²","slug":"å®‰è£…éƒ¨ç½²","permalink":"http://gangtieguo.cn/categories/å®‰è£…éƒ¨ç½²/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"},{"name":"FLINK","slug":"FLINK","permalink":"http://gangtieguo.cn/tags/FLINK/"}]},{"title":"kafkaçš„å¯åŠ¨åŠå¸¸ç”¨å‘½ä»¤","slug":"kafkaå¯åŠ¨è„šæœ¬åŠå‘½ä»¤","date":"2018-07-08T15:58:00.558Z","updated":"2018-09-09T03:51:50.714Z","comments":true,"path":"2018/07/08/kafkaå¯åŠ¨è„šæœ¬åŠå‘½ä»¤/","link":"","permalink":"http://gangtieguo.cn/2018/07/08/kafkaå¯åŠ¨è„šæœ¬åŠå‘½ä»¤/","excerpt":"kafkaçš„å¯åŠ¨è„šæœ¬ kafkaé›†ç¾¤æ²¡æœ‰è„šæœ¬èƒ½ç›´æ¥å¯åŠ¨æ‰€æœ‰çš„èŠ‚ç‚¹ï¼Œæ‰€ä»¥ä¸ºäº†ç¹æ‚å»æ¯å°æœºå™¨å¯åŠ¨ï¼Œæ‰€ä»¥ç¼–å†™äº†è„šæœ¬å¯åŠ¨æ‰€æœ‰ æ³¨æ„å‚ç…§æ›¿æ¢ vim kafka-startall.sh 1234567891011#!/bin/bashsed -e '1c borker.id=0' $KAFKA_HOME/config/server.properties$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.propertiesssh root@slave01 \"sed -i '1c borker.id=1 ' $KAFKA_HOME/config/server.properties\"ssh root@slave01 \"sed -i '5c host.name=slave01 ' $KAFKA_HOME/config/server.properties\"ssh root@slave01 \"sed -i '6c advertised.host.name=slave01 ' $KAFKA_HOME/config/server.properties\"ssh root@slave01 \"$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties\"ssh root@slave02 \"sed -i '1c borker.id=2 ' $KAFKA_HOME/config/server.properties\"ssh root@slave02 \"sed -i '5c host.name=slave02 ' $KAFKA_HOME/config/server.properties\"ssh root@slave02 \"sed -i '6c advertised.host.name=slave02 ' $KAFKA_HOME/config/server.properties\"ssh root@slave02 \"$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties\"","text":"kafkaçš„å¯åŠ¨è„šæœ¬ kafkaé›†ç¾¤æ²¡æœ‰è„šæœ¬èƒ½ç›´æ¥å¯åŠ¨æ‰€æœ‰çš„èŠ‚ç‚¹ï¼Œæ‰€ä»¥ä¸ºäº†ç¹æ‚å»æ¯å°æœºå™¨å¯åŠ¨ï¼Œæ‰€ä»¥ç¼–å†™äº†è„šæœ¬å¯åŠ¨æ‰€æœ‰ æ³¨æ„å‚ç…§æ›¿æ¢ vim kafka-startall.sh 1234567891011#!/bin/bashsed -e '1c borker.id=0' $KAFKA_HOME/config/server.properties$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.propertiesssh root@slave01 \"sed -i '1c borker.id=1 ' $KAFKA_HOME/config/server.properties\"ssh root@slave01 \"sed -i '5c host.name=slave01 ' $KAFKA_HOME/config/server.properties\"ssh root@slave01 \"sed -i '6c advertised.host.name=slave01 ' $KAFKA_HOME/config/server.properties\"ssh root@slave01 \"$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties\"ssh root@slave02 \"sed -i '1c borker.id=2 ' $KAFKA_HOME/config/server.properties\"ssh root@slave02 \"sed -i '5c host.name=slave02 ' $KAFKA_HOME/config/server.properties\"ssh root@slave02 \"sed -i '6c advertised.host.name=slave02 ' $KAFKA_HOME/config/server.properties\"ssh root@slave02 \"$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties\" kafkaçš„shellå‘½ä»¤jps -ml æŸ¥çœ‹kafkaçš„è¿è¡Œæƒ…å†µ å¯åŠ¨12nohup $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties &gt; /dev/null 2&gt;&amp;1 &amp;/usr/kafka/bin/kafka-server-start.sh -daemon /usr/kafka/config/server.properties åˆ›å»º topic1$KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper zk1:2181,zk2:2181,zk3:2181 --replication-factor 2 --partitions 3 --topic shuaige æŸ¥çœ‹æ¶ˆè´¹ä½ç½®1$KAFKA_HOME/bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper zk1:2181 --group testGroup æŸ¥çœ‹æŸä¸ª Topic çš„è¯¦æƒ…1$KAFKA_HOME/bin/kafka-topics.sh --topic test --describe --zookeeper zk1:2181,zk2:2181,zk3:2181 produceråˆ›å»ºå‘½ä»¤è¡Œæ¶ˆæ¯ç”Ÿäº§è€…1$KAFKA_HOME/bin/kafka-console-producer.sh --broker-list kafka1:9092,kafka2:9092,kafka3:9092 --topic shuaige Consumeråˆ›å»ºå‘½ä»¤è¡Œæ¶ˆè´¹è€…ä¹Ÿå¯ä»¥è¯´æ˜¯æŸ¥çœ‹message æ³¨æ„ï¼šä» kafka-0.9 ç‰ˆæœ¬åŠä»¥åï¼Œkafka çš„æ¶ˆè´¹è€…ç»„å’Œ offset ä¿¡æ¯å°±ä¸å­˜ zookeeper äº†ï¼Œè€Œæ˜¯å­˜åˆ° broker æœåŠ¡å™¨ä¸Šï¼Œæ‰€ä»¥ï¼Œå¦‚æœä½ ä¸ºæŸä¸ªæ¶ˆè´¹è€…æŒ‡å®šäº†ä¸€ä¸ªæ¶ˆè´¹è€…ç»„åç§°ï¼ˆgroup.idï¼‰ï¼Œé‚£ä¹ˆï¼Œä¸€æ—¦è¿™ä¸ªæ¶ˆè´¹è€…å¯åŠ¨ï¼Œè¿™ä¸ªæ¶ˆè´¹è€…ç»„åå’Œå®ƒè¦æ¶ˆè´¹çš„é‚£ä¸ª topic çš„ offset ä¿¡æ¯å°±ä¼šè¢«è®°å½•åœ¨ broker æœåŠ¡å™¨ä¸Šã€‚ oldç‰ˆæœ¬1$KAFKA_HOME/bin/kafka-console-consumer.sh --zookeeper zk1:2181,zk2:2181,zk3:2181 --topic shuaige --from-beginning --group testGroup newç‰ˆæœ¬1$KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9092,kafka2:9092,kafka3:9092 --topic shuaige --from-beginning --group testGroup æ¶ˆè´¹è€…è¦ä»å¤´å¼€å§‹æ¶ˆè´¹æŸä¸ª topic çš„å…¨é‡æ•°æ®ï¼Œéœ€è¦æ»¡è¶³ 2 ä¸ªæ¡ä»¶ï¼ˆspring-kafkaï¼‰ï¼š 12ï¼ˆ1ï¼‰ä½¿ç”¨ä¸€ä¸ªå…¨æ–°çš„&quot;group.id&quot;ï¼ˆå°±æ˜¯ä¹‹å‰æ²¡æœ‰è¢«ä»»ä½•æ¶ˆè´¹è€…ä½¿ç”¨è¿‡ï¼‰;ï¼ˆ2ï¼‰æŒ‡å®š&quot;auto.offset.reset&quot;å‚æ•°çš„å€¼ä¸ºearliestï¼› å¯¹åº”çš„ spring-kafka æ¶ˆè´¹è€…å®¢æˆ·ç«¯é…ç½®å‚æ•°ä¸ºï¼š 1234&lt;!-- æŒ‡å®šæ¶ˆè´¹ç»„å --&gt;&lt;entry key=&quot;group.id&quot; value=&quot;fg11&quot;/&gt;&lt;!-- ä»ä½•å¤„å¼€å§‹æ¶ˆè´¹,latest è¡¨ç¤ºæ¶ˆè´¹æœ€æ–°æ¶ˆæ¯,earliest è¡¨ç¤ºä»å¤´å¼€å§‹æ¶ˆè´¹,noneè¡¨ç¤ºæŠ›å‡ºå¼‚å¸¸,é»˜è®¤latest --&gt;&lt;entry key=&quot;auto.offset.reset&quot; value=&quot;earliest&quot;/&gt; æŸ¥çœ‹æ‰€æœ‰ topiczk è¡¨ç¤º zk çš„åœ°å€ åœ°å€è¡¨ç¤º zookeeper çš„åœ°å€ 1$KAFKA_HOME/bin/kafka-topics.sh --list --zookeeper zk1:2181,zk2:2181,zk3:2181 åˆ é™¤ topic1sh $KAFKA_HOME/bin/kafka-topics.sh --delete --zookeeper zk1:2181,zk2:2181,zk3:2181 --topic test","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/categories/å¤§æ•°æ®/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://gangtieguo.cn/tags/Kafka/"}]},{"title":"Docker-yaosong5/elké•œåƒ-ELKå®¹å™¨çš„æ­å»º","slug":"Docker-yaosong5:elké•œåƒåˆ¶ä½œ","date":"2018-07-08T15:43:05.950Z","updated":"2019-06-17T04:40:09.368Z","comments":true,"path":"2018/07/08/Docker-yaosong5:elké•œåƒåˆ¶ä½œ/","link":"","permalink":"http://gangtieguo.cn/2018/07/08/Docker-yaosong5:elké•œåƒåˆ¶ä½œ/","excerpt":"[TOC] æ¥æºå®¹å™¨ elkæ–°å»ºå®¹å™¨ï¼Œä¸ºå‡å°‘å·¥ä½œé‡ï¼Œå¼•ç”¨çš„æ˜¯æœ‰sshæœåŠ¡çš„Dockeré•œåƒkinogmt/centos-ssh:6.7ï¼Œç”Ÿæˆå®¹å™¨osä¸ºåŸºå‡†ã€‚ 1docker run -itd --name elk --hostname elk kinogmt/centos-ssh:6.7 &amp;&gt; /dev/null æ³¨æ„å¿…é¡»è¦ä»¥-dæ–¹å¼å¯åŠ¨ï¼Œä¸ç„¶sshdæœåŠ¡ä¸ä¼šå¯åŠ¨ï¼Œè¿™ç®—æ˜¯ä¸€ä¸ªå°bug","text":"[TOC] æ¥æºå®¹å™¨ elkæ–°å»ºå®¹å™¨ï¼Œä¸ºå‡å°‘å·¥ä½œé‡ï¼Œå¼•ç”¨çš„æ˜¯æœ‰sshæœåŠ¡çš„Dockeré•œåƒkinogmt/centos-ssh:6.7ï¼Œç”Ÿæˆå®¹å™¨osä¸ºåŸºå‡†ã€‚ 1docker run -itd --name elk --hostname elk kinogmt/centos-ssh:6.7 &amp;&gt; /dev/null æ³¨æ„å¿…é¡»è¦ä»¥-dæ–¹å¼å¯åŠ¨ï¼Œä¸ç„¶sshdæœåŠ¡ä¸ä¼šå¯åŠ¨ï¼Œè¿™ç®—æ˜¯ä¸€ä¸ªå°bug åœ¨å®¹å™¨ä¸­ä¸‹è½½éœ€è¦çš„elkçš„æºåŒ…ã€‚åšè§£å‹å°±ä¸èµ˜è¿°ï¼Œå¾ˆå¤šæ¡ˆä¾‹æ•™ç¨‹ã€‚ æˆ‘æ˜¯é‡‡ç”¨çš„ä¸‹è½½åˆ°å®¿ä¸»æœºï¼Œè§£å‹åï¼Œç”¨ â€œdocker cp è§£å‹åŒ…ç›®å½• os:/usr/loca/â€œæ¥ä¼ åˆ°å®¹å™¨å†…ï¼Œæ¯”åœ¨å®¹å™¨å†…ä¸‹è½½é€Ÿåº¦æ›´å¿« è®¾ç½®Home vim ~/bashrc 123456789export ES_HOME=/usr/esexport PATH=$ES_HOME/bin:$PATHexport KIBANA_HOME=/usr/kibanaexport PATH=$KIBANA_HOME/bin:$PATHexport LOGSTASH_HOME=/usr/logstashexport PATH=$LOGSTASH_HOME/bin:$PATHexport NODE_HOME=/usr/nodeexport PATH=$NODE_HOME/bin:$PATHexport NODE_PATH=$NODE_HOME/lib/node_modules source ~/.bashrc esé…ç½®åŠ å¯åŠ¨esçš„é…ç½®å¦‚ä¸‹ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# è¿™é‡ŒæŒ‡å®šçš„æ˜¯é›†ç¾¤åç§°ï¼Œéœ€è¦ä¿®æ”¹ä¸ºå¯¹åº”çš„ï¼Œå¼€å¯äº†è‡ªå‘ç°åŠŸèƒ½åï¼ŒES ä¼šæŒ‰ç…§æ­¤é›†ç¾¤åç§°è¿›è¡Œé›†ç¾¤å‘ç° #é›†ç¾¤åç§°ï¼Œé€šè¿‡ç»„æ’­çš„æ–¹å¼é€šä¿¡ï¼Œé€šè¿‡åç§°åˆ¤æ–­å±äºå“ªä¸ªé›†ç¾¤cluster.name: elk#èŠ‚ç‚¹åç§°ï¼Œè¦å”¯ä¸€(æ¯ä¸ªèŠ‚ç‚¹ä¸ä¸€æ ·)node.name: elk1#å¦‚æœæ˜¯masterèŠ‚ç‚¹è®¾ç½®æˆtrue node.master: true# æ•°æ®ç›®å½•path.data: /usr/es/data# log ç›®å½• path.logs: /usr/es/logs# ä¿®æ”¹ä¸€ä¸‹ ES çš„ç›‘å¬åœ°å€ï¼Œè¿™æ ·åˆ«çš„æœºå™¨ä¹Ÿå¯ä»¥è®¿é—® network.host: 0.0.0.0# é»˜è®¤çš„ç«¯å£å· http.port: 9200 # è®¾ç½®èŠ‚ç‚¹é—´äº¤äº’çš„tcpç«¯å£,é»˜è®¤æ˜¯9300 #transport.tcp.port: 9300#Elasticsearchå°†ç»‘å®šåˆ°å¯ç”¨çš„ç¯å›åœ°å€ï¼Œå¹¶å°†æ‰«æç«¯å£9300åˆ°9305ä»¥å°è¯•è¿æ¥åˆ°è¿è¡Œåœ¨åŒä¸€å°æœåŠ¡å™¨ä¸Šçš„å…¶ä»–èŠ‚ç‚¹ã€‚#è¿™æä¾›äº†è‡ªåŠ¨é›†ç¾¤ä½“éªŒï¼Œè€Œæ— éœ€è¿›è¡Œä»»ä½•é…ç½®ã€‚æ•°ç»„è®¾ç½®æˆ–é€—å·åˆ†éš”çš„è®¾ç½®ã€‚æ¯ä¸ªå€¼çš„å½¢å¼åº”è¯¥æ˜¯host:portæˆ–host#ï¼ˆå¦‚æœæ²¡æœ‰è®¾ç½®ï¼Œporté»˜è®¤è®¾ç½®ä¼štransport.profiles.default.port å›è½åˆ°transport.tcp.portï¼‰ã€‚#è¯·æ³¨æ„ï¼ŒIPv6ä¸»æœºå¿…é¡»æ”¾åœ¨æ‹¬å·å†…ã€‚é»˜è®¤ä¸º127.0.0.1, [::1]discovery.zen.ping.unicast.hosts: [\"elk1\",\"elk2\",\"elk3\"] #å¦‚æœæ²¡æœ‰è¿™ç§è®¾ç½®,é­å—ç½‘ç»œæ•…éšœçš„é›†ç¾¤å°±æœ‰å¯èƒ½å°†é›†ç¾¤åˆ†æˆä¸¤ä¸ªç‹¬ç«‹çš„é›†ç¾¤ - åˆ†è£‚çš„å¤§è„‘ - è¿™å°†å¯¼è‡´æ•°æ®ä¸¢å¤±# discovery.zen.minimum_master_nodes: 3 # enable corsï¼Œä¿è¯_site ç±»çš„æ’ä»¶å¯ä»¥è®¿é—® es #é¿å…å‡ºç°è·¨åŸŸé—®é¢˜ è¦è®¾ç½®ä¹‹å headeræ’ä»¶æ‰å¯ä»¥ç”¨http.cors.enabled: true http.cors.allow-origin: \"*\"# Centos6 ä¸æ”¯æŒ SecCompï¼Œè€Œ ES5.2.0 é»˜è®¤ bootstrap.system_call_filter ä¸º true è¿›è¡Œæ£€æµ‹ï¼Œæ‰€ä»¥å¯¼è‡´æ£€æµ‹å¤±è´¥ï¼Œå¤±è´¥åç›´æ¥å¯¼è‡´ ES ä¸èƒ½å¯åŠ¨ã€‚ bootstrap.memory_lock: false bootstrap.system_call_filter: false#åœ¨choremä¸­ å½“elasticsearchå®‰è£…x-packåè¿˜å¯ä»¥è®¿é—®#http.cors.allow-headers: Authorization#å¯ç”¨å®¡æ ¸ä»¥è·Ÿè¸ªä¸æ‚¨çš„Elasticsearchç¾¤é›†è¿›è¡Œçš„å°è¯•å’ŒæˆåŠŸçš„äº¤äº’#xpack.security.audit.enabled: true#è®¾ç½®ä¸ºtrueæ¥é”ä½å†…å­˜ã€‚å› ä¸ºå†…å­˜äº¤æ¢åˆ°ç£ç›˜å¯¹æœåŠ¡å™¨æ€§èƒ½æ¥è¯´æ˜¯è‡´å‘½çš„ï¼Œå½“jvmå¼€å§‹swappingæ—¶esçš„æ•ˆç‡ä¼šé™ä½ï¼Œæ‰€ä»¥è¦ä¿è¯å®ƒä¸swap#bootstrap.memory_lock: true ä¸èƒ½é€šè¿‡rootå¯åŠ¨åˆ›å»ºç”¨æˆ·elk1234useradd elkgroupadd elkusermod -a -G elk elkecho elk | passwd --stdin elk å°†elkæ·»åŠ åˆ°sudoers12echo \"elk ALL = (root) NOPASSWD:ALL\" | tee /etc/sudoers.d/elkchmod 0440 /etc/sudoers.d/elk è§£å†³sudo: sorry, you must have a tty to run sudoé—®é¢˜ï¼Œåœ¨/etc/sudoeræ³¨é‡Šæ‰ Default requiretty ä¸€è¡Œ1sudo sed -i 's/Defaults requiretty/Defaults:elk !requiretty/' /etc/sudoers ä¿®æ”¹æ–‡ä»¶æ‰€æœ‰è€…ä¸ºelkç”¨æˆ·1chown -R elk:elk /usr/es/ è®¾ç½®èµ„æºå‚æ•°ç”±äºeså¯åŠ¨ä¼šæœ‰èµ„æºè¦æ±‚ 123sudo vim /etc/security/limits.d/90-nproc.confæ·»åŠ elk soft nproc 4096 å†åœ¨docker-machineè®¾ç½®å‚æ•° 12docker-machine sshsudo sysctl -w vm.max_map_count=655360 eså¯åŠ¨è„šæœ¬æœ¬æœº su elk -c &quot;$ES_HOME/bin/elasticsearch -d&quot; sshè¿œç¨‹å¯åŠ¨å…¶ä»–ä¸»æœº ssh elk@elk1 &quot; $ES_HOME/bin/elasticsearch -d&quot; ssh root@elk1 &quot; su elk -c $ES_HOME/bin/elasticsearch &quot; å®‰è£…esæ’ä»¶headerå®‰è£…nodejsä¸€èˆ¬é¢„è£…çš„ç‰ˆæœ¬ä¸å¯¹ 123yum erase nodejs npm -y # å¸è½½æ—§ç‰ˆæœ¬çš„nodejsrpm -qa &apos;node|npm&apos; | grep -v nodesource # ç¡®è®¤nodejsæ˜¯å¦å¸è½½å¹²å‡€yum install nodejs -y # å®‰è£…npm å®‰è£…çš„ç‰ˆæœ¬ä¼šæœ‰ä¸å¯¹ ä¸‹è½½åˆé€‚ç‰ˆæœ¬ 1234cd /usrwget https://npm.taobao.org/mirrors/node/latest-v4.x/node-v4.4.7-llinux-x64.tar.gztar -zxvf node-v4.4.7-linux-x64.tar.gzmv node-v8.9.1-linux-x64 node ç›´æ¥å°†nodeç›®å½•é…ç½®åˆ°homeå³å¯ 12export NODE_HOME=/usr/nodeexport PATH=$NODE_HOME/bin:$PATH ä¸‹è½½ headerï¼Œå®‰è£…gruntï¼ˆæ‰€æœ‰å‘½ä»¤åœ¨hearçš„æ‰€åœ¨ç›®å½•æ‰§è¡Œï¼‰ wget https://github.com/mobz/elasticsearch-head/archive/master.zip unzip master.zip çœ‹å½“å‰ head æ’ä»¶ç›®å½•ä¸‹æœ‰æ—  node_modules/grunt ç›®å½•ï¼šæ²¡æœ‰ï¼šæ‰§è¡Œå‘½ä»¤åˆ›å»ºï¼š 1npm install grunt --save å®‰è£… gruntï¼šgrunt æ˜¯åŸºäº Node.js çš„é¡¹ç›®æ„å»ºå·¥å…·ï¼Œå¯ä»¥è¿›è¡Œæ‰“åŒ…å‹ç¼©ã€æµ‹è¯•ã€æ‰§è¡Œç­‰ç­‰çš„å·¥ä½œï¼Œhead æ’ä»¶å°±æ˜¯é€šè¿‡ grunt å¯åŠ¨ 1npm install -g grunt-cli å‚è€ƒhttps://blog.csdn.net/ggwxk1990/article/details/78698648 npm install å®‰è£…æ‰€ä¸‹è½½çš„headeråŒ… 1npm install é…ç½®headerä¿®æ”¹æœåŠ¡å™¨ç›‘å¬åœ°å€: Gruntfile.jsvi $HEADER_HOME/Gruntfile.js åœ¨ç¬¬ 93 è¡Œæ·»åŠ ï¼š 1hostname:'*', g) ä¿®æ”¹è¿æ¥åœ°å€ï¼švim $HEADER_HOME/_site/app.js 1285è¡Œ 1this.base_uri = this.config.base_uri || this.prefs.get(\"app-base_uri\") || \"http://192.168.33.16:9200\" headerå¯åŠ¨åœ¨ elasticsearch-head-master ç›®å½•ä¸‹ 1grunt server æˆ–è€… npm run start headerçš„é»˜è®¤ç«¯å£ä¸º9100 headerçš„åœæ­¢å‘½ä»¤elké›†ç¾¤å¯åŠ¨ç”±äºlogstashå’Œkibanaéƒ½ä¸éœ€è¦å…¶ä»–è®¾ç½®ï¼Œç›´æ¥ç”¨é¢„è®¾çš„é…ç½® elasticSearchè„šæœ¬å¯åŠ¨è„šæœ¬ vim es-start.sh 12345678#!/bin/bashsed -i '6c node.name: es1 '$ES_HOME/config/elasticsearch.ymlsu - elk -c \"$ES_HOME/bin/elasticsearch -d\"ssh root@elk2 \"sed -i '6c node.name: es2 ' $ES_HOME/config/elasticsearch.yml\"ssh root@elk2 ' su - elk -c \"$ES_HOME/bin/elasticsearch -d\" 'ssh root@elk3 \"sed -i '6c node.name: es3 ' $ES_HOME/config/elasticsearch.yml\"ssh root@elk3 ' su - elk -c \"$ES_HOME/bin/elasticsearch -d\" ' kibanaè„šæœ¬å¯åŠ¨ï¼ˆç«¯å£ä¸º5601ï¼‰ å¯åŠ¨å•æœºï¼ˆåªéœ€è¦å¯åŠ¨å•æœºï¼‰ $KIBANA_HOME/bin/kibana12345678#!/bin/bashsed -i '3c http://elk1:9200 '$KIBANA_HOME/config/kibana.ymlnohup $KIBANA_HOME/bin/kibana &amp;ssh root@elk2 \"sed -i '3c http://elk2:9200 ' $KIBANA_HOME/config/kibana.yml\"ssh root@elk2 \"nohup $KIBANA_HOME/bin/kibana &amp; \"ssh root@elk3 \"sed -i '3c http://elk3:9200 ' $KIBANA_HOME/config/kibana.yml\"ssh root@elk3 \"nohup $KIBANA_HOME/bin/kibana &amp; \" logstashè„šæœ¬å¯åŠ¨å•æœºå¯åŠ¨ 12#$LOGSTASH_HOME/bin/logstash -f é…ç½®æ–‡ä»¶çš„ç›®å½•$LOGSTASH_HOME/bin/logstash -f logstash.conf \b é›†ç¾¤å¯åŠ¨è„šæœ¬ logstash-start.sh1234#!/bin/bashnohup $LOGSTASH_HOME/bin/logstash -f $LOGSTASH_HOME/conf/$1 &amp;ssh root@elk2 \"nohup $LOGSTASH_HOME/bin/logstash -f $LOGSTASH_HOME/conf/$1 &amp; \"ssh root@elk3 \"nohup $LOGSTASH_HOME/bin/logstash -f $LOGSTASH_HOME/conf/$1 &amp; \" ä¿å­˜å®¹å™¨ä¸ºé•œåƒ1docker commit -m \"elké•œåƒ\" --author=\"yaosong\" os yaosong5/elk:1.0 osæ˜¯ä½ é…ç½®çš„å®¹å™¨å ç”Ÿæˆelk å®¹å™¨123docker run -itd --net=br --name elk1 --hostname elk1 yaosong5/elk:1.0 &amp;&gt; /dev/nulldocker run -itd --net=br --name elk2 --hostname elk2 yaosong5/elk:1.0 &amp;&gt; /dev/nulldocker run -itd --net=br --name elk3 --hostname elk3 yaosong5/elk:1.0 &amp;&gt; /dev/nulls åœæ­¢/åˆ é™¤elk å®¹å™¨1234567docker stop elk1docker stop elk2docker stop elk3docker rm elk1docker rm elk2docker rm elk3 å‚è€ƒelk æ“ä½œå‘½ä»¤esæ“ä½œå‘½ä»¤http://www.yfshare.vip/2017/11/04/%E9%83%A8%E7%BD%B2FileBeat-logstash-elasticsearch%E9%9B%86%E7%BE%A4-kibana/#%E9%85%8D%E7%BD%AE-filebeart å…¶ä»–yum erase nodejs npm -y # å¸è½½æ—§ç‰ˆæœ¬çš„nodejsrpm -qa â€˜node|npmâ€™ | grep -v nodesource # ç¡®è®¤nodejsæ˜¯å¦å¸è½½å¹²å‡€yum install nodejs -y","categories":[{"name":"å®‰è£…éƒ¨ç½²","slug":"å®‰è£…éƒ¨ç½²","permalink":"http://gangtieguo.cn/categories/å®‰è£…éƒ¨ç½²/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"},{"name":"ELK","slug":"ELK","permalink":"http://gangtieguo.cn/tags/ELK/"},{"name":"es","slug":"es","permalink":"http://gangtieguo.cn/tags/es/"}]},{"title":"Dockerå¸¸ç”¨å‘½ä»¤æ±‡é›†","slug":"Dockerå‘½ä»¤æ±‡é›†","date":"2018-06-26T02:33:37.714Z","updated":"2019-06-17T04:40:09.372Z","comments":true,"path":"2018/06/26/Dockerå‘½ä»¤æ±‡é›†/","link":"","permalink":"http://gangtieguo.cn/2018/06/26/Dockerå‘½ä»¤æ±‡é›†/","excerpt":"","text":"[TOC] Dockeræºé…ç½® å®‰è£…è¿‡ç¨‹ä¸­éœ€è¦é‡å›½å¤– docker ä»“åº“ä¸‹è½½æ–‡ä»¶ï¼Œé€Ÿåº¦å¤ªæ…¢ï¼Œå»ºè®®é…ç½® docker å›½å†…é•œåƒä»“åº“ï¼š vi /etc/docker/daemon.json 1&#123;\"registry-mirrors\":[\"http://c1f0a193.m.daocloud.io\"] &#125; å¸¸ç”¨å‘½ä»¤å¯åŠ¨å®¹å™¨(ä¹Ÿæ˜¯åˆ›å»º) 123docker run -itd --net=br --name master --hostname master yaosong5/bigdata:1.0 &amp;&gt; /dev/nullå¦‚æœä»¥ /bin/bashå¯åŠ¨çš„è¯ï¼ŒsshdæœåŠ¡ä¸ä¼šå¯åŠ¨(dockeræœªçŸ¥bug)ç”¨ &amp;&gt; /dev/nullè¿™ç§æ–¹å¼sshdæœåŠ¡æ‰ä¼šå¯åŠ¨ åˆ›å»ºå®¹å™¨-run12345--name --hostname (åŒ-h) --net= -dè¡¨ç¤ºåå°å¯åŠ¨æ­¤å‘½ä»¤ä¸ä¼šæ‰“å°å‡ºå®¹å™¨iddocker run -itd --net=br --name hm --hostname hadoop-master kiwenlau/hadoop:1.0 &amp;&gt; /dev/null ï¼ˆhadoopé•œåƒï¼‰è®¾ç½®é™æ€å›ºå®šipdocker run -d --net=br --name=c6 --ip=192.168.33.6 nginx 12345è‡ªåŠ¨åˆ†é…Ipdocker run -d --net=br --name=c1 nginxè®¾ç½®dockeré»˜è®¤ipæ®µå‘½ä»¤docker run -itd -P -p 50070:50070 -p 8088:8088 -p 8080:8080 --name master -h master --add-host slave01:172.17.0.3 --add-host slave02:172.17.0.4 centos:ssh-spark-hadoop åˆ›å»ºå®¹å™¨-Dockerfileåœ¨dockerfilleæ–‡ä»¶æ‰€åœ¨çš„ç›®å½•ä¸­æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ 123docker build -f Dockerfile -t hadoop:v1 .#ä¹Ÿå¯ä»¥æŒ‡å®šDockerfileå¦‚ä¸‹ï¼šdocker build -t centos:tt - &lt; Dockerfile å®¹å™¨æŒ‚è½½ç›®å½•åˆ†ä¸¤ç§ç±»å‹ composeæ–‡ä»¶ï¼š 12volumes: - /Users/yaosong/Yao/dev/hadoop/dfs/name:/root/hadoop/dfs/name shellå‘½ä»¤ï¼š 12-v : docker run -it -v /test:/soft centos /bin/bash â€œ:â€å‰ç›®å½•ä¸ºå®¿ä¸»æœºç›®å½•ï¼Œåç›®å½•ä¸ºå®¹å™¨ç›®å½• å¼•ç”¨ åŸºäº docker çš„å¤§æ•°æ®æ¶æ„ å¦‚æœæ˜¯macæˆ–è€…winæœºå™¨ï¼Œéœ€è¦åœ¨virtualboxè™šæ‹Ÿæœºä¸­è®¾ç½®å…±äº«æ–‡ä»¶å¤¹çš„shareåç§°å¯¹åº”macçš„ç›®å½•è™šæ‹Ÿæœºä¸­çš„ç›®å½• sudo mount -t vboxsf vagrant /Users/yaosong sudo mount -t vboxsf Yao /Users/yaosong/Yao/ å–æ¶ˆæŒ‚è½½ sudo umount vagrant åˆ é™¤æ‰€æœ‰æœªç”¨çš„ Data volumes1docker volume prune run å‘½ä»¤è§£é‡Š12345-d æ˜¯åå°å¯åŠ¨docker run -itd --net=br --name spark --hostname spark yaosong5/spark:2.1.0 &amp;&gt; /dev/nullsudo docker exec -it spark bashï¼ˆè¿›å…¥åå°å¯åŠ¨çš„å®¹å™¨ï¼‰å’Œä¸‹é¢ä¸€æ ·ï¼ˆç›´æ¥è¿›å…¥ï¼‰docker run -it --net=br --name spark --hostname spark yaosong5/spark:2.1.0 bash pauseæš‚åœæ¢å¤å®¹å™¨123docker pause å®¹å™¨åæ¢å¤æ•°æ®åº“å®¹å™¨db01æä¾›æœåŠ¡ã€‚docker unpause å®¹å™¨å exec è¿›å…¥åå°å®¹å™¨123docker exec -it spark bashdocker exec -it å®¹å™¨å bashæ‰§è¡Œå‘½ä»¤ docker exec -it å®¹å™¨å ip addr å¯ä»¥æ‹¿åˆ° a0 å®¹å™¨çš„ ip logsæŸ¥çœ‹å®¹å™¨å¯åŠ¨æ—¥å¿—1docker logs -f -t --tail 100 kanbigdata_namenode_1 æŸ¥çœ‹å®¹å™¨ä¿¡æ¯12docker inspect hmæ‰§è¡Œå‘½ä»¤ docker exec -it a0 ip addr å¯ä»¥æ‹¿åˆ° a0 å®¹å™¨çš„ ip å¯åŠ¨ å…³é—­ åˆ é™¤å®¹å™¨123docker start docker stop å®¹å™¨ådocker rm å®¹å™¨å cpå®¹å™¨å®¿ä¸»äº’æ‹·æ–‡ä»¶12docker cp /Users/yaosong/Yao/etc.tar f7e795c0fddd:/åä¸ºå®¹å™¨id:/ç›®å½• åˆ é™¤é•œåƒ1234ï¼ˆæ ¹æ®é•œåƒidåˆ é™¤ï¼‰docker rmi 00de07ebadffç”¨docker images -a æŸ¥çœ‹image idï¼Œä¹Ÿå¯docker rmi é•œåƒå:ç‰ˆæœ¬å· ä¿å­˜é•œåƒ123docker commit -m \"centos-6.9 with spark 2.2.0 and hadoop 2.8.0\" os centos:hadoop-sparkdocker commit -m \"bigdata:spark,hadoop,hive,mysql and shell foundation\" --author=\"yaosong\" master yao/os/bigdata:2.1 Dockerç”¨DockerFileåˆ›å»ºé•œåƒ123docker build -t hadoop:v1- &lt;Dockerfiledocker build -t=\"hadoop:v1\" . ï¼ˆ.è¡¨ç¤ºæ˜¯å½“å‰æ–‡ä»¶å¤¹ï¼Œä¹Ÿå°±æ˜¯dockerfileæ‰€åœ¨æ–‡ä»¶å¤¹ï¼‰docker build -f Dockerfile -t hadoop:v1 . æ­¤å‘½ä»¤ä¹Ÿå¯ ä¸€é”®å¯åŠ¨docker-compose.ymlç¼–æ’çš„æ‰€æœ‰æœåŠ¡1docker-compose -f docker-compose.yml up -d Dockeræ”¹å˜æ ‡ç­¾docker tag IMAGEID(é•œåƒid) REPOSITORY:TAGï¼ˆä»“åº“ï¼šæ ‡ç­¾ï¼‰ 1docker tag b7a66cb0e8ba yaosong5/bigdata:1.0 æœç´¢dockeré•œåƒ1docker search yaosong5 ç™»å½•dockerè´¦æˆ·1docker login ç™»å½•docker hubä¸­æ³¨å†Œçš„è´¦æˆ· ä¸Šä¼ ä»“åº“1docker push yaosong5/elk:1.0 å®¹å™¨ä¿å­˜ä¸ºé•œåƒï¼ŒåŠ è½½æœ¬åœ°é•œåƒ å¼•ç”¨1234567docker save imageID &gt; filenamedocker load &lt;filenameå¦‚ï¼šdocker save 4f9e92e56941&gt; /Users/yaosong/centosSparkHadoop.tardocker load &lt;/Users/yaosong/centosSparkHadoop.taré€šè¿‡ image ä¿å­˜çš„é•œåƒä¼šä¿å­˜æ“ä½œå†å²ï¼Œå¯ä»¥å›æ»šåˆ°å†å²ç‰ˆæœ¬ã€‚ ä¿å­˜ï¼ŒåŠ è½½å®¹å™¨å‘½ä»¤ï¼š12docker export containID &gt; filenamedocker import filename [newname] é€šè¿‡å®¹å™¨ä¿å­˜çš„é•œåƒä¸ä¼šä¿å­˜æ“ä½œå†å²ï¼Œæ‰€ä»¥æ–‡ä»¶å°ä¸€ç‚¹ã€‚å¦‚æœè¦è¿è¡Œé€šè¿‡å®¹å™¨åŠ è½½çš„é•œåƒï¼Œ éœ€è¦åœ¨è¿è¡Œçš„æ—¶å€™åŠ ä¸Šç›¸å…³å‘½ä»¤ã€‚ é«˜é˜¶å‘½ä»¤å‚è€ƒï¼šæ¸…ç†Dockerå ç”¨çš„ç£ç›˜ç©ºé—´ åˆ é™¤æ‰€æœ‰å…³é—­çš„å®¹å™¨ 1docker ps -a | grep Exit | cut -d ' ' -f 1 | xargs docker rm åˆ é™¤æ‰€æœ‰danglingé•œåƒ(å³æ— tagçš„é•œåƒ)ï¼š 1docker rmi $(docker images | grep \"^&lt;none&gt;\" | awk \"&#123;print $3&#125;\") åˆ é™¤æ‰€æœ‰danglingæ•°æ®å·(å³æ— ç”¨çš„volume)ï¼š 1docker volume rm $(docker volume ls -qf dangling=true) docker systemå®ƒå¯ä»¥ç”¨äºç®¡ç†ç£ç›˜ç©ºé—´ ç£ç›˜ä½¿ç”¨æƒ…å†µ docker system dfå‘½ä»¤ï¼Œç±»ä¼¼äºLinuxä¸Šçš„dfå‘½ä»¤ï¼Œç”¨äºæŸ¥çœ‹Dockerçš„ç£ç›˜ä½¿ç”¨æƒ…å†µ æ¸…ç†ç£ç›˜ docker system prune å‘½ä»¤å¯ä»¥ç”¨äºæ¸…ç†ç£ç›˜ï¼Œåˆ é™¤å…³é—­çš„å®¹å™¨ã€æ— ç”¨çš„æ•°æ®å·å’Œç½‘ç»œï¼Œä»¥åŠdanglingé•œåƒ(å³æ— tagçš„é•œåƒ)ã€‚docker system prune -aå‘½ä»¤æ¸…ç†å¾—æ›´åŠ å½»åº•ï¼Œå¯ä»¥å°†æ²¡æœ‰å®¹å™¨ä½¿ç”¨Dockeré•œåƒéƒ½åˆ æ‰ã€‚æ³¨æ„ï¼Œè¿™ä¸¤ä¸ªå‘½ä»¤ä¼šæŠŠä½ æš‚æ—¶å…³é—­çš„å®¹å™¨ï¼Œä»¥åŠæš‚æ—¶æ²¡æœ‰ç”¨åˆ°çš„Dockeré•œåƒéƒ½åˆ æ‰äº†â€¦æ‰€ä»¥ä½¿ç”¨ä¹‹å‰ä¸€å®šè¦æƒ³æ¸…æ¥šå¶ã€‚ Docker-machineå‘½ä»¤åˆ—å‡ºdocker-machine1docker-machine ls å¼€å¯è™šæ‹Ÿæœº1docker-machine start default å…³é—­è™šæ‹Ÿæœº1docker-machine stop default é‡å¯è™šæ‹Ÿæœº1docker-machine restart default åˆ é™¤è™šæ‹Ÿæœº1docker-machine rm default è®¾ç½®ç¯å¢ƒå˜é‡docker-machine1eval $(docker-machine env default) # Setup the environment å…¶ä»–å‚è€ƒip -4 addr chkconfig sshd on rpm -qa|grep sshã€‚ ä¸ºäº†å¿«é€Ÿå®ç°æˆ‘ä»¬å°±ä¸è‡ªå·±è£… SSH æœåŠ¡äº†ï¼Œhub.docker.com ä¸Šçš„ kinogmt/centos-ssh:6.7 è¿™ä¸ªé•œåƒå°±èƒ½æ»¡è¶³æˆ‘ä»¬çš„è¦æ±‚","categories":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"}]},{"title":"å„ç§å¿«æ·é”®","slug":"å„ç§å¿«æ·é”®","date":"2018-06-21T15:40:13.306Z","updated":"2019-06-17T04:40:09.404Z","comments":true,"path":"2018/06/21/å„ç§å¿«æ·é”®/","link":"","permalink":"http://gangtieguo.cn/2018/06/21/å„ç§å¿«æ·é”®/","excerpt":"ä¸€äº›å¸¸ç”¨å¿«æ·é”®è®©äººäº‹åŠåŠŸå€","text":"ä¸€äº›å¸¸ç”¨å¿«æ·é”®è®©äººäº‹åŠåŠŸå€ â€‹ |iterm|Command + Shift + h iterms2å¤åˆ¶å†å² åˆ†å± command + option + æ–¹å‘é”® command + [ æˆ– command + ] åˆ†å±åˆ‡æ¢å±å¹• Control + a åˆ°è¡Œé¦– Control + u æ¸…é™¤å½“å‰è¡Œ Control + e åˆ°è¡Œå°¾ Control + p / !! ä¸Šä¸€æ¡å‘½ä»¤ Control + k ä»å…‰æ ‡å¤„åˆ è‡³å‘½ä»¤è¡Œå°¾ (æœ¬æ¥ Control + u æ˜¯åˆ è‡³å‘½ä»¤è¡Œé¦–ï¼Œä½†itermä¸­æ˜¯åˆ æ‰æ•´è¡Œ) Control + w A + d ä»å…‰æ ‡å¤„åˆ è‡³å­—é¦–/å°¾ Control + k åˆ é™¤åˆ°æ–‡æœ¬æœ«å°¾ Control + h åˆ æ‰å…‰æ ‡å‰çš„è‡ªè´Ÿ Control + d åˆ æ‰å…‰æ ‡åçš„è‡ªè´Ÿ Control + r æœç´¢å‘½ä»¤å†å²ï¼Œè¿™ä¸ªè¾ƒå¸¸ç”¨ |Alfred|Command + Option + C afrendå‰ªåˆ‡æ¿å†å² Command + Option + / afrendè·¯å¾„å†å² Command + Option + \\ afrendå¯¹æœç´¢çš„è·¯å¾„è¿›è¡Œæ“ä½œ å¦‚å¤åˆ¶ç­‰ç­‰ â€‹â€‹ â€‹ |sublime|Command + Shift + d å¤åˆ¶ä¸€è¡Œ Command + Option + f æŸ¥æ‰¾å¹¶æ›¿æ¢ CTRL + - ä¸Šä¸ªæ‰“å¼€çš„æ–‡ä»¶ |idea|Command + Shift + F12 ç¼–æ å…¨å± å…¶å®å°±æ˜¯ Hide All Tool Windows (éšè—æ‰€æœ‰å·¥å…·çª—å£) è¿™ä¸ªæ“ä½œçš„å¿«æ·é”®ã€‚ Command + Option + Space ç±»åæˆ–æ¥å£åæç¤º Control + ; æ˜¯ä»€ä¹ˆ ä»£æ›¿é¼ æ ‡ Command + l è·³åˆ°æŒ‡å®šè¡Œ Command + w å…³é—­æ ‡ç­¾é¡µ Option + ä¸Š å’Œwindowsçš„ctrl+wç›¸åŒ é€’è¿›é€‰ä¸­ä»£ç å— Alt + Insert Command + N ä»£ç è‡ªåŠ¨ç”Ÿæˆï¼Œå¦‚ç”Ÿæˆå¯¹è±¡çš„ set / get æ–¹æ³•ï¼Œæ„é€ å‡½æ•°ï¼ŒtoString() ç­‰ Alt + å‰æ–¹å‘é”® Control + å‰æ–¹å‘é”® å½“å‰å…‰æ ‡è·³è½¬åˆ°å½“å‰æ–‡ä»¶çš„å‰ä¸€ä¸ªæ–¹æ³•åä½ç½® Ctrl + Alt + Enter Command + Option + Enter å…‰æ ‡æ‰€åœ¨è¡Œä¸Šç©ºå‡ºä¸€è¡Œï¼Œå…‰æ ‡å®šä½åˆ°æ–°è¡Œ Ctrl + Alt + å·¦æ–¹å‘é”® Command + Option + å·¦æ–¹å‘é”® é€€å›åˆ°ä¸Šä¸€ä¸ªæ“ä½œçš„åœ°æ–¹ Ctrl + Alt + å³æ–¹å‘é”® Command + Option + å³æ–¹å‘é”® å‰è¿›åˆ°ä¸Šä¸€ä¸ªæ“ä½œçš„åœ°æ–¹ Command + Option + T åŒ…å›´ä»£ç  Command + Shift +v å†å² Find usage æŸ¥çœ‹å˜é‡æ–¹æ³•çš„ç±»çš„ç›´æ¥ä½¿ç”¨æƒ…å†µ Shift + Enter å¼€å§‹æ–°çš„ä¸€è¡Œ Command + P æ–¹æ³•å‚æ•°æç¤º Command + U å‰å¾€çˆ¶ç±» Command + +/- å±•å¼€/æŠ˜å ä»£ç  Alt + 1,2,3...9 Command + 1,2,3...9 æ˜¾ç¤ºå¯¹åº”æ•°å€¼çš„é€‰é¡¹å¡ï¼Œå…¶ä¸­ 1 æ˜¯ Project ç”¨å¾—æœ€å¤š Control + Option + O ä¼˜åŒ–å¯¼å…¥çš„ç±»ï¼Œå¯ä»¥å¯¹å½“å‰æ–‡ä»¶å’Œæ•´ä¸ªåŒ…ç›®å½•ä½¿ç”¨ Ctrl + Alt + T å¯¹é€‰ä¸­çš„ä»£ç å¼¹å‡ºç¯ç»•é€‰é¡¹å¼¹å‡ºå±‚ Control + Option + H ç»§æ‰¿å…³ç³» Control + H æ¥å£åˆ°å®ç°ç±» Control + Shift + J æ™ºèƒ½çš„å°†ä»£ç æ‹¼æ¥æˆä¸€è¡Œ Command+Alt+V å¼•å…¥å˜é‡ï¼Œè‡ªåŠ¨å¯¼å…¥å˜é‡ Option + F7 æŸ¥è¯¢æ‰€é€‰å¯¹è±¡/å˜é‡è¢«å¼•ç”¨ 4ã€ç±»ã€æ–¹æ³•ã€æ–‡ä»¶å®šä½ æŸ¥æ‰¾ç±» ctr + N æŸ¥æ‰¾æ–‡ä»¶ Ctrl + Shift + N ç¬¦å·å®šä½ Ctrl + Alt + Shift + N æŸ¥çœ‹æ–‡ä»¶ç»“æ„ ctrl + F12 æœ€è¿‘æ‰“å¼€çš„æ–‡ä»¶ ctr + E å®šä½ä¸‹ä¸€ä¸ªæ–¹æ³• alt + down å®šä½ä¸Šä¸€ä¸ªæ–¹æ³• alt + up æŸ¥çœ‹æ–¹æ³•å‚æ•°ä¿¡æ¯ ctr + p æŸ¥çœ‹æ–¹æ³•ã€ç±»çš„ doc ctr + Q è¡Œæ•° Command + l 5ã€ç±»ã€æ–¹æ³•çš„ç»“æ„æŸ¥çœ‹ã€å®šä½ è·³åˆ°ç±»æˆ–æ–¹æ³•çš„å£°æ˜ ctr + B å®šä½åˆ°ç±»çš„çˆ¶ç±»ã€æ¥å£ ctr + U ç”»å›¾ ommand+Option+u æŸ¥çœ‹ç±»çš„ç»§æ‰¿ç»“æ„ ctr + H æŸ¥çœ‹æ–¹æ³•çš„ç»§æ‰¿ç»“æ„ ctr + Shift + H æŸ¥çœ‹ç±»æˆ–æ–¹æ³•è¢«è°ƒç”¨æƒ…å†µ ctr + alt +H åŸåœ°å‚çœ‹ç±»ã€æ–¹æ³•çš„å£°æ˜ Ctrl + Shift + I â€‹ Control + H æ˜¾ç¤ºå½“å‰ç±»çš„å±‚æ¬¡ç»“æ„ ç»§æ‰¿ Command + Shift + H æ˜¾ç¤ºæ–¹æ³•å±‚æ¬¡ç»“æ„ Control + Option + H æ˜¾ç¤ºè°ƒç”¨å±‚æ¬¡ç»“æ„ Command + L åœ¨å½“å‰æ–‡ä»¶è·³è½¬åˆ°æŸä¸€è¡Œçš„æŒ‡å®šå¤„ Command + B / Command + é¼ æ ‡ç‚¹å‡» è¿›å…¥å…‰æ ‡æ‰€åœ¨çš„æ–¹æ³•/å˜é‡çš„æ¥å£æˆ–æ˜¯å®šä¹‰å¤„ Command + Option + B è·³è½¬åˆ°å®ç°å¤„ Command + G æŸ¥æ‰¾æ¨¡å¼ä¸‹ï¼Œå‘ä¸‹æŸ¥æ‰¾ Command + Shift + U å¤§å°å†™åˆ‡æ¢ |Mac|â€‹ Shift+Command+G è·³è½¬æ‰“å¼€æ–‡ä»¶å¤¹ Command + Shift + . æ˜¾ç¤ºéšè—æ–‡ä»¶ Command + Control + ç©ºæ ¼ emojiè¡¨æƒ… Shift + Optionï¼‰+ K Apple logo ã€Œï£¿ ã€ Command+i ç®€ä»‹ Shift+Control+d æœç‹—è¡¨æƒ…åŒ… Shift + Command + ç©ºæ ¼ å†å²æ–‡ä»¶ Control + Command +F å…¨å±æ¨¡å¼ Command + z æ‰“å¼€safariä¸‹æœ€è¿‘å…³é—­tabé¡µé¢ Command + Option + Shift + Esc å¼ºåˆ¶é€€å‡ºæ´»è·ƒçš„ Command + Option + Esc å¼ºåˆ¶é€€å‡º Command + ` åˆ‡æ¢åŒä¸€åº”ç”¨çš„çª—å£ Shift + Command + d alt + cmd + space å¿«é€Ÿæ‰“å¼€finder cmd + Option + Shift + v å»æ ¼å¼ç²˜è´´ï¼ˆäº²æµ‹å¤§éƒ¨åˆ†è½¯ä»¶éƒ½å¯ä»¥ï¼‰ Command + Tab åˆ‡æ¢åº”ç”¨çš„æ—¶å€™ï¼Œå¯ä»¥æ¾å¼€Tabï¼Œç„¶åæŒ‰Qé€€å‡ºé€‰ä¸­çš„åº”ç”¨ã€‚ ä¸‰æŒ‡ç‚¹å‡»ç½‘é¡µé“¾æ¥ï¼Œå¯ä»¥é¢„è§ˆé“¾æ¥ç½‘é¡µã€‚ æŒ‰ä½å³ä¸Šè§’çš„æ–°å»ºé€‰é¡¹å¡æŒ‰é’®èƒ½å¿«é€Ÿæµè§ˆå¹¶é€‰æ‹©æœ€è¿‘å…³é—­çš„çª—å£ |finder|Command + 1ï¼Œ2ï¼Œ3 å›¾æ ‡ï¼Œåˆ—è¡¨ï¼Œåˆ†æ æ˜¾ç¤ºæ–‡ä»¶å¤¹ Command + Control + P å¤åˆ¶è·¯å¾„ è‡ªå·±é…ç½® Command + Option + B å¿«æ·é”®æ ‡è®° è‡ªå·±é…ç½® Command + O æ‰“å¼€æ–‡ä»¶å¤¹ Command + ä¸‹ è¿›å…¥æ–‡ä»¶å¤¹ Command + ä¸Š è¿”å›æ–‡ä»¶å¤¹ Command ï¼‹ï¼» è¿”å› Command ï¼‹ ï¼½ å‰è¿› | macå‘½ä»¤|æ ¹æ® asker æç¤º ä½œè¡¥å……ï¼š command + fn + å·¦/å³ï¼Œå¯ä»¥è°ƒæ•´åˆ°æ–‡ä»¶å¼€å¤´ / ç»“å°¾ã€‚ fn + å·¦/å³ç›¸å½“äºhome/endåœ¨ ç½‘é¡µå’Œå¤šæ•°æ–‡æ¡£ä¸­é€‚ç”¨ã€‚ â€‹ defaults write com.apple.finder QuitMenuItem -bool YES è®¾ç½®finderå¯ä»¥å…³é—­ open -n /Applications/WizNote.app å¤šæ¬¡æ‰“å¼€ä¸€ä¸ªåº”ç”¨ mac æ²¡æœ‰å£°éŸ³ sudo kill -9 `ps ax|grep &apos;coreaudio[a-z]&apos; |awk &apos;{print $1}&apos;` â€‹ sudo killall coreaudiodâ€‹â€‹ ä½¿ç”¨åçš„æ•ˆæœï¼Œå¯ä»¥è¯´æ˜¯éå¸¸æ˜æ˜¾äº†ï¼Œå†ä¹Ÿä¸ä¼šæœ‰åœ¨ã€ŒæŒ¤ç‰™è†ã€çš„æ„Ÿè§‰ã€‚è®©å®ƒå›åˆ°æœ€å¼€å§‹çš„çŠ¶æ€ï¼š defaults delete com.apple.dock; killall Dock defaults write com.apple.Dock autohide-delay -float 0 &amp;&amp; killall Dock æ‰“å¼€å¼€æœºå£°éŸ³ sudo nvram -d SystemAudioVolume åŒå±åˆ†ä»»åŠ¡å·¥ä½œï¼åªè¦æŒ‰ä½çª—å£å·¦ä¸Šè§’çš„ç»¿è‰²ï¼‹å³å¯ å»æ‰èµ„æºåº“æ–‡ä»¶å¤¹çš„éšè—å±æ€§ chflags nohidden ~/Library/ æ‰“å¼€éšè—å±æ€§ chflags hidden ~/Library/ è°ƒèŠ‚éŸ³é‡çš„åŒæ—¶æŒ‰ä½ Option + Shifté”® æ˜¾ç¤ºâ€œéšè—æ–‡ä»¶â€ Command + Shift + . defaults write com.apple.finder AppleShowAllFiles -bool true;killall Finder éšè— defaults write com.apple.finder AppleShowAllFiles -bool true;killall Finder å…³é—­å¼€æœºå£°éŸ³ sudo nvram SystemAudioVolume=%80ï¼Œ çœç•¥å· 1ã€ä¾æ¬¡æŒ‰ âŒƒ âŒ˜ ç©ºæ ¼ 2ã€â‡§ æ•°å­—6 Option ç‚¹å‡» Dock å›¾æ ‡ï¼ŒæŒ‰ä½ Option ç‚¹å‡» Dock ä¸­çš„å›¾æ ‡ï¼Œåˆ™ä¼šåœ¨æ¡Œé¢æ˜¾ç¤ºè¯¥åº”ç”¨æ‰€æœ‰çª—å£ Option + å·¦ï¼šå‘å·¦ç§»åŠ¨ä¸€ä¸ªå•è¯ Option + å³ï¼šå‘å³ç§»åŠ¨ä¸€ä¸ªå•è¯ Option + Deleteï¼šåˆ é™¤å·¦è¾¹ä¸€ä¸ªå•è¯ Option + Fn + Deleteï¼šåˆ é™¤å³è¾¹ä¸€ä¸ªå•è¯ è®¾ç½® dock æ˜¾ç¤ºæ—¶é—´å‘½ä»¤ æ‰“å¼€ç»ˆç«¯è¾“å…¥ä»¥ä¸‹å‘½ä»¤ #å…ˆä¿®æ”¹åœç•™æ—¶é—´ï¼ˆåé¢æ•°å­—ä¸ºåœç•™æ—¶é—´ï¼‰å¦‚ï¼š defaults write com.apple.dock autohide-delay -int 0 ##ï¼ˆæ—¶é—´è®¾ä¸ºæœ€çŸ­ï¼‰ defaults write com.apple.dock autohide-delay -int 0.5 ##ï¼ˆæ—¶é—´è®¾ä¸º 0.5sï¼‰ defaults write com.apple.dock autohide-delay -int 10 ##ï¼ˆæ—¶é—´è®¾ä¸º 10sï¼‰ #ä½¿è®¾ç½®ç”Ÿæ•ˆ killall Dock â€‹ |æ¨è|å†æ¨èä¸ªäºº æ± å¥å¼º ã€Šäººç”Ÿå…ƒç¼–ç¨‹ã€‹ä½œè€… ä»–çš„åšå®¢å’Œå¾®ä¿¡ä¸Šæœ‰å¾ˆå¤šå¹²è´§ â€‹â€‹","categories":[{"name":"å¿«æ·é”®","slug":"å¿«æ·é”®","permalink":"http://gangtieguo.cn/categories/å¿«æ·é”®/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://gangtieguo.cn/tags/Mac/"},{"name":"å¿«æ·é”®","slug":"å¿«æ·é”®","permalink":"http://gangtieguo.cn/tags/å¿«æ·é”®/"},{"name":"Idea","slug":"Idea","permalink":"http://gangtieguo.cn/tags/Idea/"},{"name":"Finder","slug":"Finder","permalink":"http://gangtieguo.cn/tags/Finder/"}]},{"title":"Centos7ä¸Šæ­å»ºJenkins","slug":"Centos7ä¸Šæ­å»ºJenkins","date":"2018-06-21T14:11:28.272Z","updated":"2019-06-17T03:26:31.988Z","comments":true,"path":"2018/06/21/Centos7ä¸Šæ­å»ºJenkins/","link":"","permalink":"http://gangtieguo.cn/2018/06/21/Centos7ä¸Šæ­å»ºJenkins/","excerpt":"ä¹‹å‰ç”¨yumæ¨¡å¼å®‰è£…ï¼Œæ€»æ˜¯å¯åŠ¨æŠ¥é”™ï¼Œè§£å†³äº†ä¸€ç•ªï¼Œæœªæ‰¾åˆ°è§£å†³æ–¹æ¡ˆï¼Œåç›´æ¥ä¸‹è½½waråŒ…è¿›è¡Œå®‰è£…éƒ¨ç½² é»˜è®¤å®‰è£…äº†Java","text":"ä¹‹å‰ç”¨yumæ¨¡å¼å®‰è£…ï¼Œæ€»æ˜¯å¯åŠ¨æŠ¥é”™ï¼Œè§£å†³äº†ä¸€ç•ªï¼Œæœªæ‰¾åˆ°è§£å†³æ–¹æ¡ˆï¼Œåç›´æ¥ä¸‹è½½waråŒ…è¿›è¡Œå®‰è£…éƒ¨ç½² é»˜è®¤å®‰è£…äº†Java 1. å®‰è£… jenkins 123456cd /optmkdir /jenkinscd jenkinsmkdir jenkins_homemkdir jenkins_nodewget http://mirrors.jenkins-ci.org/war/latest/jenkins.war 2. ç¼–å†™å¯æ‰§è¡Œæ–‡ä»¶ vim start_jenkins.sh1234#!/bin/bashJENKINS_ROOT=/opt/jenkinsexport JENKINS_HOME=$JENKINS_ROOT/jenkins_homejava -jar $JENKINS_ROOT/jenkins.war --httpPort=8000 ä¿®æ”¹æ–‡ä»¶çš„æƒé™ï¼š chmod a+x start_jenkins.sh å¯åŠ¨ jenkins: nohup ./start_jenkins.sh &gt; jenkins.log 2&gt;&amp; 1&amp; 3 è®¿é—® jenkins è¾“å…¥ http:// æœåŠ¡å™¨åœ°å€: 8000 æ³¨æ„ï¼šåœ¨å¯åŠ¨æ—¥å¿—ä¸­ä¼šå‡ºç°åˆå§‹å¯†ç ï¼Œè¿™ä¸ªç”¨æ¥é¦–æ¬¡ç™»é™†Jenkinsä½¿ç”¨ å‚è€ƒåœ¨ Centos7 ä¸Šæ­å»º jenkins","categories":[{"name":"å®‰è£…éƒ¨ç½²","slug":"å®‰è£…éƒ¨ç½²","permalink":"http://gangtieguo.cn/categories/å®‰è£…éƒ¨ç½²/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://gangtieguo.cn/tags/Jenkins/"}]},{"title":"Docker-Hadoopé›†ç¾¤ã€å¼•ç”¨ã€‘","slug":"Docker-å®‰è£…Hadoopé›†ç¾¤ã€å¼•ç”¨ã€‘","date":"2018-06-03T06:36:21.404Z","updated":"2019-03-06T03:06:19.369Z","comments":true,"path":"2018/06/03/Docker-å®‰è£…Hadoopé›†ç¾¤ã€å¼•ç”¨ã€‘/","link":"","permalink":"http://gangtieguo.cn/2018/06/03/Docker-å®‰è£…Hadoopé›†ç¾¤ã€å¼•ç”¨ã€‘/","excerpt":"Dockeré…ç½®Hadoopé›†ç¾¤ç¯å¢ƒ åœ¨ç½‘ä¸Šæ‰¾åˆ°ä¸€ä¸ªç½‘å‹è‡ªåˆ¶çš„é•œåƒï¼Œæ‹‰å–é…ç½®éƒ½æ˜¯å‚è€ƒçš„ï¼Œè®°å½•ä¸€ä¸‹ã€‚","text":"Dockeré…ç½®Hadoopé›†ç¾¤ç¯å¢ƒ åœ¨ç½‘ä¸Šæ‰¾åˆ°ä¸€ä¸ªç½‘å‹è‡ªåˆ¶çš„é•œåƒï¼Œæ‹‰å–é…ç½®éƒ½æ˜¯å‚è€ƒçš„ï¼Œè®°å½•ä¸€ä¸‹ã€‚ æ‹‰å–é•œåƒ sudo docker pull kiwenlau/hadoop-master:0.1.0sudo docker pull kiwenlau/hadoop-slave:0.1.0sudo docker pull kiwenlau/hadoop-base:0.1.0sudo docker pull kiwenlau/serf-dnsmasq:0.1.0 æŸ¥çœ‹ä¸‹è½½çš„é•œåƒ sudo docker images åœ¨githubä¸­æ‹‰å–æºä»£ç (æˆ–è€…åœ¨oschinaä¸­æ‹‰å–)git clone https://github.com/kiwenlau/hadoop-cluster-dockerå¼€æºä¸­å›½git clone http://git.oschina.net/kiwenlau/hadoop-cluster-docker è¿è¡Œå®¹å™¨æ‹‰å–é•œåƒåï¼Œæ‰“å¼€æºä»£ç æ–‡ä»¶å¤¹ï¼Œå¹¶ä¸”è¿è¡Œè„šæœ¬ cd hadoop-cluster-docker æ³¨æ„ï¼šè¿è¡Œè„šæœ¬æ—¶,éœ€è¦å…ˆå¯åŠ¨dockeræœåŠ¡ ./start-container.sh ä¸€å…±å¼€å¯äº† 3 ä¸ªå®¹å™¨ï¼Œ1 ä¸ª master, 2 ä¸ª slaveã€‚å¼€å¯å®¹å™¨åå°±è¿›å…¥äº† master å®¹å™¨ root ç”¨æˆ·çš„æ ¹ç›®å½•ï¼ˆ/rootï¼‰ æŸ¥çœ‹rootç›®å½•ä¸‹æ–‡ä»¶ æµ‹è¯•å®¹å™¨æ˜¯å¦æ­£å¸¸è¿è¡Œserf members å‚è€ƒï¼šåŸºäº Docker å¿«é€Ÿæ­å»ºå¤šèŠ‚ç‚¹ Hadoop é›†ç¾¤","categories":[{"name":"ç¯å¢ƒé…ç½®","slug":"ç¯å¢ƒé…ç½®","permalink":"http://gangtieguo.cn/categories/ç¯å¢ƒé…ç½®/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://gangtieguo.cn/tags/Docker/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://gangtieguo.cn/tags/Hadoop/"}]},{"title":"å‘½ä»¤ç§¯ç´¯","slug":"å¤§æ•°æ®å‘½ä»¤ç§¯ç´¯","date":"2018-05-31T06:10:40.710Z","updated":"2018-09-05T14:30:30.285Z","comments":true,"path":"2018/05/31/å¤§æ•°æ®å‘½ä»¤ç§¯ç´¯/","link":"","permalink":"http://gangtieguo.cn/2018/05/31/å¤§æ•°æ®å‘½ä»¤ç§¯ç´¯/","excerpt":"$HADOOP_HOME/bin/hdfs oev -i edits -o edits.xml æŸ¥çœ‹å…ƒæ•°æ®","text":"$HADOOP_HOME/bin/hdfs oev -i edits -o edits.xml æŸ¥çœ‹å…ƒæ•°æ®","categories":[{"name":"ç¢ç‰‡çŸ¥è¯†","slug":"ç¢ç‰‡çŸ¥è¯†","permalink":"http://gangtieguo.cn/categories/ç¢ç‰‡çŸ¥è¯†/"}],"tags":[{"name":"å‘½ä»¤","slug":"å‘½ä»¤","permalink":"http://gangtieguo.cn/tags/å‘½ä»¤/"},{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://gangtieguo.cn/tags/å¤§æ•°æ®/"}]},{"title":"jsontoolä½¿ç”¨","slug":"json-toolä½¿ç”¨","date":"2018-05-30T01:36:02.864Z","updated":"2018-08-10T17:46:56.174Z","comments":true,"path":"2018/05/30/json-toolä½¿ç”¨/","link":"","permalink":"http://gangtieguo.cn/2018/05/30/json-toolä½¿ç”¨/","excerpt":"","text":"json-toolä½¿ç”¨ï¼šjava -jar json-tool.jar &quot;jsonæ–‡ä»¶ç›®å½•&quot; &quot;jsonPathè·¯å¾„&quot;ç¤ºä¾‹ï¼š 1java -jar /Users/yaosong/Documents/json-tool.jar &quot;/Users/yaosong/tmp/access_report_data_by_token.json&quot; &quot;$.report_data.behavior_check[?(@.check_point_cn == &apos;æœ‹å‹åœˆåœ¨å“ªé‡Œ&apos;)].evidence&quot;","categories":[{"name":"tool","slug":"tool","permalink":"http://gangtieguo.cn/categories/tool/"}],"tags":[{"name":"å‘½ä»¤","slug":"å‘½ä»¤","permalink":"http://gangtieguo.cn/tags/å‘½ä»¤/"}]},{"title":"gitå‘½ä»¤æ€»ç»“","slug":"gitå‘½ä»¤æ€»ç»“","date":"2018-05-21T17:43:07.025Z","updated":"2019-06-17T04:40:09.376Z","comments":true,"path":"2018/05/22/gitå‘½ä»¤æ€»ç»“/","link":"","permalink":"http://gangtieguo.cn/2018/05/22/gitå‘½ä»¤æ€»ç»“/","excerpt":"æäº¤ git add .git commit -m â€œ â€œgit push origin mastergit push origin master -f æ‹‰å–git pull &lt;è¿œç¨‹ä¸»æœºå&gt; &lt;è¿œç¨‹åˆ†æ”¯å&gt;:&lt;æœ¬åœ°åˆ†æ”¯å&gt;å¦‚æ‹‰å–è¿œç¨‹çš„ master åˆ†æ”¯åˆ°æœ¬åœ° wy åˆ†æ”¯ï¼šgit pull origin master:wy åˆ†æ”¯åˆ‡æ¢","text":"æäº¤ git add .git commit -m â€œ â€œgit push origin mastergit push origin master -f æ‹‰å–git pull &lt;è¿œç¨‹ä¸»æœºå&gt; &lt;è¿œç¨‹åˆ†æ”¯å&gt;:&lt;æœ¬åœ°åˆ†æ”¯å&gt;å¦‚æ‹‰å–è¿œç¨‹çš„ master åˆ†æ”¯åˆ°æœ¬åœ° wy åˆ†æ”¯ï¼šgit pull origin master:wy åˆ†æ”¯åˆ‡æ¢ æŸ¥çœ‹åˆ†æ”¯ï¼šgit branchåˆ›å»ºåˆ†æ”¯ï¼šgit branch åˆ‡æ¢åˆ†æ”¯ï¼šgit checkout åˆ›å»º + åˆ‡æ¢åˆ†æ”¯ï¼šgit checkout -b åˆå¹¶æŸåˆ†æ”¯åˆ°å½“å‰åˆ†æ”¯ï¼šgit merge åˆ é™¤åˆ†æ”¯ï¼šgit branch -d","categories":[{"name":"å·¥ç¨‹æ¡†æ¶","slug":"å·¥ç¨‹æ¡†æ¶","permalink":"http://gangtieguo.cn/categories/å·¥ç¨‹æ¡†æ¶/"}],"tags":[{"name":"git","slug":"git","permalink":"http://gangtieguo.cn/tags/git/"}]},{"title":"éƒ¨ç½²åšå®¢åˆ°äº‘æœåŠ¡å™¨","slug":"è½¬ç§»Githubåšå®¢åˆ°äº‘æœåŠ¡å™¨","date":"2018-05-20T16:56:17.069Z","updated":"2019-06-17T04:40:09.405Z","comments":true,"path":"2018/05/21/è½¬ç§»Githubåšå®¢åˆ°äº‘æœåŠ¡å™¨/","link":"","permalink":"http://gangtieguo.cn/2018/05/21/è½¬ç§»Githubåšå®¢åˆ°äº‘æœåŠ¡å™¨/","excerpt":"ç®€å•è®°å½•è½¬ç§»åˆ°åšå®¢åˆ°äº‘æœåŠ¡å™¨","text":"ç®€å•è®°å½•è½¬ç§»åˆ°åšå®¢åˆ°äº‘æœåŠ¡å™¨ åŸç†åŠå‡†å¤‡ æˆ‘ä»¬åœ¨è‡ªå·±çš„ç”µè„‘ä¸Šå†™å¥½åšå®¢, ä½¿ç”¨ git å‘å¸ƒåˆ°ä»£ç ä»“åº“è¿›è¡Œå¤‡ä»½, git ä»“åº“æ¥æ”¶åˆ° push è¯·æ±‚å, ä½¿ç”¨ webhook é…åˆ nodejs è‡ªåŠ¨è¿›è¡ŒæœåŠ¡å™¨ç«¯é¡µé¢çš„æ›´æ–°. å‡†å¤‡å®‰è£…Gitå’ŒNodeJS (CentOS ç¯å¢ƒ) 1yum install git å®‰è£…NodeJS 1curl --silent --location https://rpm.nodesource.com/setup_5.x | bash - æœåŠ¡å™¨æ„å»ºwebhookæ–¹å¼æœåŠ¡å™¨ç«¯çš„â€ é’©å­â€æˆ‘ä»¬å€ŸåŠ©ä¸€ä¸ª node æ’ä»¶ github-webhook-handler æ¥å¿«é€Ÿå®Œæˆé…åˆ github webhook çš„æ“ä½œ, å…¶ä»– git å¹³å°ä¹Ÿæœ‰ç›¸åº”çš„æ’ä»¶, å¦‚é…åˆ coding çš„ coding-webhook-handler. ç›‘å¬è„šæœ¬ æˆ‘ä»¬å€ŸåŠ©ä¸€ä¸ª node æ’ä»¶ github webhook-handleræ¥å¿«é€Ÿå®Œæˆé…åˆ github webhook çš„æ“ä½œ, å…¶ä»– git å¹³å°ä¹Ÿæœ‰ç›¸åº”çš„æ’ä»¶, å¦‚é…åˆ coding çš„ coding-webhook-handler. ä½¿ç”¨ npm install -g github-webhook-handler å‘½ä»¤æ¥å®‰è£…åˆ°æœåŠ¡å™¨ç«¯.condingåˆ™ä¸ºnpm install -g coding-webhook-handler åˆ‡æ¢åˆ°æœåŠ¡å™¨ç«™ç‚¹ç›®å½•ï¼Œå¦‚æˆ‘çš„æ˜¯ /root/blog,æ–°å»ºä¸€ä¸ªpublicç›®å½•ï¼Œå°†ä½ çš„githubä»“åº“ä¸­çš„masteråˆ†æ”¯pullåˆ°è¯¥ç›®å½•ä¸­ï¼Œè¿™ä¸ªç›®å½•ä½œä¸ºè¿™ä¸ªåšå®¢çš„æ ¹ç›®å½•äº† 123456cd /root/blogmkdir public cd public git initgit remote add origin https://github.com/yaosong5/yaosong5.github.iogit pull origin master ç„¶åæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªwebhooks.jsæ–‡ä»¶ï¼Œå°†ä»¥ä¸‹çš„å†…å®¹ç²˜è´´ï¼Œè¿™ç›¸å½“äºNode.js æœåŠ¡å™¨çš„ä»£ç æ„å»º 123456789101112131415161718192021222324252627282930var http = require('http')var createHandler = require('github-webhook-handler')var handler = createHandler(&#123; path: '/', secret: 'yao' &#125;)function run_cmd(cmd, args, callback) &#123; var spawn = require('child_process').spawn; var child = spawn(cmd, args); var resp = \"\"; child.stdout.on('data', function(buffer) &#123; resp += buffer.toString(); &#125;); child.stdout.on('end', function() &#123; callback (resp) &#125;);&#125;http.createServer(function (req, res) &#123; handler(req, res, function (err) &#123; res.statusCode = 404 res.end('no such location') &#125;)&#125;).listen(7777)handler.on('error', function (err) &#123; console.error('Error:', err.message)&#125;)handler.on('push', function (event) &#123; console.log('Received a push event for %s to %s', event.payload.repository.name, event.payload.ref); run_cmd('sh', ['./deploy.sh',event.payload.repository.name], function(text)&#123; console.log(text) &#125;);&#125;) æ³¨æ„ä¸Šæ®µä»£ç ä¸­ç¬¬ 3 è¡Œ { path: â€˜/â€˜, secret: â€˜æ”¹ä¸ºä½ çš„secretâ€™ } ä¸­ secret å¯ä»¥æ”¹ä¸ºä½ å–œæ¬¢çš„å£ä»¤, è¿™å£ä»¤å°†åœ¨ä¸‹é¢çš„æ­¥éª¤ä¸­èµ·åˆ°ä½œç”¨ ,é…ç½®github webhooksçš„æ—¶å€™å¡«å…¥çš„å£ä»¤, è¯·ç•™æ„. ç¬¬ 19 è¡Œ listen(7777) ä¸­ 7777 ä¸ºç›‘å¬ç¨‹åºéœ€è¦ä½¿ç”¨çš„ç«¯å£. æ‰§è¡Œè„šæœ¬ä¸Šé¢çš„ javascript ä»£ç æ˜¯ç”¨æ¥æ•æ‰ github å‘æ¥çš„ä¿¡å·å¹¶å‘èµ·ä¸€ä¸ªæ‰§è¡Œ ./deploy.sh çš„è„šæœ¬, æ¥ä¸‹æ¥æˆ‘ä»¬è¿˜éœ€è¦å†™ deploy.sh çš„å†…å®¹. 123456789101112#!/bin/bashWEB_PATH='/root/blog/public'echo \"Start deployment\"cd $WEB_PATHecho \"pulling source code...\"git reset --hard origin/mastergit clean -fgit pullgit checkout masterecho \"Finished.\" å°†ä»¥ä¸Šä»£ç çš„ç¬¬ 3 è¡Œæ”¹ä¸ºä½ æœåŠ¡å™¨ä¸­çš„å®é™…ç›®å½•. æ¥ä¸‹æ¥åªéœ€è¦å¼€å¯ç›‘å¬å°±å¯ä»¥äº†. tips: åœ¨æ­¤ä¹‹å‰ä½ å¯ä»¥ä½¿ç”¨ node webhook.js æ¥æµ‹è¯•ä¸€ä¸‹ç›‘å¬ç¨‹åºæ˜¯å¦èƒ½å¤Ÿæ­£å¸¸è¿è¡Œ.æˆ‘åœ¨è¿™é‡Œç¢°åˆ°äº†ä¸€ä¸ª node ç¯å¢ƒå˜é‡çš„é—®é¢˜, è¯»å–ä¸åˆ° github-webhook-handler è¿™ä¸ªæ¨¡å—, æ‰¾äº†å¾ˆå¤šåŠæ³•ä¹Ÿæ²¡æœ‰è§£å†³, åæ¥æˆ‘ç›´æ¥åœ¨é¡¹ç›®æ ¹ç›®å½•çš„ä¸Šçº§ç›®å½•å®‰è£…äº†è¿™ä¸ªæ¨¡å—, é—®é¢˜å°±è§£å†³äº†. cd /root/blognpm install github-webhook-handlernpm ä¼šä»å½“å‰ç›®å½•ä¾æ¬¡å‘ä¸Šå¯»æ‰¾å«æœ‰ node_modules ç›®å½•å¹¶è®¿é—®è¯¥æ¨¡å—. æ™®é€šæ–¹å¼è¿è¡Œ webhook.jsåˆ©ç”¨ Linux æä¾›çš„ nohup å‘½ä»¤ï¼Œè®© webhooks.js è¿è¡Œåœ¨åå° 1nohup node webhook.js &gt; deploy.log &amp; Foreveræ–¹å¼è¿è¡Œwebhook.js æˆ‘åœ¨å®é™…ä½¿ç”¨çš„æ—¶å€™å‘ç°ï¼Œæˆ‘çš„ Node æœåŠ¡å™¨æ—¶ä¸æ—¶ä¼šè‡ªåŠ¨åœæ‰ï¼Œå…·ä½“åŸå› æˆ‘æš‚æ—¶è¿˜æ²¡æœ‰å¼„æ¸…æ¥šã€‚ä¸è¿‡ä¼¼ä¹å¾ˆå¤šäººéƒ½é‡åˆ°äº†è¿™æ ·çš„å›°æ‰°ï¼Œè¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œforever æ˜¯ä¸ªä¸é”™çš„é€‰æ‹©ã€‚å€ŸåŠ© forever è¿™ä¸ªåº“ï¼Œå®ƒå¯ä»¥ä¿è¯ Node æŒç»­è¿è¡Œä¸‹å»ï¼Œä¸€æ—¦æœåŠ¡å™¨æŒ‚äº†ï¼Œå®ƒéƒ½ä¼šé‡å¯æœåŠ¡å™¨ã€‚ å®‰è£… foreverï¼šnpm install -g foreverè¿è¡Œï¼š12cd &#123; éƒ¨ç½²æœåŠ¡å™¨çš„æ ¹ç›®å½• &#125; nohup forever start webhook.js &gt; deploy.log &amp; Ubuntu ä¸­åŸæœ¬å°±æœ‰ä¸€ä¸ªå« node çš„åŒ…ã€‚ä¸ºäº†é¿å…å†²çªï¼Œåœ¨ Ubuntu ä¸Šå®‰è£…æˆ–ä½¿ç”¨ Node å¾—ç”¨ nodejs è¿™ä¸ªåå­—ã€‚è€Œ forever é»˜è®¤æ˜¯ä½¿ç”¨ node ä½œä¸ºæ‰§è¡Œè„šæœ¬çš„ç¨‹åºåã€‚æ‰€ä»¥ä¸ºäº†å¤„ç† Ubuntu å­˜åœ¨çš„è¿™ç§ç‰¹æ®Šæƒ…å†µï¼Œåœ¨å¯åŠ¨ forever æ—¶å¾—å¦å¤–æ·»åŠ ä¸€ä¸ªå‚æ•°ï¼š(å…¶å®ƒåˆ™å¿½ç•¥) forever start webhook.js -c nodejs Githubé…ç½®webhooksé…ç½®å¥½ Webhook åï¼ŒGithub ä¼šå‘é€ä¸€ä¸ª ping æ¥æµ‹è¯•è¿™ä¸ªåœ°å€ã€‚å¦‚æœæˆåŠŸäº†ï¼Œé‚£ä¹ˆè¿™ä¸ª Webhook å‰å°±ä¼šåŠ ä¸Šä¸€ä¸ªç»¿è‰²çš„å‹¾ï¼›å¦‚æœä½ å¾—åˆ°çš„æ˜¯ä¸€ä¸ªçº¢è‰²çš„å‰ï¼Œé‚£å°±å¥½å¥½æ£€æŸ¥ä¸€ä¸‹å“ªå„¿å‡ºé—®é¢˜äº†å§ï¼ git-hookæ–¹å¼å¯é‡‡ç”¨ä¸€ç§æ›´ä¸ºç®€å•çš„éƒ¨ç½²æ–¹å¼ è¿™ç§æ–¹å¼å’Œwebhookå¯äºŒé€‰ä¸€ æœåŠ¡å™¨ä¸Šå»ºç«‹gitè£¸åº“åˆ›å»ºä¸€ä¸ªè£¸ä»“åº“ï¼Œè£¸ä»“åº“å°±æ˜¯åªä¿å­˜gitä¿¡æ¯çš„Repository, é¦–å…ˆåˆ‡æ¢åˆ°gitç”¨æˆ·ç¡®ä¿gitç”¨æˆ·æ‹¥æœ‰ä»“åº“æ‰€æœ‰æƒä¸€å®šè¦åŠ  â€“bareï¼Œè¿™æ ·æ‰æ˜¯ä¸€ä¸ªè£¸åº“ã€‚ 12cd git init --bare blog.git ä½¿ç”¨ git-hooks åŒæ­¥ç½‘ç«™æ ¹ç›®å½•åœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ post-receiveè¿™ä¸ªé’©å­ï¼Œå½“gitæœ‰æ”¶å‘çš„æ—¶å€™å°±ä¼šè°ƒç”¨è¿™ä¸ªé’©å­ã€‚ åœ¨ ~/blog.git è£¸åº“çš„ hooksæ–‡ä»¶å¤¹ä¸­ï¼Œæ–°å»ºpost-receiveæ–‡ä»¶ã€‚ vim ~/blog.git/hooks/post-receive å¡«å…¥ä»¥ä¸‹å†…å®¹ 12#!/bin/shgit --work-tree=/root/blog/public --git-dir=/root/blog.git checkout -f work-tree=/root/blog/publicè¿™ä¸ªç›®å½•æ˜¯ç½‘ç«™çš„ç½‘é¡µæ–‡ä»¶ç›®å½•ï¼Œâ€“git-dir=/root/blog.gitç›®å½•ä¸ºè£¸åº“åœ°å€ï¼Œè£¸åº“ç›‘å¬gitæäº¤ä¼šå°†æ–‡ä»¶æäº¤åˆ°ç½‘é¡µç›®å½•ä¿å­˜åï¼Œè¦èµ‹äºˆè¿™ä¸ªæ–‡ä»¶å¯æ‰§è¡Œæƒé™chmod +x post-receive é…ç½®åšå®¢æ ¹ç›®å½•_config.ymlå®Œæˆè‡ªåŠ¨åŒ–éƒ¨ç½²æ‰“å¼€ _config.yml, æ‰¾åˆ° deploy12345deploy: type: git repo: ç”¨æˆ·å@SERVERå:/home/git/blog.gitï¼ˆè£¸åº“åœ°å€ï¼‰ //&lt;repository url&gt; branch: master //è¿™é‡Œå¡«å†™åˆ†æ”¯ [branch] message: æäº¤çš„ä¿¡æ¯ //è‡ªå®šä¹‰æäº¤ä¿¡æ¯ (é»˜è®¤ä¸º Site updated: &#123;&#123; now(&apos;YYYY-MM-DD HH:mm:ss&apos;) &#125;&#125;) NginxæœåŠ¡npm å®‰è£…nginxå¯åŠ¨nginx 1service nginx start nginx -t æŸ¥çœ‹nginxé…ç½®æ–‡ä»¶è‹¥nginxæœåŠ¡å¯åŠ¨ï¼Œè®¿é—®æŠ¥403é”™è¯¯ åˆ™å°†é¦–è¡Œ user nginx æ”¹ä¸ºuser root 123456789vim /etc/nginx/nginx.confserver &#123; listen 80; # ç›‘å¬ç«¯å£ server_name 47.98.141.252:80 gangtieguo.cn wwww.gangtieguo.cn; # ä½ çš„åŸŸå location / &#123; root /root/blog/public; index index.html; &#125;&#125; é‡è½½ nginxï¼Œä½¿é…ç½®ç”Ÿæ•ˆ nginx -s reload å‚è€ƒHexo é™æ€åšå®¢æ­å»ºå¹¶å®ç°è‡ªåŠ¨éƒ¨ç½²åˆ°è¿œç¨‹ vpså°† Hexo åšå®¢å‘å¸ƒåˆ°è‡ªå·±çš„æœåŠ¡å™¨ä¸Šåˆ©ç”¨ Github çš„ Webhook åŠŸèƒ½å’Œ Node.js å®Œæˆé¡¹ç›®çš„è‡ªåŠ¨éƒ¨ç½²Webhook å®è·µ â€”â€” è‡ªåŠ¨éƒ¨ç½²Hexo å¿«é€Ÿæ­å»ºé™æ€åšå®¢å¹¶å®ç°è¿œç¨‹ VPS è‡ªåŠ¨éƒ¨ç½²é˜¿é‡Œäº‘ VPS æ­å»ºè‡ªå·±çš„çš„ Hexo åšå®¢","categories":[{"name":"åšå®¢","slug":"åšå®¢","permalink":"http://gangtieguo.cn/categories/åšå®¢/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://gangtieguo.cn/tags/Hexo/"}]},{"title":"MyBatisç›¸å…³æ³¨è§£","slug":"MyBatisæ³¨è§£","date":"2018-05-17T12:09:55.000Z","updated":"2019-06-17T04:40:09.394Z","comments":true,"path":"2018/05/17/MyBatisæ³¨è§£/","link":"","permalink":"http://gangtieguo.cn/2018/05/17/MyBatisæ³¨è§£/","excerpt":"ç°æ¥è§¦MyBaticè®°å½•ä¸€äº›æ³¨è§£","text":"ç°æ¥è§¦MyBaticè®°å½•ä¸€äº›æ³¨è§£è‡ªåŠ¨ç”Ÿæˆä¸»é”® å¯ä»¥ä½¿ç”¨ @Options æ³¨è§£çš„ userGeneratedKeys å’Œ keyProperty å±æ€§è®©æ•°æ®åº“äº§ç”Ÿ auto_incrementï¼ˆè‡ªå¢é•¿ï¼‰åˆ—çš„å€¼ï¼Œç„¶åå°†ç”Ÿæˆçš„å€¼è®¾ç½®åˆ°è¾“å…¥å‚æ•°å¯¹è±¡çš„å±æ€§ä¸­ã€‚ 123@Insert(\"insert into students(name,sex,age) values(#&#123;name&#125;,#&#123;sex&#125;,#&#123;age&#125;\") @Options(useGeneratedKeys = true, keyProperty =\"userId\") int insertUser(User user); å°†è‡ªå¢çš„Idå­˜å…¥åˆ°userIdå±æ€§ä¸­","categories":[{"name":"æ¡†æ¶","slug":"æ¡†æ¶","permalink":"http://gangtieguo.cn/categories/æ¡†æ¶/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://gangtieguo.cn/tags/Java/"},{"name":"SSH","slug":"SSH","permalink":"http://gangtieguo.cn/tags/SSH/"}]},{"title":"linuxå‘½ä»¤","slug":"Linuxå‘½ä»¤ç§¯ç´¯","date":"2018-05-10T07:37:28.983Z","updated":"2019-04-09T09:02:15.234Z","comments":true,"path":"2018/05/10/Linuxå‘½ä»¤ç§¯ç´¯/","link":"","permalink":"http://gangtieguo.cn/2018/05/10/Linuxå‘½ä»¤ç§¯ç´¯/","excerpt":"ç®€å•linuxå‘½ä»¤ nohup &amp;åå°è¿è¡Œ","text":"ç®€å•linuxå‘½ä»¤ nohup &amp;åå°è¿è¡Œ æ–‡ä»¶æŸ¥æ‰¾find / -type f -size +10Gåœ¨Linuxä¸‹å¦‚ä½•è®©æ–‡ä»¶è®©æŒ‰å¤§å°å•ä½ä¸ºM,Gç­‰æ˜“è¯»æ ¼å¼ï¼ŒS sizeå¤§å°æ’åºã€‚ ls -lhSdu -h * | sort -nå½“ç„¶æ‚¨ä¹Ÿå¯ä»¥ç»“åˆç®¡é“æ–‡ä»¶å¤¹å†…æœ€å¤§çš„å‡ ä¸ªæ–‡ä»¶ du -h * | sort -n|headåŠ¨æ€æ˜¾ç¤ºæœºå™¨å„ç«¯å£çš„é“¾æ¥æƒ…å†µwhile :; do netstat -apn | grep &quot;:80&quot; | wc -l; sleep 1; done sedæ›´æ”¹ç¬¬ä¸€è¡Œ sed -i &#39;1s/.*//&#39; sed -i â€˜1s/.*/æƒ³æ›´æ”¹çš„å†…å®¹/â€˜ 1ssh root@slave01 \"sed -i '6c advertised.host.name=slave01 ' $KAFKA_HOME/config/server.properties\" åˆ é™¤ç¬¬ä¸€è¡Œsed -i &#39;1d&#39; sed -i â€˜1dâ€™ æ–‡ä»¶åæ’å…¥ç¬¬ä¸€è¡Œ sed -i &#39;1i\\&#39; sed -i â€˜1i\\å†…å®¹â€˜ æ–‡ä»¶å cpucat /proc/cpuinfo | grep processor | wc -llscpu sz rzä¸æœåŠ¡å™¨äº¤äº’ä¸Šä¼ ä¸‹è½½æ–‡ä»¶sudo yum install lrzsz -y æŒ‚è½½ sshfs root@192.168.73.12:/home/ /csdn/win10/ å³ï¼šsshfs ç”¨æˆ·å@è¿œç¨‹ä¸»æœºIP:è¿œç¨‹ä¸»æœºè·¯å¾„ æœ¬åœ°æŒ‚è½½ç‚¹sshfs root@master:/usr/hadoop /usr/hive/hadoop æŸ¥çœ‹ç«¯å£æ˜¯å¦è¢«ç›‘å¬ä¹Ÿå¯éªŒè¯å¯¹åº”ç«¯å£ç¨‹åºæ˜¯å¦å¯åŠ¨ 12netstat -nl|grep 10000netstat -antp| grep 1000 tree1yum install -y tree tree å¯ä»¥æŸ¥çœ‹ç›®å½•ç»“æ„ è™šæ‹Ÿæœºå…±äº«æ–‡ä»¶å¤¹åœ¨virtualboxä¸­è®¾ç½®å…±äº«æ–‡ä»¶å¤¹çš„shareåç§°å¯¹åº”macçš„ç›®å½•è™šæ‹Ÿæœºä¸­çš„ç›®å½•sudo mount -t vboxsf vagrant /Users/yaosong centosæŸ¥çœ‹ç‰ˆæœ¬ cat /etc/redhat-release GLIBC2.14 not foundlibc.so.6: version â€˜GLIBC_2.14â€™ not foundæŠ¥é”™æç¤ºçš„è§£å†³æ–¹æ¡ˆ ã€å·¥ä½œã€‘Centos6.5 å‡çº§glibcè§£å†³â€œlibc.so.6: version GLIBC_2.14 not foundâ€æŠ¥é”™é—®é¢˜ å‚è€ƒï¼šDocker-PostgresSQL å…¶ä»–ip -4 addr æŸ¥çœ‹ip chkconfig sshd on rpm -qa|grep sshã€‚ ä¸ºäº†å¿«é€Ÿå®ç°æˆ‘ä»¬å°±ä¸è‡ªå·±è£… SSH æœåŠ¡äº†ï¼Œhub.docker.com ä¸Šçš„ kinogmt/centos-ssh:6.7 è¿™ä¸ªé•œåƒå°±èƒ½æ»¡è¶³æˆ‘ä»¬çš„è¦æ±‚","categories":[{"name":"Linux","slug":"Linux","permalink":"http://gangtieguo.cn/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://gangtieguo.cn/tags/linux/"},{"name":"å¼€å‘","slug":"å¼€å‘","permalink":"http://gangtieguo.cn/tags/å¼€å‘/"}]},{"title":"springé›†æˆæƒé™æ ¡éªŒ","slug":"springé›†æˆæƒé™æ ¡éªŒ","date":"2018-05-08T10:34:55.000Z","updated":"2018-05-17T12:21:22.941Z","comments":true,"path":"2018/05/08/springé›†æˆæƒé™æ ¡éªŒ/","link":"","permalink":"http://gangtieguo.cn/2018/05/08/springé›†æˆæƒé™æ ¡éªŒ/","excerpt":"shiroç®€ä»‹shiroæ˜¯æƒé™æ§åˆ¶çš„ä¸€ä¸ªæ¡†æ¶æ˜¯ä¸€ä¸ªå¼ºå¤§æ˜“ç”¨çš„Javaå®‰å…¨æ¡†æ¶ï¼Œæä¾›äº†è®¤è¯ã€æˆæƒã€åŠ å¯†å’Œä¼šè¯ç®¡ç†åŠŸèƒ½ï¼Œå¯ä¸ºä»»ä½•åº”ç”¨æä¾›å®‰å…¨ä¿éšœ - ä»å‘½ä»¤è¡Œåº”ç”¨ã€ç§»åŠ¨åº”ç”¨åˆ°å¤§å‹ç½‘ç»œåŠä¼ä¸šåº”ç”¨ã€‚","text":"shiroç®€ä»‹shiroæ˜¯æƒé™æ§åˆ¶çš„ä¸€ä¸ªæ¡†æ¶æ˜¯ä¸€ä¸ªå¼ºå¤§æ˜“ç”¨çš„Javaå®‰å…¨æ¡†æ¶ï¼Œæä¾›äº†è®¤è¯ã€æˆæƒã€åŠ å¯†å’Œä¼šè¯ç®¡ç†åŠŸèƒ½ï¼Œå¯ä¸ºä»»ä½•åº”ç”¨æä¾›å®‰å…¨ä¿éšœ - ä»å‘½ä»¤è¡Œåº”ç”¨ã€ç§»åŠ¨åº”ç”¨åˆ°å¤§å‹ç½‘ç»œåŠä¼ä¸šåº”ç”¨ã€‚ æƒé™æ§åˆ¶çš„æ–¹å¼æƒé™æœ‰å››ç§å®ç°æ–¹å¼æ³¨è§£(åŸºäºä»£ç†),urlæ‹¦æˆª(åŸºäºè¿‡æ»¤å™¨),shiroæ ‡ç­¾åº“(åŸºäºæ ‡ç­¾),ç¼–å†™ä»£ç (åŠå…¶ä¸æ¨è)ä¸è®ºå“ªç§æ–¹å¼:éƒ½éœ€è¦å¼•å…¥springç”¨äºæ•´åˆshiroçš„è¿‡æ»¤å™¨ web.xmlä¸­:DelegatingFilterProxy=&gt;springæ•´åˆshiroé…ç½®springæä¾›çš„ç”¨äºæ•´åˆshiroæ¡†æ¶çš„è¿‡æ»¤å™¨123456789101112131415 &lt;filter&gt; &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy &lt;/fileter&gt;``` filet-nameéœ€è¦å’Œ**springé…ç½®æ–‡ä»¶**ä¸­çš„ä¸€ä¸ªBEANå¯¹è±¡çš„idä¿æŒä¸€è‡´**éå¸¸é‡è¦** ### é…ç½® I. æ³¨è§£æ–¹å¼,æ³¨è§£æ˜¯åˆ©ç”¨ç”Ÿæˆçš„ä»£ç†å¯¹è±¡æ¥å®Œæˆæƒé™æ ¡éªŒ: springæ¡†æ¶ä¼šä¸ºå½“å‰actionå¯¹è±¡(åŠ æ³¨è§£çš„action)åˆ›å»ºä¸€ä¸ªä»£ç†å¯¹è±¡,å¦‚æœæœ‰æƒé™,å°±æ‰§è¡Œè¿™ä¸ªæ–¹æ³•,ä¸ç„¶å°±ä¼šæŠ¥**å¼‚å¸¸**(å°†spring,Strusté…ç½®æ–‡ä»¶ä¸°å¯Œ:æ·»åŠ æƒé™çš„æ³¨è§£,strutsæ·»åŠ æ•è·å¼‚å¸¸,è·³è½¬é¡µé¢) 1. éœ€è¦åœ¨springé…ç½®æ–‡ä»¶ä¸­è¿›è¡Œé…ç½®å¼€å¯æ³¨è§£**DefaultAdvisorAutoProxyCreator**, å¹¶é…ç½®æˆcjlibæ–¹å¼çš„æ³¨è§£ ```xml&lt;property name=\"proxyTargetClass\" value=\"true\"&gt;\\&lt;/property&gt; æ³¨è§£å®ç°æƒé™å½“ä¸ºjdkæ¨¡å¼çš„æ—¶å€™æ–¹æ³•æ³¨è§£å®ç°æƒé™è¿‡æ»¤æŠ›å¼‚å¸¸çš„åŸå› :å› ä¸ºå¦‚æœæ˜¯jdkæ–¹å¼çš„è¯,å®ç°çš„æ¥å£modelDrivenåªæœ‰ä¸€ä¸ªgetModelæ–¹æ³•æ‰€ä»¥ä¸èƒ½è¿›è¡Œå¯¹é™¤è¯¥æ–¹æ³•å¤–å…¶ä»–æ–¹æ³•è¿›è¡Œæ³¨è§£ å®šä¹‰åˆ‡é¢ç±»AuthorizationAttributeSourceAdvisor 1&lt;bean id=\"authorizationAttributeSourceAdvisor\" class=\"org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor\"&gt;&lt;/bean&gt; åœ¨éœ€è¦æƒé™æ‰èƒ½è®¿é—®çš„æ–¹æ³•ä¸Šæ·»åŠ æ³¨è§£ 1234567891011121314151617181920212223242526272829 @RequiresPermissions(\"relo_deleteè¿™æ˜¯æƒé™åç§°\") ``` II. urlæ‹¦æˆª(springxml) åŸºäºè¿‡æ»¤å™¨æˆ–è€…æ‹¦æˆªå™¨å®ç° ```xml&lt;bean id=\"shiroFilter\" class=\"org.apache.shiro.spring.web.ShiroFilterFactoryBean\"&gt; &lt;property name=\"securityManager\" ref=\"securityManager\"/&gt; &lt;property name=\"loginUrl\" value=\"/login.jsp\"/&gt; &lt;property name=\"unauthorizedUrl\" value=\"/unauthorized.jsp\"/&gt; &lt;property name=\"filterChainDefinitions\"&gt; &lt;value&gt; /css/** = anon /js/** = anon /images/** = anon /validatecode.jsp* = anon /login.jsp* = anon /userAction_login.action = anon /page_base_staff.action = perms[\"staff\"] /** = authc &lt;!--/** = authc--&gt; &lt;/value&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!--å¼€å¯è‡ªåŠ¨ä»£ç†,å¹¶ä¸”å°†ä»£ç†ä»£ç†æ¨¡å¼è®¾ç½®ä¸ºcjlib--&gt; &lt;bean id=\"defaultAdvisorAutoProxyCreator\" class=\"org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator\"&gt; &lt;!--è®¾ç½®æˆcglibæ–¹å¼--&gt; &lt;property name=\"proxyTargetClass\" value=\"true\"&gt;&lt;/property&gt; &lt;/bean&gt; shiroçš„ä½¿ç”¨ åœ¨web.xmlä¸­å¼•å…¥ç”¨äºåˆ›å»ºshiroæ¡†æ¶çš„è¿‡æ»¤å™¨web.xmlä¸­:DelegatingFilterProxy=&gt;springæ•´åˆshiroæ³¨æ„å¼•å…¥çš„ä½ç½®:è¦åœ¨strutsæ ¸å¿ƒè¿‡æ»¤å™¨çš„å‰é¢,StrutsPrepareAndExcutFilter,ä¸ç„¶,æ‰€æœ‰è¯·æ±‚ä¼šé€šè¿‡strutsè¿‡æ»¤å™¨è·ç›´æ¥è®¿é—®å¾—åˆ°,shiroçš„è¿‡æ»¤å™¨å°†ä¸ä¼šèµ·åˆ°ä½œç”¨ åœ¨Springä¸­æ•´åˆshiro2.1). shiroæ¡†æ¶è¿‡æ»¤å™¨:ShiroFilterFactoryBean éœ€è¦å£°æ˜é‚£äº›è¿‡æ»¤å™¨,é‚£äº›èµ„æºéœ€è¦åŒ¹é…é‚£äº›è¿‡æ»¤å™¨,é‡‡ç”¨urlæ‹¦æˆªæ–¹å¼è¿›è¡Œçš„è·¯å¾„å¯¹åº”çš„æ‹¦æˆªå™¨2.2). é…ç½®å®‰å…¨ç®¡ç†å™¨:DefaultWebSecurityManager éœ€è¦æ³¨å…¥ è‡ªå®šä¹‰çš„Realm beanå¯¹è±¡ 1234567891011121314151617181920212223242526272829303132333435363738 &lt;!--é…ç½®ä¸€ä¸ªshiroæ¡†æ¶çš„è¿‡æ»¤å™¨å·¥å‚bean,ç”¨äºåˆ›å»ºshiroæ¡†æ¶çš„è¿‡æ»¤å™¨--&gt; &lt;bean id=\"shiroFilter\" class=\"org.apache.shiro.spring.web.ShiroFilterFactoryBean\"&gt; &lt;property name=\"securityManager\" ref=\"securityManager\"/&gt; &lt;property name=\"loginUrl\" value=\"/login.jsp\"/&gt; &lt;property name=\"unauthorizedUrl\" value=\"/unauthorized.jsp\"/&gt; &lt;property name=\"filterChainDefinitions\"&gt; &lt;value&gt; /css/** = anon /js/** = anon /images/** = anon /validatecode.jsp* = anon /login.jsp* = anon /userAction_login.action = anon /page_base_staff.action = perms[\"staff\"] /** = authc &lt;!--/** è¡¨ç¤ºæ‰€æœ‰/ä¸‹æ‰€æœ‰è·¯å¾„,åŒ…æ‹¬ä¸‹é¢çš„æ‰€æœ‰è·¯å¾„--&gt; &lt;!--/validatecode.jsp* è¡¨ç¤ºæ‰€æœ‰é™¤äº†validatecode.jsp,è¿˜åŒ…æ‹¬jspåè¿½åŠ å…¶ä»–å†…å®¹çš„.å¦‚validatecode.jsp?'+Math.random();é˜²æ­¢éªŒè¯ç è¯»å–ç¼“å­˜ &lt;/value&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!--å¼€å¯è‡ªåŠ¨ä»£ç†,å¹¶ä¸”å°†ä»£ç†ä»£ç†æ¨¡å¼è®¾ç½®ä¸ºcjlib åŠ¨æ€ä»£ç†åˆ†ä¸ºä¸¤ç±» åŸºäºjdk åˆ›å»ºçš„ç±»å¿…é¡»è¦å®ç°ä¸€ä¸ªæ¥å£,è¿™æ˜¯é¢å‘æ¥å£çš„åŠ¨æ€ä»£ç† åŸºäºcjlib åˆ›å»ºçš„ç±»ä¸èƒ½ç”¨finalä¿®é¥°--&gt; &lt;bean id=\"defaultAdvisorAutoProxyCreator\" class=\"org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator\"&gt; &lt;!--è®¾ç½®æˆcglibæ–¹å¼--&gt; &lt;property name=\"proxyTargetClass\" value=\"true\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--å®šä¹‰aopé€šçŸ¥+åˆ‡å…¥ç‚¹--&gt; &lt;bean id=\"authorizationAttributeSourceAdvisor\" class=\"org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor\"&gt;&lt;/bean&gt; &lt;!--æ³¨å…¥å®‰å…¨ç®¡ç†å™¨--&gt; &lt;bean id=\"securityManager\" class=\"org.apache.shiro.web.mgt.DefaultWebSecurityManager\"&gt; &lt;property name=\"realm\" ref=\"bosRealm\"&gt;&lt;/property&gt; &lt;property name=\"cacheManager\" ref=\"ehCacheManager\"&gt;&lt;/property&gt; &lt;/bean&gt; åœ¨ç™»é™†è®¤è¯çš„æ–¹æ³•ä¸­åŠ å…¥subject controllerä¸­çš„loginæ–¹æ³• 1234567891011121314151617181920212223242526272829303132333435363738394041424344public String login()&#123;Subject subject = SecurityUtils.getSubject(); //åˆ›å»ºä¸€ä¸ªç”¨æˆ·åå¯†ç ä»¤ç‰Œ AuthenticationToken token = new UsernamePasswordToken(getModel().getUsername(), MD5Utils.md5( getModel().getPassword())); try &#123; //è®¤è¯ subject.login(token); &#125; catch (Exception e) &#123; this.addActionError(\"ç”¨æˆ·åæˆ–è€…å¯†ç é”™è¯¯\"); return LOGIN; &#125; /*å½“é€šè¿‡è®¤è¯,è·³å…¥ä¸»é¡µ*/ User user = (User) subject.getPrincipal(); /*å°†ç”¨æˆ·ä¿¡æ¯å­˜å…¥session*/ ServletActionContext.getRequest().getSession().setAttribute(\"currentUser\", user); /*è¿”å›ä¸»é¡µ*/ return \"\";&#125;``` 4. è‡ªå®šä¹‰Realm(ç”¨äºæƒé™çš„å…·ä½“å®æ–½,å³è®¤è¯å’Œæˆæƒ)ä¸€èˆ¬å®ç°Realmæ¥å£çš„ **AuthorizingRealm** å®ä¾‹ 4.1å®ç°è®¤è¯ é‡å†™doGetAuthenticationInfoæ–¹æ³•å¿…é¡»ç»§æ‰¿*AuthorizingRealm* åœ¨éœ€è¦äº¤ä»˜ç»™springç”Ÿæˆ,å¹¶éœ€è¦åœ¨å®‰å…¨æ³¨å†Œç®¡ç†å™¨ä¸­æ³¨å…¥å±æ€§Realm```javaprotected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException &#123;UsernamePasswordToken mytoken = (UsernamePasswordToken) token; String username = mytoken.getUsername(); DetachedCriteria dc = DetachedCriteria.forClass(User.class); dc.add(Restrictions.eq(\"username\",username)); List&lt;User&gt; list = userDao.findByCriteria(dc); if(list != null &amp;&amp; list.size() &gt;0)&#123; User user = list.get(0); String dbPassword = user.getPassword(); AuthenticationInfo info = new SimpleAuthenticationInfo(user,dbPassword,this.getName()); return info; &#125;else&#123; return null; &#125; &#125; 4.2å®ç°æˆæƒ é‡å†™doGetAuthorizationInfoæ–¹æ³• 12345678910111213141516171819202122232425262728293031323334353637383940protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123;/*è·çš„ç®€å•æˆæƒå¯¹è±¡,ç”¨äºæˆæƒçš„*/ SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); /*æˆæƒstaffæƒé™*/ //info.addStringPermission(\"staff\"); //æ­¥éª¤è·å¾—æˆæƒå¯¹è±¡,è·å¾—å½“å‰ç”¨æˆ·,è·å¾—å½“å‰ç”¨æˆ·çš„æƒé™(è‹¥ä¸ºadminå³æˆäºˆæ‰€æœ‰æƒé™),å½“å‰ç”¨æˆ·æˆæƒ //è·å¾—å¯¹è±¡ User user = (User)principals.getPrimaryPrincipal(); List&lt;Function&gt; fList = null; //è·å¾—æƒé™ if(user.getUsername().equals(\"admin\"))&#123; fList = functionDao.findAll(); &#125;else&#123; fList = functionDao.findFunctionByUserId(user.getId()); &#125; //æˆäºˆæƒé™ for(Function f : fList)&#123; info.addStringPermission(f.getCode());&#125;``` ## å…³äºShiroä¸­ä½¿ç”¨ **encache** 1.å¼•å…¥åŒ… `åœ¨springé…ç½®æ–‡ä»¶ä¸­é…ç½®ä»¥ä¸‹` 2.é…ç½®æ–‡ä»¶ehcache.xml ```xml&lt;ehcache xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"../config/ehcache.xsd\"&gt; &lt;defaultCache maxElementsInMemory=\"10000\" eternal=\"false\" timeToIdleSeconds=\"120\" timeToLiveSeconds=\"120\" overflowToDisk=\"true\" maxElementsOnDisk=\"10000000\" diskPersistent=\"false\" diskExpiryThreadIntervalSeconds=\"120\" memoryStoreEvictionPolicy=\"LRU\" /&gt;&lt;/ehcache&gt; &lt;!--eternalæ˜¯å¦æ°¸ä¹…æœ‰æ•ˆ--&gt; 3.å¼•å…¥ç¼“å­˜ç®¡ç†å™¨EhCacheManager(shiroåŒ…ä¸­çš„),å¹¶è®¾ç½®é…ç½®æ–‡ä»¶;4.å°†ç¼“å­˜ç®¡ç†å™¨æ³¨å…¥å®‰å…¨ç®¡ç†å™¨DefaultWebSecurityManager12345678910&lt;!--æ³¨å†Œå®‰å…¨ç®¡ç†å™¨--&gt;&lt;bean id=\"securityManager\" class=\"org.apache.shiro.web.mgt.DefaultWebSecurityManager\"&gt; &lt;property name=\"realm\" ref=\"bosRealm\"&gt;&lt;/property&gt; &lt;property name=\"cacheManager\" ref=\"ehCacheManager\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"bosRealm\" class=\"org.yao.bos.web.action.realm.BOSRealm\"&gt;&lt;/bean&gt; &lt;!--æ³¨å…¥ç¼“å­˜ç®¡ç†å™¨--&gt; &lt;bean id=\"ehCacheManager\" class=\"org.apache.shiro.cache.ehcache.EhCacheManager\"&gt; &lt;property name=\"cacheManagerConfigFile\" value=\"classpath:ehcache.xml\"&gt;&lt;/property&gt; &lt;/bean&gt;","categories":[{"name":"Spring","slug":"Spring","permalink":"http://gangtieguo.cn/categories/Spring/"}],"tags":[{"name":"å¼€å‘","slug":"å¼€å‘","permalink":"http://gangtieguo.cn/tags/å¼€å‘/"},{"name":"Java","slug":"Java","permalink":"http://gangtieguo.cn/tags/Java/"},{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"http://gangtieguo.cn/tags/æŠ€æœ¯/"}]}]}